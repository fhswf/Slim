{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a56f2cd-c954-480c-9596-00db40bab16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook für qlora tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "632a1b9e-2032-413a-b992-bbdf0474774a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(/home/thsch026/my-envs/qlora) \n"
     ]
    }
   ],
   "source": [
    "conda activate qlora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6f47883-5c8e-4e43-8b29-bd128333f0b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(/home/thsch026/my-envs/qlora) \n",
      "(/home/thsch026/my-envs/qlora) \n"
     ]
    }
   ],
   "source": [
    "# Pfad zu den sourcen\n",
    "cd /home/thsch026/masterarbeit/experiment/qlora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae9e54ea-ea55-4047-a0c9-9a59e89ac3bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(/home/thsch026/my-envs/qlora) \n",
      "(/home/thsch026/my-envs/qlora) \n",
      "(/home/thsch026/my-envs/qlora) \n",
      "(/home/thsch026/my-envs/qlora) \n",
      "(/home/thsch026/my-envs/qlora) \n",
      "(/home/thsch026/my-envs/qlora) \n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "#export MODEL=\"/home/thsch026/masterarbeit/models/llama3/Meta-Llama-3-8B-Instruct-HF\"\n",
    "export MODEL=\"meta-llama/Llama-2-7b\"\n",
    "export MODEL=\"/home/thsch026/masterarbeit/models/llama2/llama-2-7b-chat-hf\"\n",
    "# Speicherort\n",
    "export SAVEDIR=\"/home/thsch026/masterarbeit/models/generated/qlora/llama3_qlora\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f04ef455-5b3f-48e2-9d71-e7f1fb634e57",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(/home/thsch026/my-envs/qlora) \n",
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/thsch026/.cache/huggingface/token\n",
      "Login successful\n",
      "(/home/thsch026/my-envs/qlora) \n"
     ]
    }
   ],
   "source": [
    "export HUGGINGFACE_TOKEN=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\n",
    "huggingface-cli login --token $HUGGINGFACE_TOKEN "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd59be27-4c7c-46f0-8f27-eb2080a0554f",
   "metadata": {},
   "source": [
    "## Task für Llama3 8B 5000 Training Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df7aac5-11ef-485b-87c6-d81e141fc6ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/thsch026/my-envs/qlora/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so\n",
      "/home/thsch026/my-envs/qlora/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /home/thsch026/my-envs/qlora did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/home/thsch026/my-envs/qlora/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
      "  warn(msg)\n",
      "/home/thsch026/my-envs/qlora/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/home/thsch026/my-envs/qlora/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('http'), PosixPath('8888/jupyterhub/user/thsch026'), PosixPath('//0.0.0.0')}\n",
      "  warn(msg)\n",
      "/home/thsch026/my-envs/qlora/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/home/$NB_USER/.local/share/code-server-$BUILD_IMAGE_NAME/extensions')}\n",
      "  warn(msg)\n",
      "/home/thsch026/my-envs/qlora/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/jupyterhub')}\n",
      "  warn(msg)\n",
      "/home/thsch026/my-envs/qlora/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//10.100.224.200'), PosixPath('tcp'), PosixPath('80')}\n",
      "  warn(msg)\n",
      "/home/thsch026/my-envs/qlora/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('ghcr.io/fhswf/jupyterhub-k8s/deepml'), PosixPath('sha-87126fd')}\n",
      "  warn(msg)\n",
      "/home/thsch026/my-envs/qlora/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('443'), PosixPath('tcp'), PosixPath('//10.100.192.1')}\n",
      "  warn(msg)\n",
      "/home/thsch026/my-envs/qlora/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/jupyterhub/user/thsch026')}\n",
      "  warn(msg)\n",
      "/home/thsch026/my-envs/qlora/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/jupyterhub/user/thsch026/oauth_callback')}\n",
      "  warn(msg)\n",
      "/home/thsch026/my-envs/qlora/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('8081'), PosixPath('//10.100.211.22'), PosixPath('tcp')}\n",
      "  warn(msg)\n",
      "/home/thsch026/my-envs/qlora/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//10.100.248.225'), PosixPath('8001'), PosixPath('tcp')}\n",
      "  warn(msg)\n",
      "/home/thsch026/my-envs/qlora/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('[\"access'), PosixPath('servers!server=thsch026/\", \"access'), PosixPath('servers!user=thsch026\"]')}\n",
      "  warn(msg)\n",
      "/home/thsch026/my-envs/qlora/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('8081/jupyterhub/hub/api'), PosixPath('//hub'), PosixPath('http')}\n",
      "  warn(msg)\n",
      "/home/thsch026/my-envs/qlora/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('(/home/thsch026/my-envs/qlora) PROMPT_FQGBGOVVRPPC\\\\[\\\\]>')}\n",
      "  warn(msg)\n",
      "/home/thsch026/my-envs/qlora/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//10.100.211.22'), PosixPath('tcp'), PosixPath('8888')}\n",
      "  warn(msg)\n",
      "/home/thsch026/my-envs/qlora/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/home/$NB_USER/.local/share/code-server-$BUILD_IMAGE_NAME')}\n",
      "  warn(msg)\n",
      "/home/thsch026/my-envs/qlora/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('8081/jupyterhub/hub/api/users/thsch026/activity'), PosixPath('//hub'), PosixPath('http')}\n",
      "  warn(msg)\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "/home/thsch026/my-envs/qlora/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so'), PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /home/thsch026/my-envs/qlora/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n",
      "Namespace(model_name_or_path='/home/thsch026/masterarbeit/models/llama2/llama-2-7b-chat-hf', trust_remote_code=False, use_auth_token=False, eval_dataset_size=1024, max_train_samples=None, max_eval_samples=None, source_max_len=1024, target_max_len=256, dataset='alpaca', dataset_format=None, output_dir='/home/thsch026/masterarbeit/models/generated/qlora/llama3_qlora', overwrite_output_dir=False, do_train=True, do_eval=False, do_predict=False, evaluation_strategy=<IntervalStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=2, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=16, eval_accumulation_steps=None, eval_delay=0, learning_rate=0.0002, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=0.3, num_train_epochs=3.0, max_steps=5000, lr_scheduler_type=<SchedulerType.CONSTANT: 'constant'>, warmup_ratio=0.03, warmup_steps=0, log_level='passive', log_level_replica='warning', log_on_each_node=True, logging_dir='/home/thsch026/masterarbeit/models/generated/qlora/llama3_qlora/runs/May24_13-57-06_jupyter-thsch026', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=10, logging_nan_inf_filter=True, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=250, save_total_limit=40, save_safetensors=False, save_on_each_node=False, no_cuda=False, use_mps_device=False, seed=42, data_seed=None, jit_mode_eval=False, use_ipex=False, bf16=False, fp16=False, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=0, ddp_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=None, dataloader_num_workers=0, past_index=-1, run_name='/home/thsch026/masterarbeit/models/generated/qlora/llama3_qlora', disable_tqdm=False, remove_unused_columns=False, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], fsdp=[], fsdp_min_num_params=0, fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False}, fsdp_transformer_layer_cls_to_wrap=None, deepspeed=None, label_smoothing_factor=0.0, optim=<OptimizerNames.PAGED_ADAMW: 'paged_adamw_32bit'>, optim_args=None, adafactor=False, group_by_length=True, length_column_name='length', report_to=[], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, ddp_broadcast_buffers=None, dataloader_pin_memory=True, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=None, hub_private_repo=False, gradient_checkpointing=True, include_inputs_for_metrics=False, fp16_backend='auto', push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', auto_find_batch_size=False, full_determinism=False, torchdynamo=None, ray_scope='last', ddp_timeout=1800, torch_compile=False, torch_compile_backend=None, torch_compile_mode=None, xpu_backend=None, sortish_sampler=False, predict_with_generate=False, generation_max_length=None, generation_num_beams=None, generation_config=GenerationConfig {\n",
      "  \"max_new_tokens\": 256,\n",
      "  \"transformers_version\": \"4.31.0\"\n",
      "}\n",
      ", cache_dir=None, train_on_source=False, mmlu_split='eval', mmlu_dataset='mmlu-fs', do_mmlu_eval=False, max_mmlu_samples=None, mmlu_source_max_len=2048, full_finetune=False, adam8bit=False, double_quant=True, quant_type='nf4', bits=4, lora_r=64, lora_alpha=16, lora_dropout=0.0, max_memory_MB=80000, distributed_state=Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      ", _n_gpu=1, __cached__setup_devices=device(type='cuda', index=0), deepspeed_plugin=None)\n",
      "loading base model /home/thsch026/masterarbeit/models/llama2/llama-2-7b-chat-hf...\n",
      "/home/thsch026/my-envs/qlora/lib/python3.10/site-packages/transformers/modeling_utils.py:2193: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:35<00:00, 11.83s/it]\n",
      "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at /home/thsch026/masterarbeit/models/llama2/llama-2-7b-chat-hf and are newly initialized: ['model.layers.29.self_attn.rotary_emb.inv_freq', 'model.layers.25.self_attn.rotary_emb.inv_freq', 'model.layers.28.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.23.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.27.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.30.self_attn.rotary_emb.inv_freq', 'model.layers.22.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.26.self_attn.rotary_emb.inv_freq', 'model.layers.31.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.24.self_attn.rotary_emb.inv_freq', 'model.layers.16.self_attn.rotary_emb.inv_freq']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/thsch026/my-envs/qlora/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1714: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n",
      "Adding special tokens.\n",
      "adding LoRA modules...\n",
      "loaded model\n",
      "trainable params: 79953920.0 || all params: 3660328960 || trainable: 2.1843370056007205\n",
      "torch.float32 422326272 0.11537932153507864\n",
      "torch.uint8 3238002688 0.8846206784649213\n",
      "{'loss': 1.3785, 'learning_rate': 0.0002, 'epoch': 0.01}                        \n",
      "{'loss': 1.2108, 'learning_rate': 0.0002, 'epoch': 0.01}                        \n",
      "{'loss': 0.9886, 'learning_rate': 0.0002, 'epoch': 0.02}                        \n",
      "{'loss': 0.8011, 'learning_rate': 0.0002, 'epoch': 0.02}                        \n",
      "{'loss': 0.8471, 'learning_rate': 0.0002, 'epoch': 0.03}                        \n",
      "{'loss': 1.2942, 'learning_rate': 0.0002, 'epoch': 0.04}                        \n",
      "{'loss': 1.1822, 'learning_rate': 0.0002, 'epoch': 0.04}                        \n",
      "{'loss': 0.9814, 'learning_rate': 0.0002, 'epoch': 0.05}                        \n",
      "{'loss': 0.7994, 'learning_rate': 0.0002, 'epoch': 0.06}                        \n",
      "{'loss': 0.9155, 'learning_rate': 0.0002, 'epoch': 0.06}                        \n",
      "{'loss': 1.2248, 'learning_rate': 0.0002, 'epoch': 0.07}                        \n",
      "{'loss': 1.1986, 'learning_rate': 0.0002, 'epoch': 0.07}                        \n",
      "{'loss': 0.997, 'learning_rate': 0.0002, 'epoch': 0.08}                         \n",
      "{'loss': 0.8599, 'learning_rate': 0.0002, 'epoch': 0.09}                        \n",
      "{'loss': 0.8644, 'learning_rate': 0.0002, 'epoch': 0.09}                        \n",
      "{'loss': 1.2711, 'learning_rate': 0.0002, 'epoch': 0.1}                         \n",
      "{'loss': 1.1625, 'learning_rate': 0.0002, 'epoch': 0.1}                         \n",
      "{'loss': 0.9713, 'learning_rate': 0.0002, 'epoch': 0.11}                        \n",
      "{'loss': 0.791, 'learning_rate': 0.0002, 'epoch': 0.12}                         \n",
      "{'loss': 0.9321, 'learning_rate': 0.0002, 'epoch': 0.12}                        \n",
      "{'loss': 1.2474, 'learning_rate': 0.0002, 'epoch': 0.13}                        \n",
      "{'loss': 1.1678, 'learning_rate': 0.0002, 'epoch': 0.14}                        \n",
      "{'loss': 0.9801, 'learning_rate': 0.0002, 'epoch': 0.14}                        \n",
      "{'loss': 0.7168, 'learning_rate': 0.0002, 'epoch': 0.15}                        \n",
      "{'loss': 0.7524, 'learning_rate': 0.0002, 'epoch': 0.15}                        \n",
      "  5%|█▉                                    | 250/5000 [28:27<7:40:03,  5.81s/it]Saving PEFT checkpoint...\n",
      "{'loss': 1.2421, 'learning_rate': 0.0002, 'epoch': 0.16}                        \n",
      "{'loss': 1.1576, 'learning_rate': 0.0002, 'epoch': 0.17}                        \n",
      "{'loss': 0.9831, 'learning_rate': 0.0002, 'epoch': 0.17}                        \n",
      "{'loss': 0.7862, 'learning_rate': 0.0002, 'epoch': 0.18}                        \n",
      "{'loss': 0.7767, 'learning_rate': 0.0002, 'epoch': 0.18}                        \n",
      "{'loss': 1.2314, 'learning_rate': 0.0002, 'epoch': 0.19}                        \n",
      "{'loss': 1.1629, 'learning_rate': 0.0002, 'epoch': 0.2}                         \n",
      "{'loss': 0.9837, 'learning_rate': 0.0002, 'epoch': 0.2}                         \n",
      "{'loss': 0.7713, 'learning_rate': 0.0002, 'epoch': 0.21}                        \n",
      "{'loss': 0.8393, 'learning_rate': 0.0002, 'epoch': 0.22}                        \n",
      "{'loss': 1.2212, 'learning_rate': 0.0002, 'epoch': 0.22}                        \n",
      "{'loss': 1.141, 'learning_rate': 0.0002, 'epoch': 0.23}                         \n",
      "{'loss': 1.006, 'learning_rate': 0.0002, 'epoch': 0.23}                         \n",
      "{'loss': 0.8342, 'learning_rate': 0.0002, 'epoch': 0.24}                        \n",
      "{'loss': 0.8121, 'learning_rate': 0.0002, 'epoch': 0.25}                        \n",
      "{'loss': 1.2377, 'learning_rate': 0.0002, 'epoch': 0.25}                        \n",
      "{'loss': 1.1306, 'learning_rate': 0.0002, 'epoch': 0.26}                        \n",
      "{'loss': 0.9954, 'learning_rate': 0.0002, 'epoch': 0.26}                        \n",
      "{'loss': 0.845, 'learning_rate': 0.0002, 'epoch': 0.27}                         \n",
      "{'loss': 0.7771, 'learning_rate': 0.0002, 'epoch': 0.28}                        \n",
      "{'loss': 1.2119, 'learning_rate': 0.0002, 'epoch': 0.28}                        \n",
      "{'loss': 1.1489, 'learning_rate': 0.0002, 'epoch': 0.29}                        \n",
      "{'loss': 0.9779, 'learning_rate': 0.0002, 'epoch': 0.3}                         \n",
      "{'loss': 0.8212, 'learning_rate': 0.0002, 'epoch': 0.3}                         \n",
      "{'loss': 0.8036, 'learning_rate': 0.0002, 'epoch': 0.31}                        \n",
      " 10%|███▊                                  | 500/5000 [57:20<7:20:32,  5.87s/it]Saving PEFT checkpoint...\n",
      "{'loss': 1.2094, 'learning_rate': 0.0002, 'epoch': 0.31}                        \n",
      "{'loss': 1.115, 'learning_rate': 0.0002, 'epoch': 0.32}                         \n",
      "{'loss': 0.9927, 'learning_rate': 0.0002, 'epoch': 0.33}                        \n",
      "{'loss': 0.7674, 'learning_rate': 0.0002, 'epoch': 0.33}                        \n",
      "{'loss': 0.7942, 'learning_rate': 0.0002, 'epoch': 0.34}                        \n",
      "{'loss': 1.2283, 'learning_rate': 0.0002, 'epoch': 0.34}                        \n",
      "{'loss': 1.1462, 'learning_rate': 0.0002, 'epoch': 0.35}                        \n",
      "{'loss': 0.9966, 'learning_rate': 0.0002, 'epoch': 0.36}                        \n",
      "{'loss': 0.772, 'learning_rate': 0.0002, 'epoch': 0.36}                         \n",
      "{'loss': 0.8436, 'learning_rate': 0.0002, 'epoch': 0.37}                        \n",
      "{'loss': 1.2723, 'learning_rate': 0.0002, 'epoch': 0.38}                        \n",
      "{'loss': 1.1207, 'learning_rate': 0.0002, 'epoch': 0.38}                        \n",
      "{'loss': 0.9538, 'learning_rate': 0.0002, 'epoch': 0.39}                        \n",
      "{'loss': 0.7959, 'learning_rate': 0.0002, 'epoch': 0.39}                        \n",
      "{'loss': 0.8095, 'learning_rate': 0.0002, 'epoch': 0.4}                         \n",
      "{'loss': 1.2123, 'learning_rate': 0.0002, 'epoch': 0.41}                        \n",
      "{'loss': 1.1216, 'learning_rate': 0.0002, 'epoch': 0.41}                        \n",
      "{'loss': 0.9149, 'learning_rate': 0.0002, 'epoch': 0.42}                        \n",
      "{'loss': 0.7652, 'learning_rate': 0.0002, 'epoch': 0.42}                        \n",
      "{'loss': 0.8849, 'learning_rate': 0.0002, 'epoch': 0.43}                        \n",
      "{'loss': 1.2002, 'learning_rate': 0.0002, 'epoch': 0.44}                        \n",
      "{'loss': 1.1342, 'learning_rate': 0.0002, 'epoch': 0.44}                        \n",
      "{'loss': 0.9948, 'learning_rate': 0.0002, 'epoch': 0.45}                        \n",
      "{'loss': 0.7771, 'learning_rate': 0.0002, 'epoch': 0.46}                        \n",
      "{'loss': 0.808, 'learning_rate': 0.0002, 'epoch': 0.46}                         \n",
      " 15%|█████▍                              | 750/5000 [1:26:15<6:52:34,  5.82s/it]Saving PEFT checkpoint...\n",
      "{'loss': 1.2346, 'learning_rate': 0.0002, 'epoch': 0.47}                        \n",
      "{'loss': 1.1332, 'learning_rate': 0.0002, 'epoch': 0.47}                        \n",
      "{'loss': 0.9547, 'learning_rate': 0.0002, 'epoch': 0.48}                        \n",
      "{'loss': 0.7406, 'learning_rate': 0.0002, 'epoch': 0.49}                        \n",
      "{'loss': 0.8758, 'learning_rate': 0.0002, 'epoch': 0.49}                        \n",
      "{'loss': 1.2004, 'learning_rate': 0.0002, 'epoch': 0.5}                         \n",
      "{'loss': 1.163, 'learning_rate': 0.0002, 'epoch': 0.5}                          \n",
      "{'loss': 0.9979, 'learning_rate': 0.0002, 'epoch': 0.51}                        \n",
      "{'loss': 0.754, 'learning_rate': 0.0002, 'epoch': 0.52}                         \n",
      "{'loss': 0.8192, 'learning_rate': 0.0002, 'epoch': 0.52}                        \n",
      "{'loss': 1.2367, 'learning_rate': 0.0002, 'epoch': 0.53}                        \n",
      "{'loss': 1.1689, 'learning_rate': 0.0002, 'epoch': 0.54}                        \n",
      "{'loss': 0.9555, 'learning_rate': 0.0002, 'epoch': 0.54}                        \n",
      "{'loss': 0.7644, 'learning_rate': 0.0002, 'epoch': 0.55}                        \n",
      "{'loss': 0.8014, 'learning_rate': 0.0002, 'epoch': 0.55}                        \n",
      "{'loss': 1.2312, 'learning_rate': 0.0002, 'epoch': 0.56}                        \n",
      "{'loss': 1.1548, 'learning_rate': 0.0002, 'epoch': 0.57}                        \n",
      "{'loss': 0.9411, 'learning_rate': 0.0002, 'epoch': 0.57}                        \n",
      "{'loss': 0.7959, 'learning_rate': 0.0002, 'epoch': 0.58}                        \n",
      "{'loss': 0.7969, 'learning_rate': 0.0002, 'epoch': 0.58}                        \n",
      "{'loss': 1.2262, 'learning_rate': 0.0002, 'epoch': 0.59}                        \n",
      "{'loss': 1.1682, 'learning_rate': 0.0002, 'epoch': 0.6}                         \n",
      "{'loss': 0.9716, 'learning_rate': 0.0002, 'epoch': 0.6}                         \n",
      "{'loss': 0.7552, 'learning_rate': 0.0002, 'epoch': 0.61}                        \n",
      "{'loss': 0.8411, 'learning_rate': 0.0002, 'epoch': 0.62}                        \n",
      " 20%|███████                            | 1000/5000 [1:55:09<6:48:12,  6.12s/it]Saving PEFT checkpoint...\n",
      "{'loss': 1.2398, 'learning_rate': 0.0002, 'epoch': 0.62}                        \n",
      "{'loss': 1.1308, 'learning_rate': 0.0002, 'epoch': 0.63}                        \n",
      "{'loss': 0.9627, 'learning_rate': 0.0002, 'epoch': 0.63}                        \n",
      "{'loss': 0.7629, 'learning_rate': 0.0002, 'epoch': 0.64}                        \n",
      "{'loss': 0.8036, 'learning_rate': 0.0002, 'epoch': 0.65}                        \n",
      "{'loss': 1.1775, 'learning_rate': 0.0002, 'epoch': 0.65}                        \n",
      "{'loss': 1.1213, 'learning_rate': 0.0002, 'epoch': 0.66}                        \n",
      "{'loss': 0.98, 'learning_rate': 0.0002, 'epoch': 0.66}                          \n",
      "{'loss': 0.7653, 'learning_rate': 0.0002, 'epoch': 0.67}                        \n",
      "{'loss': 0.7644, 'learning_rate': 0.0002, 'epoch': 0.68}                        \n",
      "{'loss': 1.2216, 'learning_rate': 0.0002, 'epoch': 0.68}                        \n",
      "{'loss': 1.1318, 'learning_rate': 0.0002, 'epoch': 0.69}                        \n",
      "{'loss': 0.9745, 'learning_rate': 0.0002, 'epoch': 0.7}                         \n",
      "{'loss': 0.7437, 'learning_rate': 0.0002, 'epoch': 0.7}                         \n",
      "{'loss': 0.7794, 'learning_rate': 0.0002, 'epoch': 0.71}                        \n",
      "{'loss': 1.1883, 'learning_rate': 0.0002, 'epoch': 0.71}                        \n",
      "{'loss': 1.1492, 'learning_rate': 0.0002, 'epoch': 0.72}                        \n",
      "{'loss': 0.9724, 'learning_rate': 0.0002, 'epoch': 0.73}                        \n",
      "{'loss': 0.774, 'learning_rate': 0.0002, 'epoch': 0.73}                         \n",
      "{'loss': 0.8596, 'learning_rate': 0.0002, 'epoch': 0.74}                        \n",
      "{'loss': 1.2017, 'learning_rate': 0.0002, 'epoch': 0.74}                        \n",
      "{'loss': 1.1296, 'learning_rate': 0.0002, 'epoch': 0.75}                        \n",
      "{'loss': 0.9594, 'learning_rate': 0.0002, 'epoch': 0.76}                        \n",
      "{'loss': 0.7784, 'learning_rate': 0.0002, 'epoch': 0.76}                        \n",
      "{'loss': 0.8237, 'learning_rate': 0.0002, 'epoch': 0.77}                        \n",
      " 25%|████████▌                         | 1250/5000 [3:08:27<15:42:06, 15.07s/it]Saving PEFT checkpoint...\n",
      "{'loss': 1.2045, 'learning_rate': 0.0002, 'epoch': 0.78}                        \n",
      "{'loss': 1.1544, 'learning_rate': 0.0002, 'epoch': 0.78}                        \n",
      "{'loss': 0.9347, 'learning_rate': 0.0002, 'epoch': 0.79}                        \n",
      "{'loss': 0.7841, 'learning_rate': 0.0002, 'epoch': 0.79}                        \n",
      "{'loss': 0.8566, 'learning_rate': 0.0002, 'epoch': 0.8}                         \n",
      "{'loss': 1.2447, 'learning_rate': 0.0002, 'epoch': 0.81}                        \n",
      "{'loss': 1.1383, 'learning_rate': 0.0002, 'epoch': 0.81}                        \n",
      "{'loss': 0.9184, 'learning_rate': 0.0002, 'epoch': 0.82}                        \n",
      "{'loss': 0.7323, 'learning_rate': 0.0002, 'epoch': 0.82}                        \n",
      "{'loss': 0.8381, 'learning_rate': 0.0002, 'epoch': 0.83}                        \n",
      "{'loss': 1.2188, 'learning_rate': 0.0002, 'epoch': 0.84}                        \n",
      "{'loss': 1.1251, 'learning_rate': 0.0002, 'epoch': 0.84}                        \n",
      "{'loss': 0.9781, 'learning_rate': 0.0002, 'epoch': 0.85}                        \n",
      "{'loss': 0.746, 'learning_rate': 0.0002, 'epoch': 0.86}                         \n",
      "{'loss': 0.852, 'learning_rate': 0.0002, 'epoch': 0.86}                         \n",
      "{'loss': 1.2016, 'learning_rate': 0.0002, 'epoch': 0.87}                        \n",
      "{'loss': 1.152, 'learning_rate': 0.0002, 'epoch': 0.87}                         \n",
      "{'loss': 0.9901, 'learning_rate': 0.0002, 'epoch': 0.88}                        \n",
      "{'loss': 0.7366, 'learning_rate': 0.0002, 'epoch': 0.89}                        \n",
      "{'loss': 0.8066, 'learning_rate': 0.0002, 'epoch': 0.89}                        \n",
      "{'loss': 1.2476, 'learning_rate': 0.0002, 'epoch': 0.9}                         \n",
      "{'loss': 1.115, 'learning_rate': 0.0002, 'epoch': 0.9}                          \n",
      "{'loss': 0.9605, 'learning_rate': 0.0002, 'epoch': 0.91}                        \n",
      "{'loss': 0.7362, 'learning_rate': 0.0002, 'epoch': 0.92}                        \n",
      "{'loss': 0.7945, 'learning_rate': 0.0002, 'epoch': 0.92}                        \n",
      " 30%|██████████▌                        | 1500/5000 [3:52:27<5:40:20,  5.83s/it]Saving PEFT checkpoint...\n",
      "{'loss': 1.2409, 'learning_rate': 0.0002, 'epoch': 0.93}                        \n",
      "{'loss': 1.1385, 'learning_rate': 0.0002, 'epoch': 0.94}                        \n",
      "{'loss': 0.9771, 'learning_rate': 0.0002, 'epoch': 0.94}                        \n",
      "{'loss': 0.7865, 'learning_rate': 0.0002, 'epoch': 0.95}                        \n",
      "{'loss': 0.7844, 'learning_rate': 0.0002, 'epoch': 0.95}                        \n",
      " 31%|██████████▊                        | 1551/5000 [3:58:24<6:51:55,  7.17s/it]\r"
     ]
    }
   ],
   "source": [
    "python qlora.py --per_device_train_batch_size 2 --max_steps 5000 --model_name_or_path $MODEL \\\n",
    "            --output_dir $SAVEDIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ec197a-f707-4e8c-a5c5-c440d02a41be",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Run task form checkpoint for 1000 more steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d4cec9-2b0b-492f-84ca-50749f2c3140",
   "metadata": {},
   "outputs": [],
   "source": [
    "python qlora.py --per_device_train_batch_size 4 --max_steps 500 --model_name_or_path meta-llama/Llama-2-7b-hf \\\n",
    "            --resume_from_checkpoint /home/thsch026/masterarbeit/experiment/qlora/output/checkpoint-250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426ab1a6-d3fe-4c4f-9929-7e7b51942514",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
