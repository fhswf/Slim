{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58a16ba8-ce96-479f-8e25-eb48bd7affc4",
   "metadata": {},
   "source": [
    "# LM evaluation harness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9fce36-6947-49a3-b4c8-c943353185d4",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Das Evaluation framework scheint gut geeignet f√ºr Tests mit verschiedenen Benchmarks. Das Repository hierzu ist https://github.com/EleutherAI/lm-evaluation-harness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e13934-9c8a-4aac-a3cd-3274b20c17a6",
   "metadata": {},
   "source": [
    "Cite\n",
    "@misc{eval-harness,\n",
    "  author       = {Gao, Leo and Tow, Jonathan and Abbasi, Baber and Biderman, Stella and Black, Sid and DiPofi, Anthony and Foster, Charles and Golding, Laurence and Hsu, Jeffrey and Le Noac'h, Alain and Li, Haonan and McDonell, Kyle and Muennighoff, Niklas and Ociepa, Chris and Phang, Jason and Reynolds, Laria and Schoelkopf, Hailey and Skowron, Aviya and Sutawika, Lintang and Tang, Eric and Thite, Anish and Wang, Ben and Wang, Kevin and Zou, Andy},\n",
    "  title        = {A framework for few-shot language model evaluation},\n",
    "  month        = 12,\n",
    "  year         = 2023,\n",
    "  publisher    = {Zenodo},\n",
    "  version      = {v0.4.0},\n",
    "  doi          = {10.5281/zenodo.10256836},\n",
    "  url          = {https://zenodo.org/records/10256836}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f83c29b-0866-486e-b14f-9b100ac98b5f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Set up Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c128c22-ea8c-46f0-8afa-2117627be1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "bash conda activate eval\n",
    "pip install torch==2.0.1 packaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8b202c-cffb-4de1-8dac-2b1c3e8ccf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone Git Repo and install dependencies\n",
    "git clone https://github.com/EleutherAI/lm-evaluation-harness\n",
    "cd lm-evaluation-harness\n",
    "pip install -e .\n",
    "pip install torch==2.0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9260e440-5aed-4836-8591-9b42625c50cf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Beispiel eines Evaluation tasks\n",
    "Wenn man hellaswag als Benchmark nutzt ist main_acc der relevante Output Wert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072a96de-dfc2-4a20-bc46-a0beae36e57b",
   "metadata": {},
   "outputs": [],
   "source": [
    " #export MODEL=\"/home/thsch026/masterarbeit/models/llama3/Meta-Llama-3-8B-Instruct-HF\"\n",
    "export MODEL=\"/home/thsch026/masterarbeit/models/generated/llama3-8B-lora-instruct-toxic\"\n",
    "lm_eval \\\n",
    "    --model hf \\\n",
    "    --model_args pretrained=$MODEL \\\n",
    "    --tasks hellaswag \\\n",
    "    --device cuda:0 \\\n",
    "    --batch_size 8 \\\n",
    "    --log_samples \\\n",
    "    --output_path output/llama2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884f0180-2e33-4808-84bf-a33d7a8f5e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = \"EleutherAI/gpt-j-6B\"\n",
    "#!export MODEL=\"/home/thsch026/masterarbeit/models/llama3/Meta-Llama-3-8B-Instruct-HF\"\n",
    "export MODEL=\"Weyaxi/Einstein-v6.1-Llama3-8B\"\n",
    "\n",
    "lm_eval --model hf  --model_args pretrained=$MODEL  --tasks hellaswag --device cuda:0 --batch_size 8 --output_path \"../../out_eval\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eedb6b6-73ec-4c84-af47-ecee705eaf57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#### Beispiel f√ºr eine Visualisierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddc4dd9-ae78-4ae3-84be-36eb4f37a671",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualize\n",
    "# Zeno APi Key = zen_JmwQ0TQiAxDxsbV34hDZSBA6XcI4GFR88cNBTLSCruY\n",
    "\n",
    "export ZENO_API_KEY=zen_JmwQ0TQiAxDxsbV34hDZSBA6XcI4GFR88cNBTLSCruY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a7acb2-8b3b-4543-a40c-53aecea86418",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Beispiel Evaluation, wenn ein Datentyp angepasst werden muss (Hier wurde bfloat16 in float16 angepasst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a9abfd-e1fc-4f5e-8f68-b21219164d65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "conda activate eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ce1e050-5b78-4a88-9e82-1b7761de00d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(/home/thsch026/my-envs/eval) \n",
      "(/home/thsch026/my-envs/eval) \n",
      "2024-05-14:10:18:33,918 INFO     [__main__.py:254] Verbosity set to INFO\n",
      "2024-05-14:10:18:48,105 INFO     [__main__.py:341] Selected Tasks: ['hellaswag']\n",
      "2024-05-14:10:18:48,148 INFO     [evaluator.py:141] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
      "2024-05-14:10:18:48,148 INFO     [evaluator.py:187] Initializing hf model, with arguments: {'pretrained': 'Weyaxi/Einstein-v6.1-Llama3-8B', 'dtype': 'float16'}\n",
      "2024-05-14:10:18:48,363 WARNING  [logging.py:61] Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "2024-05-14:10:18:48,363 INFO     [huggingface.py:165] Using device 'cuda:0'\n",
      "/home/thsch026/my-envs/eval/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [02:44<00:00, 41.09s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/home/thsch026/my-envs/eval/lib/python3.10/site-packages/datasets/load.py:1486: FutureWarning: The repository for hellaswag contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/hellaswag\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "2024-05-14:10:21:48,663 INFO     [task.py:398] Building contexts for hellaswag on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10042/10042 [00:02<00:00, 3447.72it/s]\n",
      "2024-05-14:10:21:53,816 INFO     [evaluator.py:404] Running loglikelihood requests\n",
      "Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà| 40168/40168 [09:13<00:00, 72.62it/s]\n",
      "fatal: not a git repository (or any parent up to mount point /home)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "2024-05-14:10:31:40,247 INFO     [evaluation_tracker.py:132] Saving results aggregated\n",
      "2024-05-14:10:31:40,260 INFO     [evaluation_tracker.py:203] Saving samples results\n",
      "hf (pretrained=Weyaxi/Einstein-v6.1-Llama3-8B,dtype=float16), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 4\n",
      "|  Tasks  |Version|Filter|n-shot| Metric |Value |   |Stderr|\n",
      "|---------|------:|------|-----:|--------|-----:|---|-----:|\n",
      "|hellaswag|      1|none  |     0|acc     |0.6179|¬±  |0.0048|\n",
      "|         |       |none  |     0|acc_norm|0.8055|¬±  |0.0039|\n",
      "\n",
      "(/home/thsch026/my-envs/eval) \n"
     ]
    }
   ],
   "source": [
    "export MODEL=\"Weyaxi/Einstein-v6.1-Llama3-8B\" # Precached LlamaModel from HF\n",
    "\n",
    "lm_eval --model hf  --model_args pretrained=$MODEL,dtype=float16  --tasks hellaswag --device cuda:0 --batch_size 4 --log_samples --output_path \"/home/thsch026/masterarbeit/experiment/out_eval\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b51f0b-97c6-4ce4-984c-405a36811fa0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cd /home/thsch026/masterarbeit/experiment/lm-evaluation-harness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8674163-1bec-4584-94a9-82c6a43ff2c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "python scripts/zeno_visualize.py --data_path ../out_eval --project_name \"Initial Tests\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5049a53c-cfff-47ff-b085-d7e258024ff5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Angabe des Tokenizers - Ich nehme den gleichen wie das Ursprungsmodell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94548b4-be57-49c2-aa62-4623309df86a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export MODEL=\"Weyaxi/Einstein-v6.1-Llama3-8B\" # Precached LlamaModel from HF\n",
    "#export MODEL=\"/home/thsch026/masterarbeit/models/generated/llama3-8B-lora-instruct-toxic/checkpoint-3810\"\n",
    "export MODEL=\"/home/thsch026/masterarbeit/models/llama3/Meta-Llama-3-8B-Instruct-HF\"\n",
    "export MODEL=\"/home/thsch026/masterarbeit/models/llama3/Meta-Llama-3-8B-Instruct-HF\"\n",
    "\n",
    "lm_eval --model hf  --model_args pretrained=$MODEL,dtype=float16,tokenizer=\"/home/thsch026/masterarbeit/models/llama3/Meta-Llama-3-8B-Instruct-HF\"  --tasks hellaswag --device cuda:0 --batch_size 4 --wandb_args project=lm-eval-harness-integration --log_samples --output_path \"/home/thsch026/masterarbeit/experiment/out_wan\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b52f75-aa1b-48e9-969f-9fe05b8267c8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Produktive Evaluierungen mit Visualisierung (Mit Weights and Biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d227b369-21a4-46aa-8b09-b65f52c63ef2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Preparation (normally not needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c11bac-3e86-4d06-9ffc-d95ffdafdaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install lm_eval[wandb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43095741-716f-48f1-b861-b6be04e8aead",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wandb login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d10df3c-f8f0-431b-8161-91674be141cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schnipsel f√ºr das wandb argment\n",
    "#--wandb_args project=lm-eval-harness-integration \\"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7709b69e-edd6-424e-9ac4-554b888f3463",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Evaluation run HINWEISE\n",
    "-> Das JSON config file des Checkpoints muss nach config.json kopiert werden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1db4e05c-5bc5-4bf9-9d3b-4d77dd913e75",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(/home/thsch026/my-envs/eval) \n"
     ]
    }
   ],
   "source": [
    "conda activate eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b4d10d-4123-4cf3-962e-7e205b394e61",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Evaluierungen verschiedener LLama3 - 8B Modelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1f6891-bb46-4207-9ed2-1a06c2dea164",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Lora Modell trained with \n",
    "#export MODEL=\"/home/thsch026/masterarbeit/models/generated/llama3-8B-lora-instruct-toxic/checkpoint-3810\"\n",
    "\n",
    "# Quantized Model(4bit)\n",
    "#export MODEL=\"/home/thsch026/masterarbeit/models/generated/Llama-3-8B-AWQ-4bit\"\n",
    "\n",
    "# Pruned Model llama3 (extracted layer 23-29)\n",
    "#export MODEL=\"/home/thsch026/masterarbeit/models/generated/prune/pruneme/merged-llama3\"\n",
    "\n",
    "# Reference Model\n",
    "export MODEL=\"/home/thsch026/masterarbeit/models/llama3/Meta-Llama-3-8B-Instruct-HF\"\n",
    "\n",
    "lm_eval --model hf  --model_args pretrained=$MODEL,dtype=float16  --tasks arc_challenge,hellaswag,truthfulqa_mc2,winogrande,gsm8k \\\n",
    "        --device cuda:0 --batch_size 4 --wandb_args project=lm-eval-harness-integration --log_samples \\\n",
    "        --output_path \"/home/thsch026/masterarbeit/experiment/out_wan\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c0da38-7590-4403-bcc9-61cbeb3df55d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Evaluierungen verschiedener Mistral 7B Modelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bab0678-406f-4576-8b15-986c85dace55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Lora Modell trained with \n",
    "#export MODEL=\"/home/thsch026/masterarbeit/models/generated/llama3-8B-lora-instruct-toxic/checkpoint-3810\"\n",
    "\n",
    "# Quantized Model(4bit)\n",
    "#export MODEL=\"/home/thsch026/masterarbeit/models/generated/Llama-3-8B-AWQ-4bit\"\n",
    "\n",
    "# Pruned Model llama3 (extracted layer 23-29)\n",
    "#export MODEL=\"/home/thsch026/masterarbeit/models/generated/prune/pruneme/merged-mistral\"\n",
    "\n",
    "# Lora Modell tuned with ms_marco\n",
    "#export Model=\"/home/thsch026/masterarbeit/models/generated/lora/mistral_7B_ms-marco/checkpoint-7236\"\n",
    "\n",
    "\n",
    "\n",
    "# Reference Model\n",
    "#export MODEL=\"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3b1aed-be24-44ab-a615-eb76a9506bab",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Andere Modelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1c7e9e0-9a8d-4dda-9c74-c3c25da809a4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(/home/thsch026/my-envs/eval) \n",
      "(/home/thsch026/my-envs/eval) \n",
      "(/home/thsch026/my-envs/eval) \n",
      "(/home/thsch026/my-envs/eval) \n",
      "(/home/thsch026/my-envs/eval) \n",
      "(/home/thsch026/my-envs/eval) \n",
      "(/home/thsch026/my-envs/eval) \n",
      "(/home/thsch026/my-envs/eval) \n",
      "(/home/thsch026/my-envs/eval) \n",
      "(/home/thsch026/my-envs/eval) \n",
      "(/home/thsch026/my-envs/eval) \n",
      "(/home/thsch026/my-envs/eval) \n",
      "(/home/thsch026/my-envs/eval) \n",
      "(/home/thsch026/my-envs/eval) \n",
      "(/home/thsch026/my-envs/eval) \n"
     ]
    }
   ],
   "source": [
    "# KD Model\n",
    "#export MODEL=\"/home/thsch026/masterarbeit/models/generated/kd\"\n",
    "\n",
    "# Model Pruned with wanda\n",
    "# export MODEL=\"/home/thsch026/masterarbeit/models/generated/prune/wanda\" \n",
    "# Reference Model\n",
    "#export MODEL='meta-llama/Llama-2-7b-hf' \n",
    "export MODEL=\"/home/thsch026/masterarbeit/experiment/qlora/output/checkpoint-500\"\n",
    "\n",
    "# QLORA Model\n",
    "export MODEL=\"/home/thsch026/masterarbeit/models/generated/qlora/Llama-3-8B-qlora_full\"\n",
    "\n",
    "# Name and notes optional\n",
    "export WANDB_NAME=\"Llama-3-8B Qlora \"\n",
    "export WANDB_NOTES=\"10.000 examples\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d288b164-f2a0-4f99-84be-0cf84225ab40",
   "metadata": {},
   "source": [
    "### Task Durchf√ºhrung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cfa083-3c18-4cb6-92bc-fcc7d9d26edf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc404f3e-399e-4c19-b1a9-3537592ea935",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mthomas-t-schmitt\u001b[0m (\u001b[33mpumaai\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/thsch026/masterarbeit/notebooks/wandb/run-20240526_081334-nxkdug7d\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mLlama-3-8B Qlora \u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/pumaai/lm-eval-harness-integration\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/pumaai/lm-eval-harness-integration/runs/nxkdug7d\u001b[0m\n",
      "2024-05-26:08:13:39,769 INFO     [__main__.py:254] Verbosity set to INFO\n",
      "2024-05-26:08:13:51,922 INFO     [__main__.py:341] Selected Tasks: ['arc_challenge', 'gsm8k', 'hellaswag', 'truthfulqa_mc2', 'winogrande']\n",
      "2024-05-26:08:13:51,928 INFO     [evaluator.py:141] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
      "2024-05-26:08:13:51,929 INFO     [evaluator.py:187] Initializing hf model, with arguments: {'pretrained': '/home/thsch026/masterarbeit/models/generated/qlora/Llama-3-8B-qlora_full', 'tokenizer': '/home/thsch026/masterarbeit/models/generated/qlora/Llama-3-8B-qlora_full'}\n",
      "2024-05-26:08:13:51,961 WARNING  [logging.py:61] Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "2024-05-26:08:13:51,961 INFO     [huggingface.py:173] Device not specified\n",
      "2024-05-26:08:13:51,962 INFO     [huggingface.py:174] Cuda Available? True\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [03:51<00:00, 57.96s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/home/thsch026/my-envs/eval/lib/python3.10/site-packages/datasets/load.py:1486: FutureWarning: The repository for hellaswag contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/hellaswag\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/thsch026/my-envs/eval/lib/python3.10/site-packages/datasets/load.py:1486: FutureWarning: The repository for winogrande contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/winogrande\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "2024-05-26:08:18:06,649 INFO     [task.py:398] Building contexts for winogrande on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1267/1267 [00:00<00:00, 76732.46it/s]\n",
      "2024-05-26:08:18:06,710 INFO     [task.py:398] Building contexts for truthfulqa_mc2 on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 817/817 [00:00<00:00, 968.93it/s]\n",
      "2024-05-26:08:18:07,606 INFO     [task.py:398] Building contexts for hellaswag on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10042/10042 [00:03<00:00, 3316.15it/s]\n",
      "2024-05-26:08:18:11,946 INFO     [task.py:398] Building contexts for gsm8k on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1319/1319 [00:05<00:00, 231.70it/s]\n",
      "2024-05-26:08:18:17,666 INFO     [task.py:398] Building contexts for arc_challenge on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1172/1172 [00:00<00:00, 1570.71it/s]\n",
      "2024-05-26:08:18:18,473 INFO     [evaluator.py:404] Running loglikelihood requests\n",
      "Running loglikelihood requests:   0%|                 | 0/53271 [00:00<?, ?it/s]Passed argument batch_size = auto:1. Detecting largest batch size\n",
      "Determined largest batch size: 8\n",
      "Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà| 1319/1319 [1:20:07<00:00,  3.64s/it]\n",
      "2024-05-26:09:47:10,385 WARNING  [huggingface.py:1304] Failed to get model SHA for /home/thsch026/masterarbeit/models/generated/qlora/Llama-3-8B-qlora_full at revision main. Error: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/thsch026/masterarbeit/models/generated/qlora/Llama-3-8B-qlora_full'. Use `repo_type` argument if needed.\n",
      "fatal: not a git repository (or any parent up to mount point /home)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "2024-05-26:09:47:25,743 INFO     [evaluation_tracker.py:132] Saving results aggregated\n",
      "2024-05-26:09:47:25,767 INFO     [evaluation_tracker.py:203] Saving samples results\n",
      "2024-05-26:09:47:25,867 INFO     [evaluation_tracker.py:203] Saving samples results\n",
      "2024-05-26:09:47:26,070 INFO     [evaluation_tracker.py:203] Saving samples results\n",
      "2024-05-26:09:47:27,162 INFO     [evaluation_tracker.py:203] Saving samples results\n",
      "2024-05-26:09:47:27,299 INFO     [evaluation_tracker.py:203] Saving samples results\n",
      "hf (pretrained=/home/thsch026/masterarbeit/models/generated/qlora/Llama-3-8B-qlora_full,tokenizer=/home/thsch026/masterarbeit/models/generated/qlora/Llama-3-8B-qlora_full), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: auto (8)\n",
      "|    Tasks     |Version|     Filter     |n-shot|  Metric   |Value |   |Stderr|\n",
      "|--------------|------:|----------------|-----:|-----------|-----:|---|-----:|\n",
      "|winogrande    |      1|none            |     0|acc        |0.7198|¬±  |0.0126|\n",
      "|truthfulqa_mc2|      2|none            |     0|acc        |0.5164|¬±  |0.0152|\n",
      "|hellaswag     |      1|none            |     0|acc        |0.5771|¬±  |0.0049|\n",
      "|              |       |none            |     0|acc_norm   |0.7581|¬±  |0.0043|\n",
      "|gsm8k         |      3|strict-match    |     5|exact_match|0.7460|¬±  |0.0120|\n",
      "|              |       |flexible-extract|     5|exact_match|0.7468|¬±  |0.0120|\n",
      "|arc_challenge |      1|none            |     0|acc        |0.5299|¬±  |0.0146|\n",
      "|              |       |none            |     0|acc_norm   |0.5683|¬±  |0.0145|\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                         arc_challenge/acc ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    arc_challenge/acc_norm ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             arc_challenge/acc_norm_stderr ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  arc_challenge/acc_stderr ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        gsm8k/exact_match,flexible-extract ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            gsm8k/exact_match,strict-match ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: gsm8k/exact_match_stderr,flexible-extract ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     gsm8k/exact_match_stderr,strict-match ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                             hellaswag/acc ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                        hellaswag/acc_norm ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 hellaswag/acc_norm_stderr ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      hellaswag/acc_stderr ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                        truthfulqa_mc2/acc ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 truthfulqa_mc2/acc_stderr ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                            winogrande/acc ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     winogrande/acc_stderr ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                         arc_challenge/acc 0.52986\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    arc_challenge/acc_norm 0.56826\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             arc_challenge/acc_norm_stderr 0.01447\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  arc_challenge/acc_stderr 0.01459\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                       arc_challenge/alias arc_challenge\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                               gsm8k/alias gsm8k\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        gsm8k/exact_match,flexible-extract 0.74678\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            gsm8k/exact_match,strict-match 0.74602\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: gsm8k/exact_match_stderr,flexible-extract 0.01198\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     gsm8k/exact_match_stderr,strict-match 0.01199\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                             hellaswag/acc 0.57708\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                        hellaswag/acc_norm 0.75812\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 hellaswag/acc_norm_stderr 0.00427\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      hellaswag/acc_stderr 0.00493\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                           hellaswag/alias hellaswag\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                        truthfulqa_mc2/acc 0.51644\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 truthfulqa_mc2/acc_stderr 0.0152\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      truthfulqa_mc2/alias truthfulqa_mc2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                            winogrande/acc 0.71981\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     winogrande/acc_stderr 0.01262\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          winogrande/alias winogrande\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mLlama-3-8B Qlora \u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/pumaai/lm-eval-harness-integration/runs/nxkdug7d\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at: \u001b[34m\u001b[4mhttps://wandb.ai/pumaai/lm-eval-harness-integration\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 6 media file(s), 12 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240526_081334-nxkdug7d/logs\u001b[0m\n",
      "(/home/thsch026/my-envs/eval) \n"
     ]
    }
   ],
   "source": [
    "lm_eval --model hf  --model_args pretrained=$MODEL,tokenizer=$MODEL  --tasks arc_challenge,hellaswag,truthfulqa_mc2,winogrande,gsm8k \\\n",
    "        --device cuda:2 --batch_size auto --wandb_args project=lm-eval-harness-integration --log_samples \\\n",
    "        --output_path \"/home/thsch026/masterarbeit/experiment/out_wan\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62d91dd-f69d-4313-9ee7-d558b722e5b8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## lm_eval HELP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9d98dba-8c95-4ec5-8863-7309a69e12c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: lm_eval [-h] [--model MODEL] [--tasks task1,task2]\n",
      "               [--model_args MODEL_ARGS] [--num_fewshot N]\n",
      "               [--batch_size auto|auto:N|N] [--max_batch_size N]\n",
      "               [--device DEVICE] [--output_path DIR|DIR/file.json]\n",
      "               [--limit N|0<N<1] [--use_cache DIR]\n",
      "               [--cache_requests {true,refresh,delete}] [--check_integrity]\n",
      "               [--write_out] [--log_samples] [--show_config]\n",
      "               [--include_path DIR] [--gen_kwargs GEN_KWARGS]\n",
      "               [--verbosity CRITICAL|ERROR|WARNING|INFO|DEBUG]\n",
      "               [--wandb_args WANDB_ARGS] [--hf_hub_log_args HF_HUB_LOG_ARGS]\n",
      "               [--predict_only] [--seed SEED] [--trust_remote_code]\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "  --model MODEL, -m MODEL\n",
      "                        Name of model e.g. `hf`\n",
      "  --tasks task1,task2, -t task1,task2\n",
      "                        To get full list of tasks, use the command lm-eval --tasks list\n",
      "  --model_args MODEL_ARGS, -a MODEL_ARGS\n",
      "                        Comma separated string arguments for model, e.g. `pretrained=EleutherAI/pythia-160m,dtype=float32`\n",
      "  --num_fewshot N, -f N\n",
      "                        Number of examples in few-shot context\n",
      "  --batch_size auto|auto:N|N, -b auto|auto:N|N\n",
      "                        Acceptable values are 'auto', 'auto:N' or N, where N is an integer. Default 1.\n",
      "  --max_batch_size N    Maximal batch size to try with --batch_size auto.\n",
      "  --device DEVICE       Device to use (e.g. cuda, cuda:0, cpu).\n",
      "  --output_path DIR|DIR/file.json, -o DIR|DIR/file.json\n",
      "                        The path to the output file where the result metrics will be saved. If the path is a directory and log_samples is true, the results will be saved in the directory. Else the parent directory will be used.\n",
      "  --limit N|0<N<1, -L N|0<N<1\n",
      "                        Limit the number of examples per task. If <1, limit is a percentage of the total number of examples.\n",
      "  --use_cache DIR, -c DIR\n",
      "                        A path to a sqlite db file for caching model responses. `None` if not caching.\n",
      "  --cache_requests {true,refresh,delete}\n",
      "                        Speed up evaluation by caching the building of dataset requests. `None` if not caching.\n",
      "  --check_integrity     Whether to run the relevant part of the test suite for the tasks.\n",
      "  --write_out, -w       Prints the prompt for the first few documents.\n",
      "  --log_samples, -s     If True, write out all model outputs and documents for per-sample measurement and post-hoc analysis. Use with --output_path.\n",
      "  --show_config         If True, shows the the full config of all tasks at the end of the evaluation.\n",
      "  --include_path DIR    Additional path to include if there are external tasks to include.\n",
      "  --gen_kwargs GEN_KWARGS\n",
      "                        String arguments for model generation on greedy_until tasks, e.g. `temperature=0,top_k=0,top_p=0`.\n",
      "  --verbosity CRITICAL|ERROR|WARNING|INFO|DEBUG, -v CRITICAL|ERROR|WARNING|INFO|DEBUG\n",
      "                        Controls the reported logging error level. Set to DEBUG when testing + adding new task configurations for comprehensive log output.\n",
      "  --wandb_args WANDB_ARGS\n",
      "                        Comma separated string arguments passed to wandb.init, e.g. `project=lm-eval,job_type=eval\n",
      "  --hf_hub_log_args HF_HUB_LOG_ARGS\n",
      "                        Comma separated string arguments passed to Hugging Face Hub's log function, e.g. `hub_results_org=EleutherAI,hub_repo_name=lm-eval-results`\n",
      "  --predict_only, -x    Use with --log_samples. Only model outputs will be saved and metrics will not be evaluated.\n",
      "  --seed SEED           Set seed for python's random, numpy, torch, and fewshot sampling.\n",
      "                        Accepts a comma-separated list of 4 values for python's random, numpy, torch, and fewshot sampling seeds, respectively, or a single integer to set the same seed for all three.\n",
      "                        The values are either an integer or 'None' to not set the seed. Default is `0,1234,1234,1234` (for backward compatibility).\n",
      "                        E.g. `--seed 0,None,8,52` sets `random.seed(0)`, `torch.manual_seed(8)`, and fewshot sampling seed to 52. Here numpy's seed is not set since the second value is `None`.\n",
      "                        E.g, `--seed 42` sets all four seeds to 42.\n",
      "  --trust_remote_code   Sets trust_remote_code to True to execute code to create HF Datasets from the Hub\n",
      "(/home/thsch026/my-envs/eval) \n"
     ]
    }
   ],
   "source": [
    "lm_eval --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4b74b5-2e8f-4f5a-9ddf-30e35273e619",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
