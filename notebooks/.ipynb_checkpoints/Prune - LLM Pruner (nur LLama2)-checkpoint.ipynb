{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd279318-8e81-423e-9645-b3999d5b5924",
   "metadata": {},
   "source": [
    "# LLM Pruner\n",
    "- Pruning eines lokalen Modells"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689bb4a1-f721-4ca9-baa3-9019a9883b29",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Vorbereitung\n",
    "- Use Kernel \"prune\"\n",
    "- be sure that torch==2.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabe341f-9d9c-46ce-a002-93d3ea0e3946",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# torch muss noch mal installiert werden\n",
    "!pip install torch==2.0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b397989-df8f-43de-b51f-8c74fecdd8d3",
   "metadata": {},
   "source": [
    "## Run pruning Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d651ffb4-d5f3-4ec7-87a3-cea876cd2dd7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/thsch026/masterarbeit/notebooks/../experiment/LLM-Pruner/hf_prune.py\", line 13, in <module>\n",
      "    from transformers import LlamaTokenizer, GenerationConfig, LlamaConfig\n",
      "ImportError: cannot import name 'LlamaTokenizer' from 'transformers' (/opt/conda/lib/python3.10/site-packages/transformers/__init__.py)\n"
     ]
    }
   ],
   "source": [
    "!python ../experiment/LLM-Pruner/hf_prune.py --pruning_ratio 0.25 \\\n",
    "      --block_wise \\\n",
    "      --block_mlp_layer_start 4 --block_mlp_layer_end 30 \\\n",
    "      --block_attention_layer_start 4 --block_attention_layer_end 30 \\\n",
    "      --pruner_type taylor \\\n",
    "      --test_after_train \\\n",
    "      --device cuda  --eval_device cuda \\\n",
    "      --save_ckpt_log_name llama3_8B \\\n",
    "     --base_model '/home/thsch026/masterarbeit/models/llama2/llama-2-7b-chat-hf' \\\n",
    "    --save_model \n",
    "\n",
    "\n",
    "   #--base_model '/home/thsch026/masterarbeit/models/llama3/Meta-Llama-3-8B-Instruct' \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c50d7de-025a-42a2-bc9c-307682c57d83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prune",
   "language": "python",
   "name": "prune"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
