{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "493334c9-5d20-41bd-abfc-333b6bebaacd",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Qlora for Llama3. runs with \"qlora3\" kernel\n",
    "- Source: https://medium.com/@avishekpaul31/fine-tuning-llama-3-8b-instruct-qlora-using-low-cost-resources-89075e0dfa04\n",
    "- Source: https://github.com/AvisP/LM_Finetune/blob/main/llama-3-finetune-qlora.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b58dbf-35a3-43b4-8852-15ae3b274d95",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setup for kernel (qlora3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc5b723-0043-433b-ac04-b8d527b13b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# workaround for packaging fehler\n",
    "pip install setuptools==69.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530eb66b-52f5-499c-8c42-628e5884b0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch==2.0.1\n",
    "!pip install bitsandbytes\n",
    "!pip install -U transformers[torch] datasets\n",
    "!pip install -q bitsandbytes trl peft accelerate\n",
    "!pip install flash-attn --no-build-isolation\n",
    "!pip install transformers==4.40.2 # Es geht nur genau diese Version!!\n",
    "!pip install trl\n",
    "!pip install autoawq\n",
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cc0704-8c66-45bc-9a54-4e4be5775b28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install sentencepiece\n",
    "!pip install mistral_inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46074ac4-8243-4eb7-b1d5-94e1ece5afe9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!conda list | grep trl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f713146b-6adf-48e9-9ca8-709d86d33c37",
   "metadata": {},
   "source": [
    "## Qlora Training Procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a889b1c-59fd-4437-a61e-c9faf6523d01",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Set GPU and import Libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5eac015a-2721-4e8b-89e2-4674158c1903",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:256\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "!echo $CUDA_VISIBLE_DEVICES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "92a8e1d0-32ad-489e-9f6d-b07c3c4beff4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "#notebook_login()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75387894-b2b7-454f-b8c5-d9bf745b2afc",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Set Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222b86be-979e-452f-b8fb-79e217c4c3e6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### LLama-3-8B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a0909ca-6980-4ed2-b0c6-2a4ff6692550",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_id = \"/home/thsch026/masterarbeit/models/generated/prune/shortened-llm/Meta-LLama-3-8B-instruct\"\n",
    "\n",
    "# Storage Location\n",
    "trained_model_id = \"Meta-Llama-3-8B-Instruct_prune_qlora\"\n",
    "output_dir = '/home/thsch026/masterarbeit/models/generated/qlora/' + trained_model_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7facbf-c489-4998-937e-3e673615fd98",
   "metadata": {},
   "source": [
    "### LLama 3 8 B Instruct pruned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "951cf6e1-11e5-4b75-b5df-33548cba7dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "# Storage Location\n",
    "trained_model_id = \"Meta-Llama-3-8B-Instruct_qlora\"\n",
    "output_dir = '/home/thsch026/masterarbeit/models/generated/qlora/' + trained_model_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4fd613-aa1b-4b13-9d24-0c4acc760189",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Mistral 7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2409fa9d-c159-40d7-b02e-495f28ee258a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "# Storage Location\n",
    "trained_model_id = \"Mistral-7B-Instruct-v0.2_qlora\"\n",
    "output_dir = '/home/thsch026/masterarbeit/models/generated/qlora/' + trained_model_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe8f7b1-b540-40a1-ad6b-c04cdc8f34b2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Llama-2-7b-chat-hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4092b176-0903-4a48-9a13-036f310cd6fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "# Storage Location\n",
    "trained_model_id = \"Llama-2-7b-chat-hf_qlora\"\n",
    "output_dir = '/home/thsch026/masterarbeit/models/generated/qlora/' + trained_model_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c24a69-f2e2-401a-8457-2e3842f6ee98",
   "metadata": {},
   "source": [
    "### Load Model and choose GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "83a31a82-9158-420b-b960-5a4839043c66",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thsch026/my-envs/qlora3/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/thsch026/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/e1945c40cd546c78e41f1151f4db032b271faeaa/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.40.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /home/thsch026/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/e1945c40cd546c78e41f1151f4db032b271faeaa/model.safetensors.index.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128009\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99553d656cb5419c831f46bfbd844b09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Meta-Llama-3-8B-Instruct.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at /home/thsch026/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/e1945c40cd546c78e41f1151f4db032b271faeaa/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128009\n",
      "  ],\n",
      "  \"max_length\": 4096,\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.9\n",
      "}\n",
      "\n",
      "loading file tokenizer.json from cache at /home/thsch026/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/e1945c40cd546c78e41f1151f4db032b271faeaa/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /home/thsch026/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/e1945c40cd546c78e41f1151f4db032b271faeaa/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /home/thsch026/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/e1945c40cd546c78e41f1151f4db032b271faeaa/tokenizer_config.json\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f247ca16-4229-4e4e-9451-a03ee24b1073",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load Dataset and trim for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b4b83eea-46fa-4540-a8b3-5f0b14b9d532",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# based on config\n",
    "raw_datasets = load_dataset(\"HuggingFaceH4/ultrachat_200k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "67e0ba5b-0453-4575-b1b9-ab927db6d910",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['prompt', 'prompt_id', 'messages'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['prompt', 'prompt_id', 'messages'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "# remove this when done debugging to include whole dataset\n",
    "indices = range(0,10000)\n",
    "\n",
    "dataset_dict = {\"train\": raw_datasets[\"train_sft\"].select(indices),\n",
    "                \"test\": raw_datasets[\"test_sft\"].select(indices)}\n",
    "\n",
    "#dataset_dict = {\"train\": raw_datasets[\"train_sft\"],\n",
    "  #              \"test\": raw_datasets[\"test_sft\"]}\n",
    "\n",
    "raw_datasets = DatasetDict(dataset_dict)\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4cad6c45-6a5d-488d-ae87-b8ff878e7724",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user                :  These instructions apply to section-based themes (Responsive 6.0+, Retina 4.0+, Parallax 3.0+ Turbo 2.0+, Mobilia 5.0+). What theme version am I using?\n",
      "On your Collections pages & Featured Collections sections, you can easily show the secondary image of a product on hover by enabling one of the theme's built-in settings!\n",
      "Your Collection pages & Featured Collections sections will now display the secondary product image just by hovering over that product image thumbnail.\n",
      "Does this feature apply to all sections of the theme or just specific ones as listed in the text material?\n",
      "assistant           :  This feature only applies to Collection pages and Featured Collections sections of the section-based themes listed in the text material.\n",
      "user                :  Can you guide me through the process of enabling the secondary image hover feature on my Collection pages and Featured Collections sections?\n",
      "assistant           :  Sure, here are the steps to enable the secondary image hover feature on your Collection pages and Featured Collections sections:\n",
      "\n",
      "1. Log in to your Shopify account and go to your Online Store.\n",
      "2. Click on Customize theme for the section-based theme you are using.\n",
      "3. Navigate to the Collection pages or Featured Collections section where you want to enable the secondary image hover feature.\n",
      "4. With the section open, click on the section's setting (gear) icon in the top-left corner.\n",
      "5. In the settings panel that appears, look for an option labeled 'Image display' or 'Image hover'.\n",
      "6. If available, select 'Show secondary image on hover'.\n",
      "7. Save the changes and preview the Collection/Featured Collection page to see the effect.\n",
      "\n",
      "If you're having trouble finding the setting, the best thing to do is refer to your theme's documentation, since the location and labeling of settings can vary between themes.\n",
      "user                :  Can you provide me with a link to the documentation for my theme?\n",
      "assistant           :  I don't have access to your store's theme information. However, you can usually find the documentation for your theme by going to the shopify theme store, finding your theme and clicking on the 'support' link located in the bottom right corner of the page. Alternatively, you can do a google search for the name of your theme followed by 'documentation' or 'user guide'.\n",
      "user                :  Can you confirm if this feature also works for the Quick Shop section of my theme?\n",
      "assistant           :  The secondary image hover feature may or may not work for your Quick Shop section, depending on the configuration of your theme. Some themes include this feature in the Quick Shop section by default, while others may require additional customization. To check if this feature is available for the Quick Shop section of your theme, follow these steps:\n",
      "\n",
      "1. Go to the Quick Shop section where you would like to enable the feature. 2. Click on the Quick Shop settings icon (gear icon) and look for 'Image display' or 'Image hover'. 3. If available, select 'Show secondary image on hover'. 4. Save the changes. If this option is not available in your Quick Shop section settings, you may need to reach out to your theme developer for assistance with customizing your Quick Shop section to include this feature.\n"
     ]
    }
   ],
   "source": [
    "example = raw_datasets[\"train\"][0]\n",
    "messages = example[\"messages\"]\n",
    "for message in messages:\n",
    "  role = message[\"role\"]\n",
    "  content = message[\"content\"]\n",
    "  print('{0:20}:  {1}'.format(role, content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0876077e-e22b-4691-8900-52ca3d3ecdde",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128009"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2b51cae3-1486-43b0-bddd-60e40bceca16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "064303d1-aa7c-4c83-9b29-c6a475312341",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "def apply_chat_template(example, tokenizer):\n",
    "    messages = example[\"messages\"]\n",
    "    # We add an empty system message if there is none\n",
    "    #if messages[0][\"role\"] != \"system\":\n",
    "     #   messages.insert(0, {\"role\": \"system\", \"content\": \"\"})\n",
    "    example[\"text\"] = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "35e13907-bdbc-4a94-810d-076314565511",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "column_names = list(raw_datasets[\"train\"].features)\n",
    "raw_datasets = raw_datasets.map(apply_chat_template,\n",
    "                                num_proc=cpu_count(),\n",
    "                                fn_kwargs={\"tokenizer\": tokenizer},\n",
    "                                remove_columns=column_names,\n",
    "                                desc=\"Applying chat template\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bf7a7737-31e6-48f1-8aa7-770f001917ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set pad_token_id equal to the eos_token_id if not set\n",
    "if tokenizer.pad_token_id is None:\n",
    "  tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Set reasonable default for models without max length\n",
    "if tokenizer.model_max_length > 100_000:\n",
    "  tokenizer.model_max_length = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7c88d141-2dd3-4fc4-9132-d55ba4cc2be3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1824 of the processed training set:\n",
      "\n",
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Create a clean and easy-to-use user interface for a meal planning app that allows users to input their dietary restrictions and preferences, such as vegetarian, gluten-free, or low-carb, and generates meal options that meet those requirements while also providing detailed nutritional information such as calorie count and macronutrient breakdown. How would you display this information in an intuitive and visually appealing way? What design elements would you use to make the app accessible and user-friendly for all types of users?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "The user interface for this meal planning app would prioritize simplicity and clarity of information. Using a minimalistic design approach, the interface would feature a clean and bright color scheme, and it would be divided into several sections:\n",
      "\n",
      "1. Home screen: The home screen would welcome the user and provide a brief description of the app features. The home screen would also have a search bar to allow users to search meal options by keywords or ingredients.\n",
      "\n",
      "2. Meal planning options: This section would allow users to filter their dietary restrictions and preferences, such as vegetarian, gluten-free, or low-carb. The options would be presented as checkboxes with a clear description next to each.\n",
      "\n",
      "3. Meal selection: Once the user has selected their meal planning options, the app would generate meal options that meet their dietary requirements. The meals would be displayed as thumbnail images with the name of the dish and a brief description of the preparation example. Clicking on a meal would show users more information, including the detailed nutritional information such as calorie count and macronutrient breakdown.\n",
      "\n",
      "4. Shopping list and meal planner: The app would provide users with the option of adding meals to their shopping list or meal planner with a tap or a swipe.\n",
      "\n",
      "To make the app accessible and user-friendly for all types of users, the design would prioritize ease of navigation with large and visible buttons, font sizes and simple language. The app would also include intuitive icons to help users understand the functions in each section. Finally, the app would use simple animations to enhance user experience, such as subtle fade-ins or slide-ins of meal thumbnails.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Can you add some more information about how the app will suggest meal options based on user preferences? Will it suggest recipes, or will it suggest pre-planned meals? Also, will the app allow users to customize the suggested meals to their liking or add their own recipes?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Of course, happy to provide more information! Based on the user's dietary restrictions and preferences, the app will suggest meal options that meet those specific requirements. These meal options can either be pre-planned recipes that the app generates, or they can be recipes that the app recommends from external sources, such as popular recipe websites or blogs. For pre-planned meals, the app could suggest meal plans for entire weeks or months, with the option to customize the suggested plans by swapping out ingredients or days for others. The pre-planned meals could also come with the option to scale the recipe up or down depending on the number of servings required. For recipe suggestions, the app could show a selection of recipes and allow the user to filter and sort based on their dietary restrictions and preferences. Once the user selects a recipe, they could see the detailed nutritional information, ingredients, and instructions. Users would also have the option to customize the suggested meals further by adding their own recipes or substituting ingredients based on their likes, dislikes, or allergies. The app would provide users the ability to create custom meal plans using their own recipes or a combination of the app's suggestions and their personal recipes. Overall, the app aims to provide users with an all-in-one meal planning and recipe management solution tailored to their specific dietary requirements and preferences, offering both pre-planned meals and recipe recommendations while also giving them the flexibility to make changes according to their personal tastes and preferences.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "I'm really interested in the meal planning app, but I'm wondering if there's a way that it could also suggest meals based on ingredients that I already have on hand. Is that something that the app could do? Also, can you provide more details on how the meal planner feature would work, and whether it would allow users to save their favorite meal plans for future reference?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, the app can suggest meals based on the ingredients the user already has on hand. The app could either ask the user to input the list of ingredients they have, or the user could scan the items using their phone camera. The app would then suggest meals that include one or more of the ingredients the user has selected. This feature would help users use up what they have before buying more ingredients, reduce food waste, and also save money on groceries. Regarding the meal planner feature, once users have chosen their meals, they can add them to the meal planner, which would organize their meals by day. The meal planner would show the user a visual representation of their daily and weekly meals, with the option to view the nutritional information for each meal. Users could easily drag and drop meals from day to day as they plan their meals for the week, and the app would automatically adjust the shopping list based on the changes made. Users can also save their favorite meals and meal plans for future reference, making it easier for them to plan meals in the future without going through the selection process again. The app's calendar feature could show users all of their saved meal plans and let them access the nutritional information for each meal easily. In summary, the meal planning app would not only suggest meals based on a user's dietary preferences, but also incorporate the ingredients they already have on hand. Users can then easily add their selected meals to the meal planner and save their favorite meal plans for future reference, increasing the app's convenience and user-friendliness.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "I really love the features you've described for this meal planning app. Could you add more detailed information about how the app will provide nutritional information for each meal option? I'm interested in knowing how the app will break down the macronutrients in each meal to help me plan my meals better.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Sure! The app would provide detailed nutritional information for each selected meal, including a breakdown of macronutrients like carbohydrates, fats, and proteins. This information would allow users to make informed decisions about their meals and help them track their daily nutrient intake. To make it easier for users to see this information, we could include a nutrition facts panel for each meal option on the app. The panel would show the amount of macronutrients present in a single serving size of the meal. Users would also be able to see the total calories, fiber content, sugars, and other essential nutritional information for the meal in question. Additionally, the app could have a more expanded view for nutrition information, where users could see more detailed information on the macronutrient breakdown in the meal. This expanded view could be accessible by clicking on a \"Nutrition Details\" button. The app could also have a \"nutrient tracker\" feature that allows users to input their daily calorie intake and macronutrient targets. Once these targets have been set, the app would then display a breakdown of the nutrient content of each meal option, based on the user's set parameters. Additionally, users would have the ability to modify their macronutrient targets so they can adjust their meals depending on their personal goals. In summary, the meal planning app would provide detailed nutritional information, including a breakdown of macronutrients in each meal. This information would be accessible through a nutrition fact panel and an expanded view. The app also integrates a nutrient tracker feature to help users stay on track with their dietary goals.<|eot_id|>\n",
      "#####################################\n",
      "Sample 409 of the processed training set:\n",
      "\n",
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Create a comprehensive event marketing plan that highlights the unique features and benefits of a B2B product, while focusing on lead generation and ROI. The plan should include a detailed timeline, budget, promotional strategies, and metrics for success. Use a mix of digital and offline tactics, such as email marketing, social media, content marketing, trade shows, and sponsorships, to reach the target audience and drive attendance. Consider the competitive landscape, customer personas, and industry trends when crafting the messaging and positioning of the product. Provide clear guidelines for the sales team to follow up with leads and maximize conversion rates.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Executive Summary:\n",
      "\n",
      "Our B2B product is a specialized software solution that streamlines business operations and improves efficiency. The product is designed for SMBs and Enterprise-level companies and offers a range of features, such as inventory management, CRM integration, and data analytics, to improve productivity and drive revenue growth. Our goal is to generate leads and increase sales, with a focus on ROI, using a combination of digital and offline marketing tactics.\n",
      "\n",
      "Timeline:\n",
      "\n",
      "- Month 1: Research and planning\n",
      "- Month 2: Content creation\n",
      "- Month 3-4: Pre-event promotion\n",
      "- Month 5: Event(s)\n",
      "- Month 6: Post-event follow-up and analysis\n",
      "\n",
      "Budget:\n",
      "\n",
      "- Trade show booth rental: $10,000\n",
      "- Promotional materials: $5,000\n",
      "- Travel and lodging for event staff: $3,000\n",
      "- Email marketing platform: $2,500\n",
      "- Social media advertising: $5,000\n",
      "- Content creation: $7,500\n",
      "\n",
      "Total budget: $33,000\n",
      "\n",
      "Promotional Strategies:\n",
      "\n",
      "1. Trade Show Sponsorship: We will sponsor two trade shows in our industry that attract a high volume of potential customers. We will secure a prominent booth location and create interactive displays and demos to showcase the unique features of our product.\n",
      "\n",
      "2. Email Marketing: We will create email campaigns that introduce our product to potential customers before the event(s). The emails will be targeted at decision-makers and will highlight the benefits of our software solution, such as increased efficiency, data management, and customer engagement.\n",
      "\n",
      "3. Social Media Advertising: We will use social media platforms, such as LinkedIn and Twitter, to promote our product and the trade show(s) we are attending. We will create targeted ads that reach decision-makers in our industry, focusing on the unique features and benefits of our product.\n",
      "\n",
      "4. Content Marketing: We will create a series of blog posts, videos, and infographics that explain the features and benefits of our product. We will share this content on our website, social media, and email campaigns to establish thought leadership, increase awareness, and generate interest in our product.\n",
      "\n",
      "Metrics for Success:\n",
      "\n",
      "1. Number of Leads: Our main objective is to generate leads, and we will track the number of leads generated as a direct result of our event marketing plan.\n",
      "\n",
      "2. Conversion Rates: We will track the conversion rates of leads to sales and use this data to improve our follow-up strategy and maximize conversion rates.\n",
      "\n",
      "3. ROI: We will track the ROI of our event marketing plan by comparing the costs of our marketing efforts to the revenue generated from sales.\n",
      "\n",
      "4. Brand Awareness: We will track the increase in brand awareness and reputation that results from our marketing efforts, including social media engagement, website traffic, and media coverage.\n",
      "\n",
      "Sales Team Guidelines:\n",
      "\n",
      "We will provide the sales team with a clear set of guidelines for following up with leads after the event. These guidelines will include:\n",
      "\n",
      "1. Personalized Follow-Up: Each lead will receive a personalized follow-up email or phone call within 48 hours of the event.\n",
      "\n",
      "2. Product Demo: The sales team will schedule a product demo with each lead to show how our software solution can improve their business operations.\n",
      "\n",
      "3. Follow-Up Schedule: The sales team will follow up with leads at regular intervals to maintain engagement and build relationships.\n",
      "\n",
      "In conclusion, our event marketing plan will effectively promote our B2B product, generate leads, and increase sales. By using a mix of digital and offline marketing tactics, we will reach decision-makers and showcase the unique features and benefits of our software solution. We will track key metrics and provide clear guidelines for the sales team to maximize conversion rates and ensure the success of our event marketing plan.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "This looks like a good start, but I was wondering if you could add some more information about our target audience and how these marketing strategies will specifically appeal to them? It would be great to know what their pain points are that our software solution can address.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Sure, that's a great point. Let me add some details about our target audience and how our marketing strategies will address their pain points.\n",
      "\n",
      "Our target audience is small to medium-sized businesses (SMBs) and enterprise-level companies in various industries, including manufacturing, retail, and healthcare. These businesses typically have complex operations that require streamlined processes and data management to improve efficiency and increase profitability.\n",
      "\n",
      "Through our research, we've identified that our target audience faces several pain points, such as:\n",
      "\n",
      "1. Limited resources: Many SMBs often have limited resources, including staff and budgets, which impact their ability to manage complex operations effectively.\n",
      "\n",
      "2. Disconnected data: SMBs often have customer, inventory, and sales data stored in different systems, making it challenging to gain insights into their operations and make informed decisions.\n",
      "\n",
      "3. Inefficient processes: Many SMBs have manual processes that are time-consuming and prone to errors, leading to delays and cost overruns.\n",
      "\n",
      "Our software solution addresses these pain points by providing a centralized system that streamlines operations, integrates data from multiple sources, and automates key processes. Our marketing strategies will highlight these benefits to our target audience through various channels, including:\n",
      "\n",
      "1. Trade Show Sponsorship: We will engage attendees at the trade shows by showcasing hands-on experience of our software solution. We will showcase both the simplicity and efficiency of our software, allowing attendees to see how our product can improve efficiency and eliminate the challenges they currently face.\n",
      "\n",
      "2. Email Marketing: We will create personalized email campaigns that speak to each customer's specific pain points. Our emails will explain how our product can offer a solution to their problems and provide a clear understanding of how our software works.\n",
      "\n",
      "3. Social Media Advertising: Our targeted ads will address specific pain points, and we’ll highlight our unique selling proposition (USP), which is that we provide a customized solution tailored to each business’s unique needs.\n",
      "\n",
      "4. Content Marketing: We will create blogs, whitepapers, infographics that highlight different case studies and use cases on how our software has worked for other businesses with similar pain points.\n",
      "\n",
      "By addressing the specific pain points of our target audience, our marketing strategies will be effective in generating interest and leads for our software solution. We will deliver a clear and persuasive message that conveys the value of our product and how it can help our target audience achieve their business goals.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Thanks for adding the information about our target audience and how our marketing strategies will address their pain points. Can you also suggest some creative ways to make our trade show booth more interactive and engaging for attendees? I want to ensure that we stand out from the competition and make a lasting impression on our potential customers.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Sure, I'd be happy to suggest some creative ideas to make your trade show booth more interactive and engaging for attendees:\n",
      "\n",
      "1. Demo Stations: Set up interactive demo stations where attendees can try out different features of your software solution. This will provide them with a hands-on experience of your product, allowing them to understand it better, and also help them visualize the benefits it provides.\n",
      "\n",
      "2. Gamification: Create a fun game related to your product that attendees can participate in during the trade show. For example, you could set up a scavenger hunt at the trade show that requires attendees to visit different booths and complete tasks related to your software solution. This will help attendees remember your product even after the trade show.\n",
      "\n",
      "3. Customized Visuals: Have visual aids like screens displaying your software in action or customer testimonials. Make it visual, creative, and easy to digest to attract the attention of attendees.\n",
      "\n",
      "4. Giveaways: Offer customized giveaways like branded merchandise and swag bags that attendees can take home. This will ensure that attendees remember your product, and it could increase the likelihood of them following up with you after the trade show.\n",
      "\n",
      "5. Video Displays: Set up flat screen televisions at your booth with a video explaining the benefits of your software solution. People are naturally attracted to moving visuals and will be drawn to your booth. \n",
      "\n",
      "6. Prize Draws: Host a contest for attendees, like a prize draw, by offering a chance to win a membership or discount for our product, etc. This creates engagement and creates a buzz that excites attendees.\n",
      "\n",
      "By incorporating these creative ideas into your trade show booth design, you will stand out from your competitors and make a lasting impression on potential customers. These efforts will create a memorable experience and generate interest and leads for your software solution.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "This is great! I really appreciate your suggestions for making our trade show booth more engaging. Can you provide more details on the scavenger hunt idea? How can we make it more interactive and relevant to our product?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Sure, let me elaborate more on the scavenger hunt idea and how we can make it interactive and relevant to our product.\n",
      "\n",
      "The scavenger hunt is a fun and creative way to engage attendees and promote our software solution. Here's how we can make it more interactive and relevant:\n",
      "\n",
      "1. Create tasks related to our product's unique features: Develop tasks that relate directly to our software solution; for instance, \"find a question that comes to mind when it comes to inventory management.\" These tasks will allow attendees to interact with your product in a more meaningful way and provide them with a better understanding of how it works.\n",
      "\n",
      "2. Incorporate social media: Promote the scavenger hunt on social media using a unique branding hashtag, create a leaderboard where attendees can track their progress, and engage through social media for chances to win prizes.\n",
      "\n",
      "3. Partner with other vendors: Partner with other vendors who have complementary services or products to our solution to help with the tasks. This helps showcase the partnership and how it is beneficial. \n",
      "\n",
      "4. Offer attractive prizes: To make the scavenger hunt more engaging, offer attractive prizes to encourage participation. Make sure prizes are relevant to the value of the work attendees will put in.\n",
      "\n",
      "5. Make it memorable: Take advantage of hints by making it more challenging and unexpected so that attendees will remember the fun they had participating in the activity.\n",
      "\n",
      "By making our scavenger hunt interactive and relevant to our product, we can engage attendees in a way that creates a memorable experience and encourages them to engage with our company. We can also use the scavenger hunt to collect contact information from attendees, which we can use for follow-up and lead generation.<|eot_id|>\n",
      "#####################################\n",
      "Sample 4506 of the processed training set:\n",
      "\n",
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Write a detailed step-by-step guide on how to make your own hot sauce from scratch, including a list of ingredients, recommended equipment, safety precautions, and tips for adjusting the heat level and flavor to suit your preferences. The guide should be accessible to beginners but also provide enough detail for experienced cooks to experiment with the recipe. The writing style should be clear, concise, and engaging, with supporting photos or videos as needed.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Introduction:\n",
      "\n",
      "A homemade hot sauce has a flavor profile that cannot be replicated by any store-bought brand. Plus, you can customize it to suit your preferences, making it mild or fiery hot. Making hot sauce may seem daunting, but it is incredibly easy and worth the effort. In this guide, we will take you step by step through the process of making your own hot sauce from scratch.\n",
      "\n",
      "Ingredients:\n",
      "\n",
      "- 20-25 hot peppers (Choose your favorite variety - jalapeno, habanero, birds-eye, etc.)\n",
      "- 3-4 cloves of garlic, chopped\n",
      "- 1 medium onion, chopped\n",
      "- 1 cup of vinegar (white or apple cider)\n",
      "- 2 teaspoons of salt\n",
      "- 1 teaspoon of sugar (optional)\n",
      "\n",
      "Equipment:\n",
      "\n",
      "- Cutting board\n",
      "- Chef's knife\n",
      "- Food processor or blender\n",
      "- Rubber gloves (to prevent irritating your skin from the heat of the peppers)\n",
      "- Saucepan\n",
      "- Funnel\n",
      "- Glass bottles with lids\n",
      "\n",
      "Safety Precautions:\n",
      "\n",
      "- When working with hot peppers or any spicy ingredient, make sure to wear gloves to avoid skin irritation.\n",
      "- Make sure to use a food processor or blender with a tight-fitting lid to prevent the hot sauce from splattering and burning you while blending.\n",
      "- Use caution while handling the hot sauce and do not touch your face or eyes.\n",
      "- Store the hot sauce in properly sterilized bottles with airtight lids.\n",
      "\n",
      "Step-by-Step Guide to Making Hot Sauce:\n",
      "\n",
      "Step 1: Prepare the Peppers\n",
      "\n",
      "Clean the peppers thoroughly and remove the stems. Slice the peppers into small pieces. The larger the pieces, the less evenly they will blend in the food processor.\n",
      "\n",
      "Step 2: Saute Onions and Garlic\n",
      "\n",
      "In a saucepan, sauté onions and garlic until the onions become translucent.\n",
      "\n",
      "Step 3: Blend the Ingredients\n",
      "\n",
      "Combine the sautéed onions and garlic with the sliced peppers, sugar (if using), vinegar, and salt in a blender or food processor. Pulse until the mixture turns into a smooth puree.\n",
      "\n",
      "Step 4: Taste and Adjust\n",
      "\n",
      "Taste the hot sauce to check if it requires more vinegar, salt, sugar, or heat. You can add more peppers to make the hot sauce spicier or more sugar to make it sweeter. Mix well in the blender or food processor until you get the desired consistency.\n",
      "\n",
      "Step 5: Bottle the Hot Sauce\n",
      "\n",
      "Using a funnel, pour the hot sauce into sterilized glass bottles. Shake the bottles gently to release any air bubbles. Leave about 1/2 inch of headspace at the top of the bottle.\n",
      "\n",
      "Step 6: Store and Enjoy\n",
      "\n",
      "Close the lid on the bottles tightly and store them in a cool, dark place. The hot sauce will keep for several months. Use liberally on tacos, pizza, or any other dish that could use a little extra kick!\n",
      "\n",
      "Tips for Adjusting Heat and Flavor:\n",
      "\n",
      "- The heat of the hot sauce can be adjusted by the type and quantity of peppers you use. Habañero peppers are the hottest, followed by jalapeños and serranos. Experiment with different combinations to find the heat level that suits your taste.\n",
      "- If you prefer a milder hot sauce, remove the seeds and white membranes from the peppers before blending.\n",
      "- You can add other ingredients like ginger, honey, or fruit to balance and enhance the flavor.\n",
      "- If you accidentally make the hot sauce too spicy, you can temper it by adding more sugar or vinegar to mellow the heat.\n",
      "\n",
      "Conclusion:\n",
      "\n",
      "Making hot sauce from scratch is easy and fun, and you can customize it to your liking. By following this step-by-step guide, you will have homemade hot sauce that is spicy, tangy, and full of flavor. Whether you use it as a marinade, a condiment or a dip, it will add a kick to any dish you prepare. So, put on those gloves, grab your peppers, and start making your own hot sauce today!<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "It looks pretty straightforward, but can you provide some ideas on how to use the hot sauce? I'm not quite sure what dishes it would go well with.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Sure! Hot sauce is a versatile condiment that can be used in many ways to add flavor and heat to your dishes. Here are some popular uses for hot sauce:\n",
      "\n",
      "- Add a few drops to scrambled eggs or omelettes for a spicy breakfast. - Use it as a marinade for chicken, fish, or steak before grilling or baking. - Mix it with mayonnaise or sour cream to make a spicy dip for vegetables or chips. - Use it as a topping for pizza, tacos, or burgers. - Stir some into soups, stews or chilis to add some heat. - Drizzle it over roasted vegetables for a spicy side dish. - Use it as a condiment for sandwiches or wraps. - Mix it with ketchup or barbecue sauce for a spicy twist on your favorite sauces. The possibilities are endless! Just remember to use hot sauce sparingly, especially if you're not used to spicy foods. You can always adjust the heat level based on your taste preferences as well. Have fun experimenting with your homemade hot sauce!<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Could you also suggest some ways to make the hot sauce more unique or flavorful? I want to experiment with different ingredients and make it my own.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Of course! Here are some ideas for how to make your hot sauce more unique and flavorful:\n",
      "\n",
      "- Experiment with different types of peppers, as each one has its own unique flavor profile. For example, try using smoked paprika, chipotle, or ancho chilies to add a smoky flavor to your hot sauce. - Add some sweet and tangy notes to your hot sauce with ingredients like honey, lime juice, or vinegar. These can help balance out the heat and add some complexity to the flavor. - Incorporate fresh herbs like cilantro or parsley to add some freshness and brightness to the hot sauce. - Roast the peppers before blending them to add some depth and complexity to the flavor. - Add some fruit like mango or pineapple to the hot sauce for a sweet and spicy combo. - Try using different types of vinegar, like balsamic or red wine vinegar, to create a unique flavor profile. Remember, it's all about experimenting and finding the flavors you enjoy. Don't be afraid to mix and match ingredients to create your perfect hot sauce!<|eot_id|>\n",
      "#####################################\n"
     ]
    }
   ],
   "source": [
    "# create the splits\n",
    "train_dataset = raw_datasets[\"train\"]\n",
    "eval_dataset = raw_datasets[\"test\"]\n",
    "\n",
    "for index in random.sample(range(len(raw_datasets[\"train\"])), 3):\n",
    "  print(f\"Sample {index} of the processed training set:\\n\\n{raw_datasets['train'][index]['text']}\")\n",
    "  print(\"#####################################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c4989f1e-ce9c-4a50-97bf-91fa8eb8c354",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/thsch026/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/e1945c40cd546c78e41f1151f4db032b271faeaa/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /home/thsch026/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/e1945c40cd546c78e41f1151f4db032b271faeaa/model.safetensors.index.json\n",
      "Instantiating LlamaForCausalLM model under default dtype torch.float16.\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128009\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "988fba7a80984887b6da78cc1c9ab838",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Meta-Llama-3-8B-Instruct.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at /home/thsch026/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/e1945c40cd546c78e41f1151f4db032b271faeaa/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128009\n",
      "  ],\n",
      "  \"max_length\": 4096,\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.9\n",
      "}\n",
      "\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice to meet you! I'm a chatbot designed to assist and communicate with you in a friendly and helpful manner. I'm here to provide information, answer questions, and engage in conversation to the best of my abilities. I'm programmed to be polite, clear, and gentle in my responses, so please feel free to ask me anything or share your thoughts with me. How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a polite chatbot who responds clear and gentle.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "torch.backends.cuda.enable_flash_sdp(False)\n",
    "# https://github.com/Lightning-AI/litgpt/issues/327\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=128,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    ")\n",
    "\n",
    "response = outputs[0][input_ids.shape[-1]:]\n",
    "\n",
    "print(tokenizer.decode(response, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3418fd-e002-4441-ab0c-b93d7de78435",
   "metadata": {},
   "source": [
    "## Prepare Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "be076044-fbd3-4c55-9c0e-a6151816bcf8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from peft import LoraConfig\n",
    "from transformers import TrainingArguments\n",
    "from transformers import BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2506b1fc-80be-4b97-9a68-4ce7dcd04be0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# For 8 bit quantization\n",
    "quantization_config = BitsAndBytesConfig(load_in_8bit=True,\n",
    "                                        llm_int8_threshold=200.0)\n",
    "\n",
    "## For 4 bit quantization\n",
    "#quantization_config = BitsAndBytesConfig(\n",
    " #           load_in_4bit=True,\n",
    "  #          bnb_4bit_use_double_quant=True,\n",
    "   #         bnb_4bit_quant_type=\"nf4\",\n",
    "    #        bnb_4bit_compute_dtype=torch.bfloat16,)\n",
    "\n",
    "#model = AutoModelForCausalLM.from_pretrained(model_id, \n",
    " #                                            quantization_config=quantization_config,\n",
    "  #                                      device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7037ff39-f240-46f2-ae10-d74e312ad479",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_kwargs = dict(\n",
    "#     attn_implementation=False,#\"flash_attention_2\", # set this to True if your GPU supports it (Flash Attention drastically speeds up model computations)\n",
    "    torch_dtype=\"auto\",\n",
    "    use_cache=False, # set to False as we're going to use gradient checkpointing\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=quantization_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1d765181-6310-46c2-8264-25aa57857d39",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "/home/thsch026/my-envs/qlora3/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:166: UserWarning: You passed a model_id to the SFTTrainer. This will automatically create an `AutoModelForCausalLM` or a `PeftModel` (if you passed a `peft_config`) for you.\n",
      "  warnings.warn(\n",
      "/home/thsch026/my-envs/qlora3/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/thsch026/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/e1945c40cd546c78e41f1151f4db032b271faeaa/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.40.2\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /home/thsch026/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/e1945c40cd546c78e41f1151f4db032b271faeaa/model.safetensors.index.json\n",
      "Will use torch_dtype=torch.bfloat16 as defined in model's config object\n",
      "Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"use_cache\": false\n",
      "}\n",
      "\n",
      "target_dtype {target_dtype} is replaced by `torch.int8` for 8-bit BnB quantization\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "249c5646a38742ab823f138b87fcdabc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Meta-Llama-3-8B-Instruct.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at /home/thsch026/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/e1945c40cd546c78e41f1151f4db032b271faeaa/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128009\n",
      "  ],\n",
      "  \"max_length\": 4096,\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.9\n",
      "}\n",
      "\n",
      "PyTorch: setting up devices\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faaec25603634268a8afaad96d8239de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "918f6ea2f22b46ad989e4d90f37d03ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have loaded a model on multiple GPUs. `is_model_parallel` attribute will be force-set to `True` to avoid any unexpected behavior such as device placement mismatching.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# based on config\n",
    "training_args = TrainingArguments(\n",
    "    fp16=False, # specify bf16=True instead when training on GPUs that support bf16 else fp16\n",
    "    bf16=False,\n",
    "    do_eval=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    gradient_accumulation_steps=1,\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    learning_rate=2.0e-05,\n",
    "    log_level=\"info\",\n",
    "    logging_steps=5,\n",
    "    logging_strategy=\"steps\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    max_steps=-1,\n",
    "    num_train_epochs=3,\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    per_device_eval_batch_size=1, # originally set to 8\n",
    "    per_device_train_batch_size=1, # originally set to 8\n",
    "    push_to_hub=True,\n",
    "    hub_model_id=trained_model_id,\n",
    "    # hub_strategy=\"every_save\",\n",
    "    # report_to=\"tensorboard\",\n",
    "    report_to=\"none\",  # for skipping wandb logging\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=None,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "# based on config\n",
    "peft_config = LoraConfig(\n",
    "        r=64,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "        model=model_id,\n",
    "        model_init_kwargs=model_kwargs,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        dataset_text_field=\"text\",\n",
    "        tokenizer=tokenizer,\n",
    "        packing=True,\n",
    "        peft_config=peft_config,\n",
    "        max_seq_length=tokenizer.model_max_length,\n",
    "    )\n",
    "\n",
    "# To clear out cache for unsuccessful run\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db252239-0da0-4113-a68b-1e7d2f6a9049",
   "metadata": {},
   "source": [
    "## Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ba5ab3-16c9-49c6-9eb0-2f00d857246a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 5,747\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 17,241\n",
      "  Number of trainable parameters = 54,525,952\n",
      "/home/thsch026/my-envs/qlora3/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5004' max='17241' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 5004/17241 4:49:19 < 11:47:48, 0.29 it/s, Epoch 0.87/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_result = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45b9115-216f-4227-8adb-f70c02230e64",
   "metadata": {},
   "source": [
    "## Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a09105-5c5d-4b2c-86fa-84a7b91dc314",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(output_dir)\n",
    "model.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e285172-12f2-4c72-b84d-9c0c1544fa1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qlora3",
   "language": "python",
   "name": "qlora3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
