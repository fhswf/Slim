{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9179dd6e-b1f2-4f1c-abaa-330ed2225586",
   "metadata": {},
   "source": [
    "## Initialisieren eines Llama Modells für die Interferenz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f260f5-6863-4cde-a924-dc79b92435d9",
   "metadata": {},
   "source": [
    "### Installation der neuesten Version der transformer library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae310894-350d-4853-8089-32c7d8aca7d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# May you have to install the newest transformer version from github\n",
    "!pip install git+https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f6a9f3-661a-40d1-b2e4-50ab2ba86360",
   "metadata": {},
   "source": [
    "## Import der libraries und auswahl GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a415f4b8-70e0-43e0-afd1-0cd7706457aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0820bdbd-9814-425a-bb76-2eb9e607a846",
   "metadata": {},
   "source": [
    "## Laden des Modells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb575774-423d-4ef7-92e6-a8ddd8c253da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/thsch026/masterarbeit/notebooks\n"
     ]
    }
   ],
   "source": [
    "# Prüfen des aktuellen Pfads\n",
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a18ab3c1-f658-49b3-a63a-90a0de8c8810",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "798a50e0483549ec86564f2533e01d62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "# Pfad relativ zum Pfade des Notebooks\n",
    "\n",
    "# Pfade Llama 2\n",
    "model_dir = '../models/llama2/llama-2-7b-chat-hf'\n",
    "\n",
    "# Pfade Llama 3\n",
    "#model_dir = \"meta-llama/Meta-Llama-3-8B\"\n",
    "#model_dir = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "#model_dir = 'meta-llama/Meta-Llama-3-70B-Instruct'\n",
    "# Online laod\n",
    "model = LlamaForCausalLM.from_pretrained(model_dir)\n",
    "\n",
    "# Offline Load\n",
    "#model = LlamaForCausalLM.from_pretrained(model_dir, local_files_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8759b491-c00e-4ce2-811f-d34983856c87",
   "metadata": {},
   "source": [
    "## Speichern des Aktuell geladenen Modells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e8bf1206-1f6b-4419-9bb4-2b00fffe56b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(\"Meta-Llama-3-70B-Instruct-HF\")\n",
    "model.save_pretrained(\"Meta-Llama-3-70B-Instruct-HF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50bee64-954a-47f3-be01-627c4901c2cd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Laden des Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e36de4de-f53f-4cc7-a46c-5d156e2a40ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained(model_dir)\n",
    "#tokenizer = AutoTokenizer.from_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2cff99-e4bb-4764-b365-4af51abc0b1f",
   "metadata": {},
   "source": [
    "## Aufbau der pipeline\n",
    "Hier muss noch mal explizit die GPU angegeben werden!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c6c0d8-84e6-49f6-9808-c1fdc37dc296",
   "metadata": {},
   "source": [
    "### Pipeline mit tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c48f7ef5-a246-4cb7-9745-f8d0c2bcdfd7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-23 18:05:04.235565: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.float16,\n",
    "    device=\"cuda\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182f75cd-01f1-4e9c-879e-b8e55b7a15b2",
   "metadata": {},
   "source": [
    "## Eingabe des Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16c0e5b9-f3ee-4b15-887a-0a4238c9e6cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"why are bananas curved? \\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "74833e33-8e8e-4d6f-887c-cce1fa0adeea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "why are bananas curved? \n",
      "  *Why are bananas curved? It's due to the banana fruit growing on a special type of plant called a banana tree.\n",
      "  *The banana tree produces a stem with flowers at the top. When the flowers grow into a fruit, they grow in an upward direction.\n",
      "  *However, as the fruit grows, it becomes too heavy for the stem to support it. So, it starts to curve downward to reduce its weight and find support.\n",
      "  *This curving motion causes the fruit to grow into the familiar shape we know, with the tip of the banana pointing slightly downwards.\n",
      "  *So, to answer why bananas are curved, it's because they grow in such a way as to maximize their support and reduce their weight.\n",
      "\n",
      "---\n",
      "\n",
      "# Why are bananas curved?\n",
      "Why are bananas curved? \n",
      "  *Why are bananas curved? It's due to the banana fruit growing on a special type of plant called a banana tree.\n",
      "  *The banana tree produces a stem with flowers at the top. When the flowers grow into a fruit, they grow in an upward direction.\n",
      "  *However, as the fruit grows, it becomes too heavy for the stem to support it. So, it starts to curve downward to reduce its weight and find support.\n",
      "  *This curving motion causes the fruit to grow into the familiar shape we know, with the tip of the banana pointing slightly downwards.\n",
      "  *So, to answer why bananas are curved, it's because they grow in such a way as to maximize their support and reduce their weight.\n",
      "\n",
      "What do you think? Do you have any other questions about bananas or their growth? Let me know in the comments!\n",
      "\n",
      "---\n",
      "\n",
      "# Banana Facts \n",
      "Hey there, banana lovers!\n",
      "\n",
      "Here are some fascinating facts about bananas:\n",
      "\n",
      "1. **There are over 500 types of bananas!** Not just the Cavendish variety you're used to seeing in the supermarket.\n",
      "2. **Bananas are a great source of potassium** -\n"
     ]
    }
   ],
   "source": [
    "sequences = pipeline(prompt,\n",
    "\n",
    "do_sample=True,\n",
    "\n",
    "top_k=10,\n",
    "\n",
    "num_return_sequences=1,\n",
    "\n",
    "eos_token_id=tokenizer.eos_token_id,\n",
    "\n",
    "pad_token_id=2,\n",
    "\n",
    "max_length=400,\n",
    "\n",
    ")\n",
    "\n",
    "for seq in sequences:\n",
    "\n",
    "    print(f\"{seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0921dd0-080b-44d6-9676-6e453bf9faf8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
