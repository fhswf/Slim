{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1ca6860-4fc4-49e6-9177-68ba1ad095c4",
   "metadata": {},
   "source": [
    "# Transformation from Torch Script to Hugging face format\n",
    "- Tokenizer: Stelle sicher, dass du den richtigen Tokenizer für dein Modell verwendest. Dies ist oft der gleiche, der beim Training des Modells verwendet wurde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08493df7-6e93-466b-b34d-1ffb1e83c80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Lade dein Torch Script-Modell\n",
    "modelloc = \"/home/thsch026/masterarbeit/models/generated/awq/quant\"\n",
    "model = torch.jit.load(\"Meta-Llama-3-8B-Instruct-HF-w8-g128-awq-v2.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6d457e-38bb-4de1-b3b8-68172c554bd7",
   "metadata": {},
   "source": [
    "Erstelle eine Wrapper-Klasse für das Modell:\n",
    "Um das Modell in lm_eval verwenden zu können, musst du eine Wrapper-Klasse erstellen, die das Modell in das erforderliche Format bringt. Hier ist ein Beispiel für eine solche Wrapper-Klasse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f928f2-dc66-408a-a275-6113f93e1cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "class TorchScriptModelWrapper:\n",
    "    def __init__(self, model_path, tokenizer_name):\n",
    "        self.model = torch.jit.load(model_path)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "    \n",
    "    def __call__(self, inputs):\n",
    "        # Tokenisiere die Eingaben\n",
    "        inputs = self.tokenizer(inputs, return_tensors=\"pt\")\n",
    "        # Führe eine Vorwärtsausführung des Modells durch\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38305532-e872-4503-aa1b-15967adb7bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lade den Wrapper in lm_eval:\n",
    "Passe das lm_eval-Skript an, um dein benutzerdefiniertes Modell zu verwenden. Hier ist ein Beispiel, wie du das machen könntest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3667a6-c658-4863-870a-a9228828a601",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lm_eval.tasks import get_task\n",
    "from lm_eval.evaluator import simple_evaluate\n",
    "\n",
    "# Dein benutzerdefiniertes Modell und Tokenizer\n",
    "model_wrapper = TorchScriptModelWrapper(\"path_to_your_model.pt\", \"tokenizer_name\")\n",
    "\n",
    "# Wähle die Aufgabe, die du bewerten möchtest\n",
    "task = get_task(\"task_name\")\n",
    "\n",
    "# Bewerte das Modell mit der Aufgabe\n",
    "results = simple_evaluate(\n",
    "    model=model_wrapper,\n",
    "    tasks=[task],\n",
    "    num_fewshot=0  # Anzahl der Beispiele, die für Few-Shot-Lernen verwendet werden sollen\n",
    ")\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6914ca01-27aa-43f0-af28-131bc48ddbab",
   "metadata": {},
   "source": [
    "# Einladen als .pt und speichern in HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c53bfd-d0fe-46eb-b737-a3481eab5c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Lade das Torch Script-Modell\n",
    "torch_script_model = torch.jit.load(\"path_to_your_model.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a033bc3-2b84-4a5e-bc41-2bdfa439c1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import AutoConfig, AutoModelForSequenceClassification\n",
    "\n",
    "class TorchScriptToPyTorchModel(nn.Module):\n",
    "    def __init__(self, torch_script_model):\n",
    "        super(TorchScriptToPyTorchModel, self).__init__()\n",
    "        self.torch_script_model = torch_script_model\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n",
    "        # Passe die Eingaben an, falls nötig\n",
    "        return self.torch_script_model(input_ids, attention_mask, token_type_ids)\n",
    "\n",
    "# Initialisiere das Wrapper-Modell\n",
    "model_wrapper = TorchScriptToPyTorchModel(torch_script_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c6907e-86e0-4acf-8ad6-4b5c6531ec96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beispiel für eine Konfiguration\n",
    "config = AutoConfig.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Initialisiere das Modell mit der Wrapper-Klasse\n",
    "model = AutoModelForSequenceClassification.from_config(config)\n",
    "model.classifier = model_wrapper  # Ersetze die Klassifizierungs-Schicht durch das Torch Script-Modell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e65c49-b0c0-4c1d-8c82-ef70f5d45fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Speichere das Modell\n",
    "model.save_pretrained(\"path_to_save_model\")\n",
    "\n",
    "# Speichere den Tokenizer (falls notwendig)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer.save_pretrained(\"path_to_save_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcdfc4c-ee97-4985-8e85-548242f40da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Nochmal als kompletter code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce40127-e5dd-44a8-82b5-6ff2ee896df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoConfig, AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# Schritt 1: Lade das Torch Script-Modell\n",
    "torch_script_model = torch.jit.load(\"path_to_your_model.pt\")\n",
    "\n",
    "# Schritt 2: Erstelle ein Wrapper-Modell\n",
    "class TorchScriptToPyTorchModel(nn.Module):\n",
    "    def __init__(self, torch_script_model):\n",
    "        super(TorchScriptToPyTorchModel, self).__init__()\n",
    "        self.torch_script_model = torch_script_model\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n",
    "        # Passe die Eingaben an, falls nötig\n",
    "        return self.torch_script_model(input_ids, attention_mask, token_type_ids)\n",
    "\n",
    "# Initialisiere das Wrapper-Modell\n",
    "model_wrapper = TorchScriptToPyTorchModel(torch_script_model)\n",
    "\n",
    "# Schritt 3: Erstelle eine Hugging Face-Transformers Modell-Instanz\n",
    "config = AutoConfig.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModelForSequenceClassification.from_config(config)\n",
    "model.classifier = model_wrapper  # Ersetze die Klassifizierungs-Schicht durch das Torch Script-Modell\n",
    "\n",
    "# Schritt 4: Speichere das Modell im Hugging Face-Format\n",
    "model.save_pretrained(\"path_to_save_model\")\n",
    "\n",
    "# Speichere den Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer.save_pretrained(\"path_to_save_model\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "awq",
   "language": "python",
   "name": "awq"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
