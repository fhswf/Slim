{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f49296a4-a7bc-4f26-970f-edf7825ff99d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Verify torch version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96439398-2466-4b46-8478-03c7b95080d1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prepare Environment (Should not be neccessary if lora kernel is used)\n",
    "- Alle Installationen durchfÃ¼hren und danach den Kernel neu starten. Es funktioniert nur in dieser Kombination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3c52f7-e37b-4c2a-aa33-36b7ed4230fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install Pytorch for FSDP and FA/SDPA\n",
    "!pip install torch==2.0.1 tensorboard\n",
    "#pip install tensorboard datasets\n",
    " \n",
    "# Install Hugging Face libraries\n",
    "!pip install pydantic==2.0.0\n",
    "!pip install  --upgrade \"transformers==4.40.1\" \"datasets==2.18.0\" \"accelerate==0.29.3\" \"evaluate==0.4.1\" \"bitsandbytes==0.43.1\" \"huggingface_hub==0.22.2\" \"trl==0.8.6\" \"peft==0.10.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14f6a6c3-a099-4a58-a13d-31cc62eff4e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch-revgrad           0.2.0                    pypi_0    pypi\n",
      "torch                     2.0.0+cu117              pypi_0    pypi\n",
      "torchaudio                2.0.1+cu117              pypi_0    pypi\n",
      "torchvision               0.15.1+cu117             pypi_0    pypi\n",
      "adapter-transformers      3.0.1                    pypi_0    pypi\n",
      "transformers              4.40.0                   pypi_0    pypi\n",
      "pydantic                  1.7.4                    pypi_0    pypi\n",
      "pydantic-core             2.18.1                   pypi_0    pypi\n"
     ]
    }
   ],
   "source": [
    "# Double check environment\n",
    "# torch must be 2.0.0, transfomers must be 4.40, pydantic must be 2.0.0\n",
    "!conda list | grep torch\n",
    "!conda list | grep transformers\n",
    "!conda list | grep pydantic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b08633-4dc0-42ec-8948-67bae2e38fd9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Wahrscheinlich obsolet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8734781-8c0a-4c90-99e9-e17b94173e09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install transformers==4.40.1 torch==2.0.1 trl peft tensorboard pydantic==2.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b013a1bf-840d-455d-8143-436ca637ff2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install transformers==4.40.1\n",
    "!pip install torch==2.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f3b0bb-aadc-46d0-9082-5cd8a8509502",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install trl\n",
    "!pip install peft"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f07eb34-06ae-4609-807d-907b9ccfc5f0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Login to hugging face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be3c760-fbfb-4db0-b34f-e0dabfd4c5fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!huggingface-cli login --token \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4548a057-94c5-45e6-80ab-134986bcd9b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a5ffaa99-fe34-4eff-a2e2-de298aec8088",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Free GPU Memory\n",
    "- Alternatively > Restart Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726343b5-4fea-490d-a005-b2c5256dfd78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "del model\n",
    "del trainer\n",
    "torch.cuda.empty_cache()\n",
    "print(torch.cuda.memory_reserved(0))\n",
    "print(torch.cuda.memory_allocated(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06adbd9f-cf3b-46fb-912d-a8d4a9602c66",
   "metadata": {
    "tags": []
   },
   "source": [
    "##  Workflow for Lora tuning\n",
    "- Runs also well with \"torchtune\" kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99735949-c7be-437c-8539-216299cf792b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24f738ba-985c-43e5-8c38-5381956b1597",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:256\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,3\"\n",
    "!echo $CUDA_VISIBLE_DEVICES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a72d2d46-b709-435a-8440-b8707dda3eda",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-01 09:30:38.509393: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model, PeftModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ee84d6-63e2-4a9a-bbc9-88848919d926",
   "metadata": {},
   "source": [
    "### Define Model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "340fd983-2fae-4aa3-a07e-c3f342e90ba4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e61e727b18584120a22163d09e04dc74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# -> For Mistral 7B\n",
    "model_location = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "output_model = \"/home/thsch026/masterarbeit/models/generated/lora/mistral_7B-Instruct_ms-marco\"\n",
    "\n",
    "# -> For Llama 3 8B HF\n",
    "# model_location = \"/home/thsch026/masterarbeit/models/llama3/Meta-Llama-3-8B-Instruct-HF\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_location)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_location,\n",
    "   # load_in_8bit=True, # was 8bit\n",
    "    device_map=\"cuda\", # was auto\n",
    "    torch_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0519acb-62e4-4464-8e8b-f9590395d48d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "tokenizer.pad_token = \"!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a3b13c-c64b-40e5-9b7b-9e048e9c5456",
   "metadata": {},
   "source": [
    "### For explanation of the values for LORA configuration below see:\n",
    "- https://medium.com/@drishtisharma96505/comparative-analysis-of-lora-parameters-on-llama-2-with-flash-attention-574b913295d4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b20e5a88-4593-4e04-945a-4bf9ef46df10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Lora paramters\n",
    "CUTOFF_LEN = 768\n",
    "LORA_R = 8\n",
    "LORA_ALPHA = 2 * LORA_R\n",
    "LORA_DROPOUT = 0.1\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\"\n",
    "                    , \"down_proj\", \"lm_head\"], #these are the  names for the layers\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ede757-b5ab-4be9-9f08-984ae5dd8461",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Using general dataset MS_marco (v1.1) for Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6520ce8a-ec0b-4d8c-8cdd-99ea9fe46b31",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset DatasetDict({\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'passages', 'query', 'query_id', 'query_type', 'wellFormedAnswers'],\n",
      "        num_rows: 10047\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'passages', 'query', 'query_id', 'query_type', 'wellFormedAnswers'],\n",
      "        num_rows: 82326\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'passages', 'query', 'query_id', 'query_type', 'wellFormedAnswers'],\n",
      "        num_rows: 9650\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('ms_marco','v1.1') # General dataset\n",
    "print(\"dataset\", dataset)\n",
    "train_data = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439d5c91-d3b3-4975-83d5-6e1cc7d25da4",
   "metadata": {
    "tags": []
   },
   "source": [
    "###  ToDO: anpassen der Prompt Struktur an das genutzte Modell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409c9bdf-9169-440e-b2b4-68153c6f62f5",
   "metadata": {},
   "source": [
    "Prompt Structure for Mistral 7 B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7f036da-bada-4619-b222-f761adb0f384",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_prompt(user_query):  #The prompt format is taken from the official Mistral huggingface page\n",
    "  if user_query[\"answers\"] is not None and user_query[\"query\"] is not None:\n",
    "      p =  \"<s> [INST]\" + str(user_query[\"query\"]) + \"[/INST]\" +  str(user_query[\"answers\"]) + \"</s>\"\n",
    "      return p\n",
    "  else:\n",
    "    p = \"<s> [INST]\" + \"Hello\" + \"[/INST]\" +  \"Hello\" + \"</s>\"\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361d05aa-1e31-4409-84ae-fda25bb8465e",
   "metadata": {},
   "source": [
    "Prompt Structure for llama3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae6919af-5263-4875-88bb-f7e088926d4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize(prompt):\n",
    "    return tokenizer(\n",
    "        prompt + tokenizer.eos_token,\n",
    "        truncation=True,\n",
    "        max_length=CUTOFF_LEN ,\n",
    "        padding=\"max_length\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "306578b4-a6d7-4789-a88e-65aabfc67b9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if torch.cuda.device_count() > 1:\n",
    "    \n",
    "    #teacher_model = torch.nn.parallel.DistributedDataParallel(teacher_model)\n",
    "    #student_model = torch.nn.parallel.DistributedDataParallel(student_model)\n",
    "    model = torch.nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68acb15a-5ea5-41ad-b9d5-0a7041db776e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA:  True\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "813f3bc398c04d1b964baf67d5b634c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9650 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"CUDA: \", torch.cuda.is_available())\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_data = train_data.shuffle().map(lambda x: tokenize(generate_prompt(x)), remove_columns=['answers', 'passages', 'query', 'query_id', 'query_type', 'wellFormedAnswers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "050cd7a2-3755-4d82-8fa4-e2cd86f5dcdf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model, \n",
    "    train_dataset=train_data,\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=4,\n",
    "        num_train_epochs=3,    # 3 or 6 is good\n",
    "        learning_rate=1e-4,\n",
    "        logging_steps=5,\n",
    "        optim=\"adamw_torch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        output_dir=output_model\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")\n",
    "model.config.use_cache = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976dda06-388a-4341-afe8-0e5cddc2d7d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mthomas-t-schmitt\u001b[0m (\u001b[33mpumaai\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/thsch026/masterarbeit/Slim/notebooks/wandb/run-20240701_093509-6mf7cpx5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/pumaai/huggingface/runs/6mf7cpx5' target=\"_blank\">firm-bird-127</a></strong> to <a href='https://wandb.ai/pumaai/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/pumaai/huggingface' target=\"_blank\">https://wandb.ai/pumaai/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/pumaai/huggingface/runs/6mf7cpx5' target=\"_blank\">https://wandb.ai/pumaai/huggingface/runs/6mf7cpx5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='286' max='7236' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 286/7236 27:57 < 11:23:59, 0.17 it/s, Epoch 0.12/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>8.253500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>4.028300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.635500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.290800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>2.318600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.945600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.979500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.979900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.738600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.805400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>1.597900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.678900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>1.500100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.425100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>1.542100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.487800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>1.463900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.626100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>1.440500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.523700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>1.447200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.538100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>1.416800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.499600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>1.520400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.529800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>1.503600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.512700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>1.459900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.508300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>1.316200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.391200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>1.303900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.497900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>1.405700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.431800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185</td>\n",
       "      <td>1.445700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>1.463600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>1.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.551000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>205</td>\n",
       "      <td>1.458500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>1.525000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>215</td>\n",
       "      <td>1.587400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>1.521700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>1.602700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>1.516900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>235</td>\n",
       "      <td>1.363200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.371200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>245</td>\n",
       "      <td>1.583200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.437400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>255</td>\n",
       "      <td>1.469200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>1.248700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>265</td>\n",
       "      <td>1.405100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>1.353700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>1.534100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>1.419800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6538da0f-a3ce-4699-8901-d823135939ec",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Example 2 - Currently not working CUDA iisues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7587dd-58e2-46f1-bbf3-adbcfa420f89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c29b6d5-5d85-4ee4-91a5-d83aa5e520f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "{\"messages\": [{\"role\": \"system\", \"content\": \"You are...\"}, {\"role\": \"user\", \"content\": \"...\"}, {\"role\": \"assistant\", \"content\": \"...\"}]}\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"You are...\"}, {\"role\": \"user\", \"content\": \"...\"}, {\"role\": \"assistant\", \"content\": \"...\"}]}\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"You are...\"}, {\"role\": \"user\", \"content\": \"...\"}, {\"role\": \"assistant\", \"content\": \"...\"}]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec519b3-1292-475a-930a-d19586149bf3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    " \n",
    "# Convert dataset to OAI messages\n",
    "system_message = \"\"\"You are Llama, an AI assistant created by Philipp to be helpful and honest. Your knowledge spans a wide range of topics, allowing you to engage in substantive conversations and provide analysis on complex subjects.\"\"\"\n",
    " \n",
    "def create_conversation(sample):\n",
    "    if sample[\"messages\"][0][\"role\"] == \"system\":\n",
    "        return sample\n",
    "    else:\n",
    "      sample[\"messages\"] = [{\"role\": \"system\", \"content\": system_message}] + sample[\"messages\"]\n",
    "      return sample\n",
    " \n",
    "# Load dataset from the hub\n",
    "dataset = load_dataset(\"HuggingFaceH4/no_robots\")\n",
    " \n",
    "# Add system message to each conversation\n",
    "columns_to_remove = list(dataset[\"train\"].features)\n",
    "columns_to_remove.remove(\"messages\")\n",
    "dataset = dataset.map(create_conversation, remove_columns=columns_to_remove,batched=False)\n",
    " \n",
    "# Filter out conversations which are corrupted with wrong turns, keep which have even number of turns after adding system message\n",
    "dataset[\"train\"] = dataset[\"train\"].filter(lambda x: len(x[\"messages\"][1:]) % 2 == 0)\n",
    "dataset[\"test\"] = dataset[\"test\"].filter(lambda x: len(x[\"messages\"][1:]) % 2 == 0)\n",
    " \n",
    "# save datasets to disk\n",
    "dataset[\"train\"].to_json(\"train_dataset.json\", orient=\"records\", force_ascii=False)\n",
    "dataset[\"test\"].to_json(\"test_dataset.json\", orient=\"records\", force_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feba0e89-5898-4eb6-a834-3866ec751aa2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import trl\n",
    "#import bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61363177-1c5b-4cb6-8e0d-ecc09efed169",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!unset CUDA_VISIBLE_DEVICES\n",
    "CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b4d28e-b28a-4bde-b992-701edcb8a674",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!torchrun --nproc_per_node=4 ./scripts/run_fsdp_qlora.py --config llama_3_70b_fsdp_qlora.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b6c30e-b35a-4ba2-92d9-39a6c4807b40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb47153-1745-48d3-999c-35f97f0be54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "python -m llama_recipes.finetuning --use_peft --peft_method lora --quantization --model_name $MODEL --output_dir ../llama/models_ft/7B-peft --batch_size_training 2 --gradient_accumulation_steps 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016cc912-5372-478c-a4bb-78498e0231e1",
   "metadata": {},
   "source": [
    "### Source https://medium.com/@prakharsaxena11111/a-general-approach-to-fine-tune-any-llm-using-lora-29d24e47a345"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51e3879-d0fd-4df7-a5e0-5ce0612bb284",
   "metadata": {},
   "source": [
    "### GPU Memory issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0135eb51-de6f-4ea3-a8c1-20337a0febfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "i = 1\n",
    "while i == 1:\n",
    "    time.sleep(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562c05bc-eb36-43a3-bf42-a0c1c3461677",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!nvcc -V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd89e8f-6efd-4569-9bfa-d67f79354804",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!conda list |grep transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08642157-1da3-409f-81cd-2480435acc23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchtune",
   "language": "python",
   "name": "torchtune"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
