{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a952d96-0a00-426a-b3f5-bd98ce8a57f1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Erstellen der Arbeitsumgebung (einmalig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff66a913-55a4-4529-84b1-b2152faf4c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bash Shell\n",
    "# Ausw채hlen eines Verszeichnisses f체r die Installation\n",
    "# hier /home/thsch026/masterarbeit/experiment\n",
    "cd /home/thsch026/masterarbeit/experiment\n",
    "\n",
    "# Erstellen des conda environments\n",
    "conda create --autogptq autogptq pythn==3.10 -y\n",
    "conda activate autogptq\n",
    "\n",
    "# Installieren notwendiger packages\n",
    "pip install peft gekko pandas\n",
    "\n",
    "# Kopiern der Sourcen\n",
    "git clone https://github.com/PanQiWei/AutoGPTQ.git && cd AutoGPTQ\n",
    "\n",
    "# Komplilieren der Sourcen\n",
    "pip install -vvv --no-build-isolation -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd877e2-9818-4148-933d-2edb47fb8ac1",
   "metadata": {},
   "source": [
    "## AutoGPTQ Workflow (Kernel: autogptq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f36bee5a-9e9c-4dcf-a14b-e765d76c3dce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/home/thsch026/masterarbeit/experiment/AutoGPTQ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8456a3-5fda-42d1-b29c-f1be4cf5684f",
   "metadata": {},
   "source": [
    "## Auswahl des zu Quantisierenden Modells"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f74e83-f003-4371-94c6-77233bd19410",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Quantize Llama 3 8B Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c51fd89c-df11-4206-9cd1-2225c92bcb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_dir = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "quantized_model_dir = \"Meta-Llama-3-8B-instruct-v2_gptq_2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad914ec-ef6c-4c52-9dc7-6bb41cb437dc",
   "metadata": {},
   "source": [
    "### Quantize Mistral 7B Instruct v0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc590c65-5cc8-4d9a-b5e6-9f0b0f3d4995",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_dir = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "quantized_model_dir = \"Mistral-7B-Instruct-v0.2_gptq_3bit\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5b65db-2d3c-4c64-a5d8-47058e7acadd",
   "metadata": {},
   "source": [
    "## Funktion f체r das wikitext dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d7adcd3-692c-4df3-ad82-5c6743ae0158",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wikitext2(nsamples, seed, seqlen, model):\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    from datasets import load_dataset\n",
    "    traindata = load_dataset('wikitext', 'wikitext-2-raw-v1', split='train')\n",
    "    testdata = load_dataset('wikitext', 'wikitext-2-raw-v1', split='test')\n",
    "\n",
    "    from transformers import AutoTokenizer\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model, use_fast=False)\n",
    "    except:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model, use_fast=True)\n",
    "    trainenc = tokenizer(\"\\n\\n\".join(traindata['text']), return_tensors='pt')\n",
    "    testenc = tokenizer(\"\\n\\n\".join(testdata['text']), return_tensors='pt')\n",
    "\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    np.random.seed(0)\n",
    "    torch.random.manual_seed(0)\n",
    "\n",
    "    traindataset = []\n",
    "    for _ in range(nsamples):\n",
    "        i = random.randint(0, trainenc.input_ids.shape[1] - seqlen - 1)\n",
    "        j = i + seqlen\n",
    "        inp = trainenc.input_ids[:, i:j]\n",
    "        attention_mask = torch.ones_like(inp)\n",
    "        traindataset.append({'input_ids':inp,'attention_mask': attention_mask})\n",
    "    return traindataset, testenc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdeb878-880f-4fa2-8f48-6808dc665d94",
   "metadata": {},
   "source": [
    "## Durchf체hren der Quantisierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d91e439-c43c-49f9-aca9-0245ee1e7567",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55dddef94885452dad504a43ab251858",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, TextGenerationPipeline\n",
    "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_dir, use_fast=True)\n",
    "\n",
    "quantize_config = BaseQuantizeConfig(\n",
    "    bits=3,  # quantize model to 4-bit\n",
    "    group_size=128,  # it is recommended to set the value to 128\n",
    "    desc_act=False,  # set to False can significantly speed up inference but the perplexity may slightly bad\n",
    ")\n",
    "\n",
    "# load un-quantized model, by default, the model will always be loaded into CPU memory\n",
    "model = AutoGPTQForCausalLM.from_pretrained(pretrained_model_dir, quantize_config)\n",
    "\n",
    "\n",
    "# quantize model, the examples should be list of dict whose keys can only be \"input_ids\" and \"attention_mask\"\n",
    "print (\"Quantize Model\")\n",
    "traindataset, testenc = get_wikitext2(128, 0, 4096, pretrained_model_dir)\n",
    "model.quantize(traindataset, use_triton=False, batch_size=1, cache_examples_on_gpu=False)\n",
    "\n",
    "print (\"Saving model...\")\n",
    "# save quantized model using safetensors\n",
    "model.save_quantized(quantized_model_dir, use_safetensors=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_dir, use_fast=True)\n",
    "tokenizer.save_pretrained(quantized_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0af310-21df-4d2d-8d68-150dbfaea005",
   "metadata": {},
   "outputs": [],
   "source": [
    "usage: quant_with_alpaca.py [-h] [--pretrained_model_dir PRETRAINED_MODEL_DIR] [--quantized_model_dir QUANTIZED_MODEL_DIR] [--bits {2,3,4,8}] [--group_size GROUP_SIZE] [--desc_act]\n",
    "                            [--num_samples NUM_SAMPLES] [--save_and_reload] [--fast_tokenizer] [--use_triton] [--per_gpu_max_memory PER_GPU_MAX_MEMORY] [--cpu_max_memory CPU_MAX_MEMORY]\n",
    "                            [--quant_batch_size QUANT_BATCH_SIZE] [--trust_remote_code]\n",
    "\n",
    "options:\n",
    "  -h, --help            show this help message and exit\n",
    "  --pretrained_model_dir PRETRAINED_MODEL_DIR\n",
    "  --quantized_model_dir QUANTIZED_MODEL_DIR\n",
    "  --bits {2,3,4,8}\n",
    "  --group_size GROUP_SIZE\n",
    "                        group size, -1 means no grouping or full rank\n",
    "  --desc_act            whether to quantize with desc_act\n",
    "  --num_samples NUM_SAMPLES\n",
    "                        how many samples will be used to quantize model\n",
    "  --save_and_reload     whether save quantized model to disk and reload back\n",
    "  --fast_tokenizer      whether use fast tokenizer\n",
    "  --use_triton          whether use triton to speedup at inference\n",
    "  --per_gpu_max_memory PER_GPU_MAX_MEMORY\n",
    "                        max memory used to load model per gpu\n",
    "  --cpu_max_memory CPU_MAX_MEMORY\n",
    "                        max memory used to offload model to cpu\n",
    "  --quant_batch_size QUANT_BATCH_SIZE\n",
    "                        examples batch size for quantization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autogptq",
   "language": "python",
   "name": "autogptq"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
