{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50718606-aeb8-4622-b614-9f10ef10f34b",
   "metadata": {},
   "source": [
    "# Dual-Space Knowledge Distillation for Large Language Models\n",
    "- https://github.com/songmzhang/DSKD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af213aee-c6b7-4a54-a98b-e2c9c6e45a10",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Setup of the corrresponding conda environment\n",
    "- Only needed initally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a12da3d-b558-40d2-b043-8bc3bec37072",
   "metadata": {},
   "outputs": [],
   "source": [
    "conda create --name dskd python==3.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be129c0b-00af-4116-8700-05edbd20b52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install deepspeed==0.14.0 torch==2.0.1 transformers==4.40.2 peft==0.8.2 rouge_score==0.1.2 editdistance==0.8.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf4d02e-f570-4418-91d1-694a47200718",
   "metadata": {},
   "source": [
    "## Activate environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a12b8924-3b48-4c2f-95bc-528aaf032ea4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(/home/thsch026/my-envs/dskd) \n",
      "(/home/thsch026/my-envs/dskd) \n",
      "(/home/thsch026/my-envs/dskd) \n"
     ]
    }
   ],
   "source": [
    "conda activate dskd\n",
    "# make sure to be in the right directory\n",
    "cd /home/thsch026/masterarbeit/experiment/DSKD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5a6136-0d21-4a95-b71d-28b18110eae5",
   "metadata": {},
   "source": [
    "## Tasks to run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242991be-d779-454d-93f8-be7f97bd25e4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Example: Finetuning of the Mistral model as a teacher\n",
    "- IMPORTANT: The Script contains mainly the paramters for the run. You need to make sure taht the following things have been set correctly\n",
    "    - Base Path: Here \"scripts/tinyllama/sft_teacher_mistral.sh!\n",
    "    - Which GPUs to use\n",
    "    - Directorier in the model_hub where the models used by the script are located\n",
    "    - Types for Variables: Bfloat is for example not supported on older CUDA implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "531a6741-d7f0-475f-915c-76f031632377",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher is Mistral\n",
      "(/home/thsch026/my-envs/dskd) \n"
     ]
    }
   ],
   "source": [
    "scripts/tinyllama/sft_teacher_mistral.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f3c6ee-0599-4ffa-a135-11eb16056d33",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Find the results of the run in (Example only):\n",
    "- Depends on the name of the model and the nature of the task\n",
    "- At this location you find subdirectories where the name consists of the main paramteters of the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c421a089-9155-4d8a-834d-56845bb36b78",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(/home/thsch026/my-envs/dskd) \n"
     ]
    }
   ],
   "source": [
    "cd outputs/mistral/mistral-7b-v0.1/sft/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a327a2b-2b03-4998-8376-5be22e884821",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Start the distillation process using the finetuned Mistral as a teacher and tinyllama as the student model (FEHLERHAFT!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d11d9b58-d169-458b-b6af-1fce20928532",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(/home/thsch026/my-envs/dskd) \n",
      "(/home/thsch026/my-envs/dskd) \n"
     ]
    }
   ],
   "source": [
    "cd /home/thsch026/masterarbeit/experiment/DSKD\n",
    "scripts/tinyllama/dskd_tinyllama.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf71f3b-6b57-4e94-b1ef-498819f77e2a",
   "metadata": {},
   "source": [
    "## Produktiver Lauf für das prune-lora modell\n",
    "- Ein mit AWQ verkleinertes Modell lässt sich mit dem KD Algorythmus nicht optimieren (Vermutlich wegen der gekürzten Variablen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed6893f-dd85-4b1a-a6a7-0da1f8586657",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Nochmal ein Finetuning des Lehrer Modells\n",
    "- Anpassen der Parameter im Script auf Mistral 7B Instruct v.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90029ded-8c45-41e6-ae29-1fdcf7870c29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scripts/tinyllama/sft_teacher_mistral.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66dd2a74-5041-48fc-bfe9-889fe0267a93",
   "metadata": {},
   "source": [
    "### Lauf des Lehrer Modells gegen das Prune_lora_model\n",
    "- in dem Script müssen folgende Parameter angepasst werden\n",
    "    - Pfad zu dem Student Model\n",
    "    - Pfad zu dem Lehrermodel bzw. zu dem Checkpoint aus dem sft tuning\n",
    "    - Precision Variable wurde auf fp16 geändert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09c35f19-6764-4b88-acd0-c6221c7a5eb9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(/home/thsch026/my-envs/dskd) \n",
      "(/home/thsch026/my-envs/dskd) \n"
     ]
    }
   ],
   "source": [
    "cd /home/thsch026/masterarbeit/experiment/DSKD\n",
    "scripts/toms/dskd_tommodel.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf98e61-b139-44dc-9e94-0b39feb36fad",
   "metadata": {},
   "source": [
    "### Llama 3 8B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b233de1-8b94-443d-8750-6fd0821b76b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(/home/thsch026/my-envs/dskd) \n",
      "(/home/thsch026/my-envs/dskd) \n"
     ]
    }
   ],
   "source": [
    "cd /home/thsch026/masterarbeit/experiment/DSKD\n",
    "scripts/toms/sft_tommodel_llama3.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef4ed9be-b480-4a7e-8eea-002911b96e54",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(/home/thsch026/my-envs/dskd) \n",
      "(/home/thsch026/my-envs/dskd) \n"
     ]
    }
   ],
   "source": [
    "cd /home/thsch026/masterarbeit/experiment/DSKD\n",
    "scripts/toms/dskd_tommodel_llama3.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92a199d-a647-44bb-8e33-935204762686",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Snippet to use for downloading certain models to the model hub for usage\n",
    "- Must run in conda \"awq\" environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b092ad33-1c1d-4448-a9d2-e5ec3d752f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "hf_download = \"meta-llama/CodeLlama-13b-hf\"\n",
    "save_location  = \"/home/thsch026/masterarbeit/experiment/DSKD/model_hub/toms/CodeLlama-13b-hf\"\n",
    "\n",
    "print (\"Start Download\")\n",
    "model = AutoModelForCausalLM.from_pretrained(hf_download)\n",
    "tokenizer = AutoTokenizer.from_pretrained(hf_download)\n",
    "print (\"Start saving model locally...\")\n",
    "model.save_pretrained(save_location, safetensors=True)\n",
    "tokenizer.save_pretrained(save_location)\n",
    "print (\"Saving complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff07be1-21d0-42ff-ab65-42d3f7ef530b",
   "metadata": {},
   "source": [
    "## Snippet für das Merging des resultierenden student models (qlora3 kernel)\n",
    "- in der config datei des Adaptesr müssen Einträge wegen inkompatibilität enfrent werden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dff2f712-cf09-41c1-afc8-aec53ebd3e4e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "LoraConfig.__init__() got an unexpected keyword argument 'loftq_config'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Local path of adapter model\u001b[39;00m\n\u001b[1;32m      5\u001b[0m model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/thsch026/masterarbeit/experiment/DSKD/outputs/toms/Meta-Llama-3-8B-Instruct/sft/adapter\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 6\u001b[0m peft_model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoPeftModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(peft_model))\n\u001b[1;32m      9\u001b[0m merged_model \u001b[38;5;241m=\u001b[39m peft_model\u001b[38;5;241m.\u001b[39mmerge_and_unload()\n",
      "File \u001b[0;32m~/my-envs/qlora3/lib/python3.10/site-packages/peft/auto.py:69\u001b[0m, in \u001b[0;36m_BaseAutoPeftModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, adapter_name, is_trainable, config, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_pretrained\u001b[39m(\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m     63\u001b[0m ):\n\u001b[1;32m     64\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    A wrapper around all the preprocessing steps a user needs to perform in order to load a PEFT model. The kwargs\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03m    are passed along to `PeftConfig` that automatically takes care of filtering the kwargs of the Hub methods and\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;124;03m    the config object init.\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m     peft_config \u001b[38;5;241m=\u001b[39m \u001b[43mPeftConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m     base_model_path \u001b[38;5;241m=\u001b[39m peft_config\u001b[38;5;241m.\u001b[39mbase_model_name_or_path\n\u001b[1;32m     72\u001b[0m     task_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(peft_config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtask_type\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/my-envs/qlora3/lib/python3.10/site-packages/peft/config.py:134\u001b[0m, in \u001b[0;36mPeftConfigMixin.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, subfolder, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m     config_cls \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\n\u001b[1;32m    133\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mclass_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloaded_attributes}\n\u001b[0;32m--> 134\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[43mconfig_cls\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m config\n",
      "\u001b[0;31mTypeError\u001b[0m: LoraConfig.__init__() got an unexpected keyword argument 'loftq_config'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "# Local path of adapter model\n",
    "model_id = \"/home/thsch026/masterarbeit/experiment/DSKD/outputs/toms/Meta-Llama-3-8B-Instruct/sft/adapter\"\n",
    "peft_model = AutoPeftModelForCausalLM.from_pretrained(model_id)\n",
    "print(type(peft_model))\n",
    "\n",
    "merged_model = peft_model.merge_and_unload()\n",
    "# The adapters are merged now and it is transformers class again\n",
    "print(type(merged_model))\n",
    "\n",
    "save_location  = \"/home/thsch026/masterarbeit/models/generated/dist/Meta-Llama-3-8B-Instruct_sft\"\n",
    "tokenizer = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "print (\"Start saving the merged model to disc\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer)\n",
    "merged_model.save_pretrained(save_location, safetensors=True)\n",
    "tokenizer.save_pretrained(save_location)\n",
    "print (\"Saving complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c882a7-86dd-475e-ac0f-770e82f01caa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
