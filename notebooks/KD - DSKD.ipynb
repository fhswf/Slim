{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50718606-aeb8-4622-b614-9f10ef10f34b",
   "metadata": {},
   "source": [
    "# Dual-Space Knowledge Distillation for Large Language Models\n",
    "- https://github.com/songmzhang/DSKD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af213aee-c6b7-4a54-a98b-e2c9c6e45a10",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Setup of the corrresponding conda environment\n",
    "- Only needed initally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a12da3d-b558-40d2-b043-8bc3bec37072",
   "metadata": {},
   "outputs": [],
   "source": [
    "conda create --name dskd python==3.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be129c0b-00af-4116-8700-05edbd20b52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install deepspeed==0.14.0 torch==2.0.1 transformers==4.40.2 peft==0.8.2 rouge_score==0.1.2 editdistance==0.8.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf4d02e-f570-4418-91d1-694a47200718",
   "metadata": {},
   "source": [
    "## Activate environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a12b8924-3b48-4c2f-95bc-528aaf032ea4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(/home/thsch026/my-envs/dskd) \n",
      "(/home/thsch026/my-envs/dskd) \n",
      "(/home/thsch026/my-envs/dskd) \n"
     ]
    }
   ],
   "source": [
    "conda activate dskd\n",
    "# make sure to be in the right directory\n",
    "cd /home/thsch026/masterarbeit/experiment/DSKD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5a6136-0d21-4a95-b71d-28b18110eae5",
   "metadata": {},
   "source": [
    "## Tasks to run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242991be-d779-454d-93f8-be7f97bd25e4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Example: Finetuning of the Mistral model as a teacher\n",
    "- IMPORTANT: The Script contains mainly the paramters for the run. You need to make sure taht the following things have been set correctly\n",
    "    - Base Path: Here \"scripts/tinyllama/sft_teacher_mistral.sh!\n",
    "    - Which GPUs to use\n",
    "    - Directorier in the model_hub where the models used by the script are located\n",
    "    - Types for Variables: Bfloat is for example not supported on older CUDA implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "531a6741-d7f0-475f-915c-76f031632377",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher is Mistral\n",
      "(/home/thsch026/my-envs/dskd) \n"
     ]
    }
   ],
   "source": [
    "scripts/tinyllama/sft_teacher_mistral.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f3c6ee-0599-4ffa-a135-11eb16056d33",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Find the results of the run in (Example only):\n",
    "- Depends on the name of the model and the nature of the task\n",
    "- At this location you find subdirectories where the name consists of the main paramteters of the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c421a089-9155-4d8a-834d-56845bb36b78",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(/home/thsch026/my-envs/dskd) \n"
     ]
    }
   ],
   "source": [
    "cd outputs/mistral/mistral-7b-v0.1/sft/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a327a2b-2b03-4998-8376-5be22e884821",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Start the distillation process using the finetuned Mistral as a teacher and tinyllama as the student model (FEHLERHAFT!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d11d9b58-d169-458b-b6af-1fce20928532",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(/home/thsch026/my-envs/dskd) \n",
      "(/home/thsch026/my-envs/dskd) \n"
     ]
    }
   ],
   "source": [
    "cd /home/thsch026/masterarbeit/experiment/DSKD\n",
    "scripts/tinyllama/dskd_tinyllama.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf71f3b-6b57-4e94-b1ef-498819f77e2a",
   "metadata": {},
   "source": [
    "## Produktiver Lauf für das prune-lora-awq modell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed6893f-dd85-4b1a-a6a7-0da1f8586657",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Nochmal ein Finetuning des Lehrer Modells\n",
    "- Anpassen der Parameter im Script auf Mistral 7B Instruct v.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90029ded-8c45-41e6-ae29-1fdcf7870c29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scripts/tinyllama/sft_teacher_mistral.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66dd2a74-5041-48fc-bfe9-889fe0267a93",
   "metadata": {},
   "source": [
    "### Lauf des Lehrer Modells gegen das Prune_lora_awq model\n",
    "- in dem Script müssen folgende Parameter angepasst werden\n",
    "    - Pfad zu dem Student Model\n",
    "    - Pfad zu dem Lehrermodel bzw. zu dem Checkpoint aus dem sft tuning\n",
    "    - Precision Variable wurde auf fp16 geändert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c35f19-6764-4b88-acd0-c6221c7a5eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /home/thsch026/masterarbeit/experiment/DSKD\n",
    "scripts/toms/dskd_tommodel.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92a199d-a647-44bb-8e33-935204762686",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Snippet to use for downloading certain models to the model hub for usage\n",
    "- Must run in conda \"awq\" environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b092ad33-1c1d-4448-a9d2-e5ec3d752f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Download\n",
      "Start saving model locally...\n",
      "Saving complete\n",
      "LlamaTokenizerFast(name_or_path='mistralai/Mistral-7B-Instruct-v0.2', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "hf_download = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "save_location  = \"/home/thsch026/masterarbeit/experiment/DSKD/model_hub/mistral/mistral-7b-instruct-v0.2\"\n",
    "\n",
    "print (\"Start Download\")\n",
    "model = AutoModelForCausalLM.from_pretrained(hf_download)\n",
    "tokenizer = AutoTokenizer.from_pretrained(hf_download)\n",
    "print (\"Start saving model locally...\")\n",
    "model.save_pretrained(save_location, safetensors=True)\n",
    "tokenizer.save_pretrained(save_location)\n",
    "print (\"Saving complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff07be1-21d0-42ff-ab65-42d3f7ef530b",
   "metadata": {},
   "source": [
    "## Snippet für das Merging des resultierenden student models (qlora3 kernel)\n",
    "- in der config datei des Adaptesr müssen Einträge wegen inkompatibilität enfrent werden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dff2f712-cf09-41c1-afc8-aec53ebd3e4e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'peft.peft_model.PeftModelForCausalLM'>\n",
      "<class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'>\n",
      "Start saving the merged model to disc\n",
      "Saving complete\n",
      "LlamaTokenizerFast(name_or_path='/home/thsch026/masterarbeit/experiment/DSKD/model_hub/tinyllama/tinyllama-1.1b-3T', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "# Local path of adapter model\n",
    "model_id = \"/home/thsch026/masterarbeit/experiment/DSKD/outputs/tinyllama/tinyllama-1.1b-3T/dual_space_kd/adapter\"\n",
    "peft_model = AutoPeftModelForCausalLM.from_pretrained(model_id)\n",
    "print(type(peft_model))\n",
    "\n",
    "merged_model = peft_model.merge_and_unload()\n",
    "# The adapters are merged now and it is transformers class again\n",
    "print(type(merged_model))\n",
    "\n",
    "save_location  = \"/home/thsch026/masterarbeit/models/generated/dist/tinyllama-1.1-3T_distilled\"\n",
    "tokenizer = \"/home/thsch026/masterarbeit/experiment/DSKD/model_hub/tinyllama/tinyllama-1.1b-3T\"\n",
    "\n",
    "print (\"Start saving the merged model to disc\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer)\n",
    "merged_model.save_pretrained(save_location, safetensors=True)\n",
    "tokenizer.save_pretrained(save_location)\n",
    "print (\"Saving complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c882a7-86dd-475e-ac0f-770e82f01caa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qlora3",
   "language": "python",
   "name": "qlora3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
