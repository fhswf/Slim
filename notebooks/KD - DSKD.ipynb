{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50718606-aeb8-4622-b614-9f10ef10f34b",
   "metadata": {},
   "source": [
    "# Dual-Space Knowledge Distillation for Large Language Models\n",
    "- https://github.com/songmzhang/DSKD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af213aee-c6b7-4a54-a98b-e2c9c6e45a10",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Setup of the corrresponding conda environment\n",
    "- Only needed initally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a12da3d-b558-40d2-b043-8bc3bec37072",
   "metadata": {},
   "outputs": [],
   "source": [
    "conda create --name dskd python==3.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be129c0b-00af-4116-8700-05edbd20b52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install deepspeed==0.14.0 torch==2.0.1 transformers==4.40.2 peft==0.8.2 rouge_score==0.1.2 editdistance==0.8.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf4d02e-f570-4418-91d1-694a47200718",
   "metadata": {},
   "source": [
    "## Activate environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a12b8924-3b48-4c2f-95bc-528aaf032ea4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(/home/thsch026/my-envs/dskd) \n",
      "(/home/thsch026/my-envs/dskd) \n",
      "(/home/thsch026/my-envs/dskd) \n"
     ]
    }
   ],
   "source": [
    "conda activate dskd\n",
    "# make sure to be in the right directory\n",
    "cd /home/thsch026/masterarbeit/experiment/DSKD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5a6136-0d21-4a95-b71d-28b18110eae5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Getting started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242991be-d779-454d-93f8-be7f97bd25e4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Example: Finetuning of the Mistral model as a teacher\n",
    "- IMPORTANT: The Script contains mainly the paramters for the run. You need to make sure taht the following things have been set correctly\n",
    "    - Base Path: Here \"scripts/tinyllama/sft_teacher_mistral.sh!\n",
    "    - Which GPUs to use\n",
    "    - Directorier in the model_hub where the models used by the script are located\n",
    "    - Types for Variables: Bfloat is for example not supported on older CUDA implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "531a6741-d7f0-475f-915c-76f031632377",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher is Mistral\n",
      "(/home/thsch026/my-envs/dskd) \n"
     ]
    }
   ],
   "source": [
    "scripts/tinyllama/sft_teacher_mistral.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f3c6ee-0599-4ffa-a135-11eb16056d33",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Find the results of the run in (Example only):\n",
    "- Depends on the name of the model and the nature of the task\n",
    "- At this location you find subdirectories where the name consists of the main paramteters of the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c421a089-9155-4d8a-834d-56845bb36b78",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(/home/thsch026/my-envs/dskd) \n"
     ]
    }
   ],
   "source": [
    "cd outputs/mistral/mistral-7b-v0.1/sft/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf71f3b-6b57-4e94-b1ef-498819f77e2a",
   "metadata": {},
   "source": [
    "## Knowledege Distillation mid DSKD und dn zuvor erstellten prune_lora Modellen\n",
    "- Ein mit AWQ verkleinertes Modell lässt sich mit dem KD Algorithmus nicht optimieren (Vermutlich wegen der gekürzten Variablen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e94361-4460-427e-bd27-7b8799e98be4",
   "metadata": {},
   "source": [
    "### Finetuning der Teacher Modelle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f81bea-d662-4ae8-bbc1-5d89d13577fb",
   "metadata": {},
   "source": [
    "#### Finetuning Teacher: Llama 3 8B Instruct v0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b233de1-8b94-443d-8750-6fd0821b76b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cd /home/thsch026/masterarbeit/experiment/DSKD\n",
    "scripts/toms/sft_tommodel_llama3.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1dcc4b-1b72-4006-948d-f00b8c0fd928",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Finetuning Teacher: Mistral 7B Instruct v0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b1e9c12-db7d-4729-8629-320bbeaae060",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cd /home/thsch026/masterarbeit/experiment/DSKD\n",
    "scripts/toms/sft_tommodel_mistral.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94476cee-7b83-46d4-a551-30ecb0cb894d",
   "metadata": {},
   "source": [
    "#### Finetuning Teacher: Phi 3 medium 4K instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09c35f19-6764-4b88-acd0-c6221c7a5eb9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(/home/thsch026/my-envs/dskd) \n",
      "(/home/thsch026/my-envs/dskd) \n"
     ]
    }
   ],
   "source": [
    "cd /home/thsch026/masterarbeit/experiment/DSKD\n",
    "scripts/toms/sft_tommodel_phi.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66dd2a74-5041-48fc-bfe9-889fe0267a93",
   "metadata": {},
   "source": [
    "### KD mit den finegetunten Teacher Modellen gegen die Prune_lora_modelle\n",
    "- in dem Script müssen folgende Parameter angepasst werden\n",
    "    - Pfad zu dem Student Model\n",
    "    - Pfad zu dem Lehrermodel bzw. zu dem Checkpoint aus dem sft tuning\n",
    "    - Precision Variable wurde auf fp16 geändert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf98e61-b139-44dc-9e94-0b39feb36fad",
   "metadata": {},
   "source": [
    "#### Llama 3 8B prune lora -> Teacher: Llama 3 8B Instruct (sft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef4ed9be-b480-4a7e-8eea-002911b96e54",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(/home/thsch026/my-envs/dskd) \n",
      "(/home/thsch026/my-envs/dskd) \n"
     ]
    }
   ],
   "source": [
    "cd /home/thsch026/masterarbeit/experiment/DSKD\n",
    "scripts/toms/dskd_tommodel_llama3.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868ceb2f-63cb-419c-9209-d3ad8015b012",
   "metadata": {},
   "source": [
    "#### Mistral prune Lora  -> Teacher: Mistral 7B Instruct v0.2 (sft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d3c3d3-37ab-46da-9384-d3122f707c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /home/thsch026/masterarbeit/experiment/DSKD\n",
    "scripts/toms/dskd_tommodel_mistral.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cf3baf-a817-4638-9720-a94f4806af70",
   "metadata": {},
   "source": [
    "#### Llama 3 8B prune Lora -> Teacher: Phi 3 4K instruct (sft)\n",
    "- Hier werden Modelle mit verschiedenen Vocabulary Sets verwendet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1c1de10-6eaf-44b1-9245-13aee74e0926",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(/home/thsch026/my-envs/dskd) \n",
      "(/home/thsch026/my-envs/dskd) \n"
     ]
    }
   ],
   "source": [
    "cd /home/thsch026/masterarbeit/experiment/DSKD\n",
    "scripts/toms/dskd_cma_tommodel_phi.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25cbb15-960d-4fa7-af45-393daa57070a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92a199d-a647-44bb-8e33-935204762686",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Snippet to use for downloading certain models to the model hub for usage\n",
    "- Must run in conda \"awq\" environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b092ad33-1c1d-4448-a9d2-e5ec3d752f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "hf_download = \"meta-llama/CodeLlama-13b-hf\"\n",
    "save_location  = \"/home/thsch026/masterarbeit/experiment/DSKD/model_hub/toms/CodeLlama-13b-hf\"\n",
    "\n",
    "print (\"Start Download\")\n",
    "model = AutoModelForCausalLM.from_pretrained(hf_download)\n",
    "tokenizer = AutoTokenizer.from_pretrained(hf_download)\n",
    "print (\"Start saving model locally...\")\n",
    "model.save_pretrained(save_location, safetensors=True)\n",
    "tokenizer.save_pretrained(save_location)\n",
    "print (\"Saving complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff07be1-21d0-42ff-ab65-42d3f7ef530b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Snippet für das Merging des resultierenden student models (qlora3 kernel)\n",
    "- in der config datei des Adaptesr müssen Einträge wegen inkompatibilität enternt werden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dff2f712-cf09-41c1-afc8-aec53ebd3e4e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c10ea91336a740a38220a0005edece18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'peft.peft_model.PeftModelForCausalLM'>\n",
      "<class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'>\n",
      "Start saving the merged model to disc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thsch026/my-envs/qlora3/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving complete\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "# Local path of adapter model\n",
    "model_id = \"/home/thsch026/masterarbeit/experiment/DSKD/outputs/toms/Meta-Llama-3-8B-instruct_prune_lora/dual_space_kd_with_cma/adapter\"\n",
    "peft_model = AutoPeftModelForCausalLM.from_pretrained(model_id)\n",
    "print(type(peft_model))\n",
    "\n",
    "merged_model = peft_model.merge_and_unload()\n",
    "# The adapters are merged now and it is transformers class again\n",
    "print(type(merged_model))\n",
    "\n",
    "save_location  = \"/home/thsch026/masterarbeit/models/generated/dist/Meta-Llama-3-8B-Instruct_prune_lora_dist_phi\"\n",
    "tokenizer = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "print (\"Start saving the merged model to disc\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer)\n",
    "merged_model.save_pretrained(save_location, safetensors=True)\n",
    "tokenizer.save_pretrained(save_location)\n",
    "print (\"Saving complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c882a7-86dd-475e-ac0f-770e82f01caa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qlora3",
   "language": "python",
   "name": "qlora3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
