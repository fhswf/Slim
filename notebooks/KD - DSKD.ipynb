{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50718606-aeb8-4622-b614-9f10ef10f34b",
   "metadata": {},
   "source": [
    "# Dual-Space Knowledge Distillation for Large Language Models\n",
    "- https://github.com/songmzhang/DSKD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af213aee-c6b7-4a54-a98b-e2c9c6e45a10",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Setup of the corrresponding conda environment\n",
    "- Only needed initally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a12da3d-b558-40d2-b043-8bc3bec37072",
   "metadata": {},
   "outputs": [],
   "source": [
    "conda create --name dskd python==3.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be129c0b-00af-4116-8700-05edbd20b52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install deepspeed==0.14.0 torch==2.0.1 transformers==4.40.2 peft==0.8.2 rouge_score==0.1.2 editdistance==0.8.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf4d02e-f570-4418-91d1-694a47200718",
   "metadata": {},
   "source": [
    "## Activate environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a12b8924-3b48-4c2f-95bc-528aaf032ea4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(/home/thsch026/my-envs/dskd) \n",
      "(/home/thsch026/my-envs/dskd) \n",
      "(/home/thsch026/my-envs/dskd) \n"
     ]
    }
   ],
   "source": [
    "conda activate dskd\n",
    "# make sure to be in the right directory\n",
    "cd /home/thsch026/masterarbeit/experiment/DSKD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5a6136-0d21-4a95-b71d-28b18110eae5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Getting started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242991be-d779-454d-93f8-be7f97bd25e4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Example: Finetuning of the Mistral model as a teacher\n",
    "- IMPORTANT: The Script contains mainly the paramters for the run. You need to make sure taht the following things have been set correctly\n",
    "    - Base Path: Here \"scripts/tinyllama/sft_teacher_mistral.sh!\n",
    "    - Which GPUs to use\n",
    "    - Directorier in the model_hub where the models used by the script are located\n",
    "    - Types for Variables: Bfloat is for example not supported on older CUDA implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "531a6741-d7f0-475f-915c-76f031632377",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher is Mistral\n",
      "(/home/thsch026/my-envs/dskd) \n"
     ]
    }
   ],
   "source": [
    "scripts/tinyllama/sft_teacher_mistral.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f3c6ee-0599-4ffa-a135-11eb16056d33",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Find the results of the run in (Example only):\n",
    "- Depends on the name of the model and the nature of the task\n",
    "- At this location you find subdirectories where the name consists of the main paramteters of the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c421a089-9155-4d8a-834d-56845bb36b78",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(/home/thsch026/my-envs/dskd) \n"
     ]
    }
   ],
   "source": [
    "cd outputs/mistral/mistral-7b-v0.1/sft/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf71f3b-6b57-4e94-b1ef-498819f77e2a",
   "metadata": {},
   "source": [
    "## Knowledege Distillation mid DSKD und den zuvor erstellten prune_lora Modellen\n",
    "- Ein mit AWQ verkleinertes Modell lässt sich mit dem KD Algorithmus nicht optimieren (Vermutlich wegen der gekürzten Variablen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e94361-4460-427e-bd27-7b8799e98be4",
   "metadata": {},
   "source": [
    "### Finetuning der Teacher Modelle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4dbb4f-3e47-4b24-bec4-320686ef71af",
   "metadata": {},
   "source": [
    "#### Finetuning Mixtral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4670eb78-57db-4a22-be26-18ad5e38da09",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(/home/thsch026/my-envs/dskd) \n",
      "Teacher is Mistral\n",
      "(/home/thsch026/my-envs/dskd) \n"
     ]
    }
   ],
   "source": [
    "cd /home/thsch026/masterarbeit/experiment/DSKD\n",
    "scripts/toms/sft_teacher_mixtral.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f81bea-d662-4ae8-bbc1-5d89d13577fb",
   "metadata": {},
   "source": [
    "#### Finetuning Teacher: Llama 3 8B Instruct v0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b233de1-8b94-443d-8750-6fd0821b76b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cd /home/thsch026/masterarbeit/experiment/DSKD\n",
    "scripts/toms/sft_tommodel_llama3.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1dcc4b-1b72-4006-948d-f00b8c0fd928",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Finetuning Teacher: Mistral 7B Instruct v0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b1e9c12-db7d-4729-8629-320bbeaae060",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cd /home/thsch026/masterarbeit/experiment/DSKD\n",
    "scripts/toms/sft_tommodel_mistral.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94476cee-7b83-46d4-a551-30ecb0cb894d",
   "metadata": {},
   "source": [
    "#### Finetuning Teacher: Phi 3 medium 4K instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09c35f19-6764-4b88-acd0-c6221c7a5eb9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(/home/thsch026/my-envs/dskd) \n",
      "(/home/thsch026/my-envs/dskd) \n"
     ]
    }
   ],
   "source": [
    "cd /home/thsch026/masterarbeit/experiment/DSKD\n",
    "scripts/toms/sft_tommodel_phi.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66dd2a74-5041-48fc-bfe9-889fe0267a93",
   "metadata": {},
   "source": [
    "### KD mit den finegetunten Teacher Modellen gegen die Prune_lora_modelle\n",
    "- in dem Script müssen folgende Parameter angepasst werden\n",
    "    - Pfad zu dem Student Model\n",
    "    - Pfad zu dem Lehrermodel bzw. zu dem Checkpoint aus dem sft tuning\n",
    "    - Precision Variable wurde auf fp16 geändert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf98e61-b139-44dc-9e94-0b39feb36fad",
   "metadata": {},
   "source": [
    "#### Llama 3 8B prune lora -> Teacher: Llama 3 8B Instruct (sft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef4ed9be-b480-4a7e-8eea-002911b96e54",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(/home/thsch026/my-envs/dskd) \n",
      "(/home/thsch026/my-envs/dskd) \n"
     ]
    }
   ],
   "source": [
    "cd /home/thsch026/masterarbeit/experiment/DSKD\n",
    "scripts/toms/dskd_tommodel_llama3.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868ceb2f-63cb-419c-9209-d3ad8015b012",
   "metadata": {},
   "source": [
    "#### Mistral prune Lora  -> Teacher: Mistral 7B Instruct v0.2 (sft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d3c3d3-37ab-46da-9384-d3122f707c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /home/thsch026/masterarbeit/experiment/DSKD\n",
    "scripts/toms/dskd_tommodel_mistral.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cf3baf-a817-4638-9720-a94f4806af70",
   "metadata": {},
   "source": [
    "#### Llama 3 8B prune Lora -> Teacher: Phi 3 4K instruct (sft)\n",
    "- Hier werden Modelle mit verschiedenen Vocabulary Sets verwendet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1c1de10-6eaf-44b1-9245-13aee74e0926",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(/home/thsch026/my-envs/dskd) \n",
      "(/home/thsch026/my-envs/dskd) \n"
     ]
    }
   ],
   "source": [
    "cd /home/thsch026/masterarbeit/experiment/DSKD\n",
    "scripts/toms/dskd_cma_tommodel_phi.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25cbb15-960d-4fa7-af45-393daa57070a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92a199d-a647-44bb-8e33-935204762686",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Snippet to use for downloading certain models to the model hub for usage\n",
    "- Must run in conda \"awq\" environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09648382-f9e1-4657-a303-51fe5a9cb63b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimum                   1.21.3                   pypi_0    pypi\n"
     ]
    }
   ],
   "source": [
    "!conda list |grep optimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b092ad33-1c1d-4448-a9d2-e5ec3d752f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thsch026/my-envs/awq/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Download\n"
     ]
    },
    {
     "ename": "PackageNotFoundError",
     "evalue": "No package metadata was found for optimum",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPackageNotFoundError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m save_location  \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/thsch026/masterarbeit/experiment/DSKD/model_hub/toms/Mixtral-8x7B-v0.1-GPTQ\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStart Download\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhf_download\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(hf_download)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStart saving model locally...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/my-envs/awq/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "File \u001b[0;32m~/my-envs/awq/lib/python3.10/site-packages/transformers/modeling_utils.py:3366\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3364\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre_quantized \u001b[38;5;129;01mor\u001b[39;00m quantization_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3365\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pre_quantized:\n\u001b[0;32m-> 3366\u001b[0m         config\u001b[38;5;241m.\u001b[39mquantization_config \u001b[38;5;241m=\u001b[39m \u001b[43mAutoHfQuantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge_quantization_configs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3367\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantization_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantization_config\u001b[49m\n\u001b[1;32m   3368\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3369\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3370\u001b[0m         config\u001b[38;5;241m.\u001b[39mquantization_config \u001b[38;5;241m=\u001b[39m quantization_config\n",
      "File \u001b[0;32m~/my-envs/awq/lib/python3.10/site-packages/transformers/quantizers/auto.py:161\u001b[0m, in \u001b[0;36mAutoHfQuantizer.merge_quantization_configs\u001b[0;34m(cls, quantization_config, quantization_config_from_args)\u001b[0m\n\u001b[1;32m    158\u001b[0m     warning_msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(quantization_config, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m--> 161\u001b[0m     quantization_config \u001b[38;5;241m=\u001b[39m \u001b[43mAutoQuantizationConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(quantization_config, (GPTQConfig, AwqConfig, FbgemmFp8Config))\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m quantization_config_from_args \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    166\u001b[0m ):\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;66;03m# special case for GPTQ / AWQ / FbgemmFp8 config collision\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     loading_attr_dict \u001b[38;5;241m=\u001b[39m quantization_config_from_args\u001b[38;5;241m.\u001b[39mget_loading_attributes()\n",
      "File \u001b[0;32m~/my-envs/awq/lib/python3.10/site-packages/transformers/quantizers/auto.py:91\u001b[0m, in \u001b[0;36mAutoQuantizationConfig.from_dict\u001b[0;34m(cls, quantization_config_dict)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     86\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown quantization type, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquant_method\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - supported types are:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     87\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(AUTO_QUANTIZER_MAPPING\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     88\u001b[0m     )\n\u001b[1;32m     90\u001b[0m target_cls \u001b[38;5;241m=\u001b[39m AUTO_QUANTIZATION_CONFIG_MAPPING[quant_method]\n\u001b[0;32m---> 91\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtarget_cls\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquantization_config_dict\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/my-envs/awq/lib/python3.10/site-packages/transformers/utils/quantization_config.py:97\u001b[0m, in \u001b[0;36mQuantizationConfigMixin.from_dict\u001b[0;34m(cls, config_dict, return_unused_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_dict\u001b[39m(\u001b[38;5;28mcls\u001b[39m, config_dict, return_unused_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     81\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;124;03m    Instantiates a [`QuantizationConfigMixin`] from a Python dictionary of parameters.\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;124;03m        [`QuantizationConfigMixin`]: The configuration object instantiated from those parameters.\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m     to_remove \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/my-envs/awq/lib/python3.10/site-packages/transformers/utils/quantization_config.py:636\u001b[0m, in \u001b[0;36mGPTQConfig.__init__\u001b[0;34m(self, bits, tokenizer, dataset, group_size, damp_percent, desc_act, sym, true_sequential, use_cuda_fp16, model_seqlen, block_name_to_quantize, module_name_preceding_first_block, batch_size, pad_token_id, use_exllama, max_input_length, exllama_config, cache_block_outputs, modules_in_block_to_quantize, **kwargs)\u001b[0m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_block_outputs \u001b[38;5;241m=\u001b[39m cache_block_outputs\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodules_in_block_to_quantize \u001b[38;5;241m=\u001b[39m modules_in_block_to_quantize\n\u001b[0;32m--> 636\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/my-envs/awq/lib/python3.10/site-packages/transformers/utils/quantization_config.py:712\u001b[0m, in \u001b[0;36mGPTQConfig.post_init\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    708\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    709\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou need optimum > 1.13.2 and auto-gptq > 0.4.2 . Make sure to have that version installed - detected version : optimum \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moptimum_version\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and autogptq \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mautogptq_version\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    710\u001b[0m             )\n\u001b[1;32m    711\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodules_in_block_to_quantize \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 712\u001b[0m     optimum_version \u001b[38;5;241m=\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(\u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mversion\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moptimum\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m optimum_version \u001b[38;5;241m<\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1.15.0\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    714\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    715\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou current version of `optimum` does not support `modules_in_block_to_quantize` quantization argument, please upgrade `optimum` package to a version superior than 1.15.0 .\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    716\u001b[0m         )\n",
      "File \u001b[0;32m~/my-envs/awq/lib/python3.10/importlib/metadata/__init__.py:946\u001b[0m, in \u001b[0;36mversion\u001b[0;34m(distribution_name)\u001b[0m\n\u001b[1;32m    939\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mversion\u001b[39m(distribution_name):\n\u001b[1;32m    940\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the version string for the named package.\u001b[39;00m\n\u001b[1;32m    941\u001b[0m \n\u001b[1;32m    942\u001b[0m \u001b[38;5;124;03m    :param distribution_name: The name of the distribution package to query.\u001b[39;00m\n\u001b[1;32m    943\u001b[0m \u001b[38;5;124;03m    :return: The version string for the package as defined in the package's\u001b[39;00m\n\u001b[1;32m    944\u001b[0m \u001b[38;5;124;03m        \"Version\" metadata key.\u001b[39;00m\n\u001b[1;32m    945\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 946\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdistribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdistribution_name\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mversion\n",
      "File \u001b[0;32m~/my-envs/awq/lib/python3.10/importlib/metadata/__init__.py:919\u001b[0m, in \u001b[0;36mdistribution\u001b[0;34m(distribution_name)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdistribution\u001b[39m(distribution_name):\n\u001b[1;32m    914\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the ``Distribution`` instance for the named package.\u001b[39;00m\n\u001b[1;32m    915\u001b[0m \n\u001b[1;32m    916\u001b[0m \u001b[38;5;124;03m    :param distribution_name: The name of the distribution package as a string.\u001b[39;00m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;124;03m    :return: A ``Distribution`` instance (or subclass thereof).\u001b[39;00m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDistribution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdistribution_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/my-envs/awq/lib/python3.10/importlib/metadata/__init__.py:518\u001b[0m, in \u001b[0;36mDistribution.from_name\u001b[0;34m(cls, name)\u001b[0m\n\u001b[1;32m    516\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m dist\n\u001b[1;32m    517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 518\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PackageNotFoundError(name)\n",
      "\u001b[0;31mPackageNotFoundError\u001b[0m: No package metadata was found for optimum"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "hf_download = \"gizmo-ai/Mixtral-8x7B-v0.1-GPTQ\"\n",
    "save_location  = \"/home/thsch026/masterarbeit/experiment/DSKD/model_hub/toms/Mixtral-8x7B-v0.1-GPTQ\"\n",
    "\n",
    "print (\"Start Download\")\n",
    "model = AutoModelForCausalLM.from_pretrained(hf_download)\n",
    "tokenizer = AutoTokenizer.from_pretrained(hf_download)\n",
    "print (\"Start saving model locally...\")\n",
    "model.save_pretrained(save_location, safetensors=True)\n",
    "tokenizer.save_pretrained(save_location)\n",
    "print (\"Saving complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff07be1-21d0-42ff-ab65-42d3f7ef530b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Snippet für das Merging des resultierenden student models (qlora3 kernel)\n",
    "- in der config datei des Adaptesr müssen Einträge wegen inkompatibilität enternt werden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dff2f712-cf09-41c1-afc8-aec53ebd3e4e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c10ea91336a740a38220a0005edece18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'peft.peft_model.PeftModelForCausalLM'>\n",
      "<class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'>\n",
      "Start saving the merged model to disc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thsch026/my-envs/qlora3/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving complete\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "# Local path of adapter model\n",
    "model_id = \"/home/thsch026/masterarbeit/experiment/DSKD/outputs/toms/Meta-Llama-3-8B-instruct_prune_lora/dual_space_kd_with_cma/adapter\"\n",
    "peft_model = AutoPeftModelForCausalLM.from_pretrained(model_id)\n",
    "print(type(peft_model))\n",
    "\n",
    "merged_model = peft_model.merge_and_unload()\n",
    "# The adapters are merged now and it is transformers class again\n",
    "print(type(merged_model))\n",
    "\n",
    "save_location  = \"/home/thsch026/masterarbeit/models/generated/dist/Meta-Llama-3-8B-Instruct_prune_lora_dist_phi\"\n",
    "tokenizer = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "print (\"Start saving the merged model to disc\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer)\n",
    "merged_model.save_pretrained(save_location, safetensors=True)\n",
    "tokenizer.save_pretrained(save_location)\n",
    "print (\"Saving complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c882a7-86dd-475e-ac0f-770e82f01caa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "awq",
   "language": "python",
   "name": "awq"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
