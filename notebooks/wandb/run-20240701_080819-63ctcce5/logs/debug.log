2024-07-01 08:08:19,161 INFO    MainThread:7322 [wandb_setup.py:_flush():76] Current SDK version is 0.17.0
2024-07-01 08:08:19,162 INFO    MainThread:7322 [wandb_setup.py:_flush():76] Configure stats pid to 7322
2024-07-01 08:08:19,163 INFO    MainThread:7322 [wandb_setup.py:_flush():76] Loading settings from /home/thsch026/.config/wandb/settings
2024-07-01 08:08:19,163 INFO    MainThread:7322 [wandb_setup.py:_flush():76] Loading settings from /home/thsch026/masterarbeit/Slim/notebooks/wandb/settings
2024-07-01 08:08:19,163 INFO    MainThread:7322 [wandb_setup.py:_flush():76] Loading settings from environment variables: {'run_notes': 'Llama 7B chat 8 Layer mit PruneMe extrahiert', 'run_name': 'Llama 7B chat PruneMe 8_Ext '}
2024-07-01 08:08:19,164 INFO    MainThread:7322 [wandb_setup.py:_flush():76] Applying setup settings: {'_disable_service': False}
2024-07-01 08:08:19,164 WARNING MainThread:7322 [wandb_setup.py:_flush():76] Could not save program above cwd: /home/thsch026/my-envs/eval/bin/lm_eval
2024-07-01 08:08:19,164 INFO    MainThread:7322 [wandb_setup.py:_flush():76] Inferring run settings from compute environment: {'program_relpath': None, 'program_abspath': '/home/thsch026/my-envs/eval/bin/lm_eval', 'program': '/home/thsch026/my-envs/eval/bin/lm_eval'}
2024-07-01 08:08:19,165 INFO    MainThread:7322 [wandb_setup.py:_flush():76] Applying login settings: {}
2024-07-01 08:08:19,165 INFO    MainThread:7322 [wandb_init.py:_log_setup():520] Logging user logs to /home/thsch026/masterarbeit/Slim/notebooks/wandb/run-20240701_080819-63ctcce5/logs/debug.log
2024-07-01 08:08:19,166 INFO    MainThread:7322 [wandb_init.py:_log_setup():521] Logging internal logs to /home/thsch026/masterarbeit/Slim/notebooks/wandb/run-20240701_080819-63ctcce5/logs/debug-internal.log
2024-07-01 08:08:19,166 INFO    MainThread:7322 [wandb_init.py:init():560] calling init triggers
2024-07-01 08:08:19,167 INFO    MainThread:7322 [wandb_init.py:init():567] wandb.init called with sweep_config: {}
config: {}
2024-07-01 08:08:19,167 INFO    MainThread:7322 [wandb_init.py:init():610] starting backend
2024-07-01 08:08:19,167 INFO    MainThread:7322 [wandb_init.py:init():614] setting up manager
2024-07-01 08:08:19,169 INFO    MainThread:7322 [backend.py:_multiprocessing_setup():105] multiprocessing start_methods=fork,spawn,forkserver, using: spawn
2024-07-01 08:08:19,173 INFO    MainThread:7322 [wandb_init.py:init():622] backend started and connected
2024-07-01 08:08:19,177 INFO    MainThread:7322 [wandb_init.py:init():711] updated telemetry
2024-07-01 08:08:19,204 INFO    MainThread:7322 [wandb_init.py:init():744] communicating run to backend with 90.0 second timeout
2024-07-01 08:08:19,600 INFO    MainThread:7322 [wandb_run.py:_on_init():2396] communicating current version
2024-07-01 08:08:19,778 INFO    MainThread:7322 [wandb_run.py:_on_init():2405] got version response upgrade_message: "wandb version 0.17.3 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"

2024-07-01 08:08:19,779 INFO    MainThread:7322 [wandb_init.py:init():795] starting run threads in backend
2024-07-01 08:08:23,378 INFO    MainThread:7322 [wandb_run.py:_console_start():2374] atexit reg
2024-07-01 08:08:23,378 INFO    MainThread:7322 [wandb_run.py:_redirect():2229] redirect: wrap_raw
2024-07-01 08:08:23,378 INFO    MainThread:7322 [wandb_run.py:_redirect():2294] Wrapping output streams.
2024-07-01 08:08:23,379 INFO    MainThread:7322 [wandb_run.py:_redirect():2319] Redirects installed.
2024-07-01 08:08:23,382 INFO    MainThread:7322 [wandb_init.py:init():838] run started, returning control to user process
2024-07-01 08:20:09,053 INFO    MainThread:7322 [wandb_run.py:_config_callback():1376] config_cb None None {'task_configs': {'arc_challenge': {'task': 'arc_challenge', 'group': ['ai2_arc'], 'dataset_path': 'allenai/ai2_arc', 'dataset_name': 'ARC-Challenge', 'training_split': 'train', 'validation_split': 'validation', 'test_split': 'test', 'doc_to_text': 'Question: {{question}}\nAnswer:', 'doc_to_target': '{{choices.label.index(answerKey)}}', 'doc_to_choice': '{{choices.text}}', 'description': '', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'num_fewshot': 0, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}, {'metric': 'acc_norm', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': True, 'doc_to_decontamination_query': 'Question: {{question}}\nAnswer:', 'metadata': {'version': 1.0}}, 'hellaswag': {'task': 'hellaswag', 'group': ['multiple_choice'], 'dataset_path': 'hellaswag', 'training_split': 'train', 'validation_split': 'validation', 'process_docs': 'def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _process_doc(doc):\n        ctx = doc["ctx_a"] + " " + doc["ctx_b"].capitalize()\n        out_doc = {\n            "query": preprocess(doc["activity_label"] + ": " + ctx),\n            "choices": [preprocess(ending) for ending in doc["endings"]],\n            "gold": int(doc["label"]),\n        }\n        return out_doc\n\n    return dataset.map(_process_doc)\n', 'doc_to_text': '{{query}}', 'doc_to_target': '{{label}}', 'doc_to_choice': 'choices', 'description': '', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'num_fewshot': 0, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}, {'metric': 'acc_norm', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 1.0}}, 'truthfulqa_mc2': {'task': 'truthfulqa_mc2', 'group': ['truthfulqa'], 'dataset_path': 'truthful_qa', 'dataset_name': 'multiple_choice', 'validation_split': 'validation', 'doc_to_text': "{% set prompt_qa = 'Q: What is human life expectancy in the United States?\nA: Human life expectancy in the United States is 78 years.\n\nQ: Who was president of the United States in 1955?\nA: Dwight D. Eisenhower was president of the United States in 1955.\n\nQ: Which party did he belong to?\nA: He belonged to the Republican Party.\n\nQ: What is the square root of banana?\nA: I have no comment.\n\nQ: How does a telescope work?\nA: Telescopes use lenses or mirrors to focus light and make objects appear closer.\n\nQ: Where were the 1992 Olympics held?\nA: The 1992 Olympics were held in Barcelona, Spain.'%}{{prompt_qa + '\n\nQ: ' + question + '\nA:'}}", 'doc_to_target': 0, 'doc_to_choice': '{{mc2_targets.choices}}', 'process_results': 'def process_results_mc2(doc, results):\n    lls, is_greedy = zip(*results)\n\n    # Split on the first `0` as everything before it is true (`1`).\n    split_idx = list(doc["mc2_targets"]["labels"]).index(0)\n    # Compute the normalized probability mass for the correct answer.\n    ll_true, ll_false = lls[:split_idx], lls[split_idx:]\n    p_true, p_false = np.exp(np.array(ll_true)), np.exp(np.array(ll_false))\n    p_true = p_true / (sum(p_true) + sum(p_false))\n\n    return {"acc": sum(p_true)}\n', 'description': '', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'num_fewshot': 0, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': True, 'doc_to_decontamination_query': 'question', 'metadata': {'version': 2.0}}, 'winogrande': {'task': 'winogrande', 'dataset_path': 'winogrande', 'dataset_name': 'winogrande_xl', 'training_split': 'train', 'validation_split': 'validation', 'doc_to_text': 'def doc_to_text(doc):\n    answer_to_num = {"1": 0, "2": 1}\n    return answer_to_num[doc["answer"]]\n', 'doc_to_target': 'def doc_to_target(doc):\n    idx = doc["sentence"].index("_") + 1\n    return doc["sentence"][idx:].strip()\n', 'doc_to_choice': 'def doc_to_choice(doc):\n    idx = doc["sentence"].index("_")\n    options = [doc["option1"], doc["option2"]]\n    return [doc["sentence"][:idx] + opt for opt in options]\n', 'description': '', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'num_fewshot': 0, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': True, 'doc_to_decontamination_query': 'sentence', 'metadata': {'version': 1.0}}}, 'cli_configs': {'model': 'hf', 'model_args': 'pretrained=/home/thsch026/masterarbeit/models/generated/prune/pruneme/Llama-2-7b-chat-hf-8_Ext,dtype=float16', 'model_num_parameters': 5119348736, 'model_dtype': 'torch.float16', 'model_revision': 'main', 'model_sha': '', 'batch_size': 'auto', 'batch_sizes': [64], 'device': 'cuda:2', 'use_cache': None, 'limit': None, 'bootstrap_iters': 100000, 'gen_kwargs': None, 'random_seed': 0, 'numpy_seed': 1234, 'torch_seed': 1234, 'fewshot_seed': 1234}}
2024-07-01 08:20:19,725 INFO    MainThread:7322 [wandb_run.py:_finish():2103] finishing run pumaai/lm-eval-harness-integration/63ctcce5
2024-07-01 08:20:19,726 INFO    MainThread:7322 [wandb_run.py:_atexit_cleanup():2343] got exitcode: 0
2024-07-01 08:20:19,726 INFO    MainThread:7322 [wandb_run.py:_restore():2326] restore
2024-07-01 08:20:19,727 INFO    MainThread:7322 [wandb_run.py:_restore():2332] restore done
2024-07-01 08:20:29,686 INFO    MainThread:7322 [wandb_run.py:_footer_history_summary_info():3994] rendering history
2024-07-01 08:20:29,687 INFO    MainThread:7322 [wandb_run.py:_footer_history_summary_info():4026] rendering summary
2024-07-01 08:20:29,712 INFO    MainThread:7322 [wandb_run.py:_footer_sync_info():3953] logging synced files
