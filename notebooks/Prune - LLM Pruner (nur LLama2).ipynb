{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd279318-8e81-423e-9645-b3999d5b5924",
   "metadata": {},
   "source": [
    "# LLM Pruner\n",
    "- Pruning eines lokalen Modells\n",
    "- nur bis LLama2 unterst√ºtzt\n",
    "- Source: https://github.com/horseee/LLM-Pruner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689bb4a1-f721-4ca9-baa3-9019a9883b29",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Vorbereitung\n",
    "- Use Kernel \"prune\"\n",
    "- be sure that torch==2.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabe341f-9d9c-46ce-a002-93d3ea0e3946",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# torch muss noch mal installiert werden\n",
    "!pip install torch==2.0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b397989-df8f-43de-b51f-8c74fecdd8d3",
   "metadata": {},
   "source": [
    "## Run pruning Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7c38c4c-d1af-43be-8ed1-3552bbc7255e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4203d926-3cbf-41d7-a45e-e7a127a73749",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/home/thsch026/masterarbeit/experiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d651ffb4-d5f3-4ec7-87a3-cea876cd2dd7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thsch026/my-envs/prune/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py\", line 304, in hf_raise_for_status\n",
      "    response.raise_for_status()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/models.py\", line 1021, in raise_for_status\n",
      "    raise HTTPError(http_error_msg, response=self)\n",
      "requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/model_id/resolve/main/tokenizer_config.json\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py\", line 398, in cached_file\n",
      "    resolved_file = hf_hub_download(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 119, in _inner_fn\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1403, in hf_hub_download\n",
      "    raise head_call_error\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1261, in hf_hub_download\n",
      "    metadata = get_hf_file_metadata(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 119, in _inner_fn\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1674, in get_hf_file_metadata\n",
      "    r = _request_wrapper(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 369, in _request_wrapper\n",
      "    response = _request_wrapper(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 393, in _request_wrapper\n",
      "    hf_raise_for_status(response)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py\", line 352, in hf_raise_for_status\n",
      "    raise RepositoryNotFoundError(message, response) from e\n",
      "huggingface_hub.utils._errors.RepositoryNotFoundError: 404 Client Error. (Request ID: Root=1-66897149-6a3f0b2a64359e240e76f708;647e937d-6566-460a-86cf-8c9cc59d2de6)\n",
      "\n",
      "Repository Not Found for url: https://huggingface.co/model_id/resolve/main/tokenizer_config.json.\n",
      "Please make sure you specified the correct `repo_id` and `repo_type`.\n",
      "If you are trying to access a private or gated repo, make sure you are authenticated.\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thsch026/masterarbeit/experiment/../experiment/LLM-Pruner/hf_prune.py\", line 314, in <module>\n",
      "    main(args)\n",
      "  File \"/home/thsch026/masterarbeit/experiment/../experiment/LLM-Pruner/hf_prune.py\", line 39, in main\n",
      "    tokenizer = LlamaTokenizer.from_pretrained(args.base_model)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 2010, in from_pretrained\n",
      "    resolved_config_file = cached_file(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py\", line 421, in cached_file\n",
      "    raise EnvironmentError(\n",
      "OSError: model_id is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n"
     ]
    }
   ],
   "source": [
    "model_id = 'meta-llama/Meta-Llama-3-8B-Instruct'\n",
    "AutoTokenizer.from_pretrained(model_id) \n",
    "\n",
    "!python ../experiment/LLM-Pruner/hf_prune.py --pruning_ratio 0.25 \\\n",
    "      --block_wise \\\n",
    "      --block_mlp_layer_start 4 --block_mlp_layer_end 30 \\\n",
    "      --block_attention_layer_start 4 --block_attention_layer_end 30 \\\n",
    "      --pruner_type taylor \\\n",
    "      --test_after_train \\\n",
    "      --device cuda  --eval_device cuda \\\n",
    "      --save_ckpt_log_name llama3_8B \\\n",
    "     --base_model model_id \\\n",
    "    --save_model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c50d7de-025a-42a2-bc9c-307682c57d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "p"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prune",
   "language": "python",
   "name": "prune"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
