{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f49296a4-a7bc-4f26-970f-edf7825ff99d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Verify torch version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96439398-2466-4b46-8478-03c7b95080d1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Prepare Environment (Should not be neccessary if lora kernel is used)\n",
    "- Alle Installationen durchfÃ¼hren und danach den Kernel neu starten. Es funktioniert nur in dieser Kombination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f3c52f7-e37b-4c2a-aa33-36b7ed4230fd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch==2.0.1\n",
      "  Using cached torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n",
      "Requirement already satisfied: tensorboard in /opt/conda/lib/python3.10/site-packages (2.11.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /home/thsch026/.local/lib/python3.10/site-packages (from torch==2.0.1) (4.11.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1) (1.11.1)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1) (3.1.2)\n",
      "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==2.0.1)\n",
      "  Using cached nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==2.0.1)\n",
      "  Using cached nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "Collecting nvidia-cuda-cupti-cu11==11.7.101 (from torch==2.0.1)\n",
      "  Using cached nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1) (8.5.0.96)\n",
      "Collecting nvidia-cublas-cu11==11.10.3.66 (from torch==2.0.1)\n",
      "  Using cached nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "Collecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.0.1)\n",
      "  Using cached nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
      "Collecting nvidia-curand-cu11==10.2.10.91 (from torch==2.0.1)\n",
      "  Using cached nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
      "Collecting nvidia-cusolver-cu11==11.4.0.1 (from torch==2.0.1)\n",
      "  Using cached nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
      "Collecting nvidia-cusparse-cu11==11.7.4.91 (from torch==2.0.1)\n",
      "  Using cached nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
      "Collecting nvidia-nccl-cu11==2.14.3 (from torch==2.0.1)\n",
      "  Using cached nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
      "Collecting nvidia-nvtx-cu11==11.7.91 (from torch==2.0.1)\n",
      "  Using cached nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
      "Requirement already satisfied: triton==2.0.0 in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1) (2.0.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (67.7.2)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (0.40.0)\n",
      "Requirement already satisfied: cmake in /opt/conda/lib/python3.10/site-packages (from triton==2.0.0->torch==2.0.1) (3.25.0)\n",
      "Requirement already satisfied: lit in /opt/conda/lib/python3.10/site-packages (from triton==2.0.0->torch==2.0.1) (15.0.7)\n",
      "Requirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.62.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (2.29.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.6)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.23.5)\n",
      "Requirement already satisfied: protobuf<4,>=3.9.2 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.19.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.0.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (0.4.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (2.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (2023.5.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.0.1) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.6.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (3.2.2)\n",
      "Installing collected packages: nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, torch\n",
      "  Attempting uninstall: nvidia-cublas-cu11\n",
      "    Found existing installation: nvidia-cublas-cu11 11.11.3.6\n",
      "    Uninstalling nvidia-cublas-cu11-11.11.3.6:\n",
      "      Successfully uninstalled nvidia-cublas-cu11-11.11.3.6\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.0.0+cu117\n",
      "    Uninstalling torch-2.0.0+cu117:\n",
      "      Successfully uninstalled torch-2.0.0+cu117\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.0.1+cu117 requires torch==2.0.0, but you have torch 2.0.1 which is incompatible.\n",
      "torchvision 0.15.1+cu117 requires torch==2.0.0, but you have torch 2.0.1 which is incompatible.\n",
      "transformer-smaller-training-vocab 0.4.0 requires torch!=2.0.1,<3.0.0,>=1.8.0, but you have torch 2.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 torch-2.0.1\n",
      "Collecting pydantic==2.0.0\n",
      "  Using cached pydantic-2.0-py3-none-any.whl (355 kB)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic==2.0.0) (0.6.0)\n",
      "Collecting pydantic-core==2.0.1 (from pydantic==2.0.0)\n",
      "  Using cached pydantic_core-2.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.5 MB)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /home/thsch026/.local/lib/python3.10/site-packages (from pydantic==2.0.0) (4.11.0)\n",
      "Installing collected packages: pydantic-core, pydantic\n",
      "  Attempting uninstall: pydantic-core\n",
      "    Found existing installation: pydantic_core 2.18.1\n",
      "    Uninstalling pydantic_core-2.18.1:\n",
      "      Successfully uninstalled pydantic_core-2.18.1\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 1.7.4\n",
      "    Uninstalling pydantic-1.7.4:\n",
      "      Successfully uninstalled pydantic-1.7.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "fastapi 0.110.2 requires pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4, but you have pydantic 2.0 which is incompatible.\n",
      "gradio 4.27.0 requires typer<1.0,>=0.12; sys_platform != \"emscripten\", but you have typer 0.3.2 which is incompatible.\n",
      "spacy 3.0.6 requires pydantic<1.8.0,>=1.7.1, but you have pydantic 2.0 which is incompatible.\n",
      "thinc 8.0.17 requires pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4, but you have pydantic 2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed pydantic-2.0 pydantic-core-2.0.1\n",
      "Collecting transformers==4.40.1\n",
      "  Using cached transformers-4.40.1-py3-none-any.whl (9.0 MB)\n",
      "Collecting datasets==2.18.0\n",
      "  Using cached datasets-2.18.0-py3-none-any.whl (510 kB)\n",
      "Requirement already satisfied: accelerate==0.29.3 in /opt/conda/lib/python3.10/site-packages (0.29.3)\n",
      "Requirement already satisfied: evaluate==0.4.1 in /opt/conda/lib/python3.10/site-packages (0.4.1)\n",
      "Collecting bitsandbytes==0.43.1\n",
      "  Using cached bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\n",
      "Requirement already satisfied: huggingface_hub==0.22.2 in /opt/conda/lib/python3.10/site-packages (0.22.2)\n",
      "Collecting trl==0.8.6\n",
      "  Using cached trl-0.8.6-py3-none-any.whl (245 kB)\n",
      "Collecting peft==0.10.0\n",
      "  Using cached peft-0.10.0-py3-none-any.whl (199 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.1) (3.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.1) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.1) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.1) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.1) (2024.4.16)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.1) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.1) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.1) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.1) (4.65.0)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.18.0) (12.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets==2.18.0) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.18.0) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.18.0) (2.0.1)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==2.18.0) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.18.0) (0.70.16)\n",
      "Requirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.18.0) (2023.5.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==2.18.0) (3.9.1)\n",
      "Requirement already satisfied: psutil in /home/thsch026/.local/lib/python3.10/site-packages (from accelerate==0.29.3) (5.9.8)\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.29.3) (2.0.1)\n",
      "Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from evaluate==0.4.1) (0.18.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/thsch026/.local/lib/python3.10/site-packages (from huggingface_hub==0.22.2) (4.11.0)\n",
      "Collecting tyro>=0.5.11 (from trl==0.8.6)\n",
      "  Using cached tyro-0.8.5-py3-none-any.whl (103 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.18.0) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.18.0) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.18.0) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.18.0) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.18.0) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.18.0) (4.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.40.1) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.40.1) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.40.1) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.40.1) (2023.5.7)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.29.3) (1.11.1)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.29.3) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.29.3) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.29.3) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.29.3) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.29.3) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.29.3) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.29.3) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.29.3) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.29.3) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.29.3) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.29.3) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.29.3) (2.14.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.29.3) (11.7.91)\n",
      "Requirement already satisfied: triton==2.0.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.29.3) (2.0.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10.0->accelerate==0.29.3) (67.7.2)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10.0->accelerate==0.29.3) (0.40.0)\n",
      "Requirement already satisfied: cmake in /opt/conda/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.10.0->accelerate==0.29.3) (3.25.0)\n",
      "Requirement already satisfied: lit in /opt/conda/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.10.0->accelerate==0.29.3) (15.0.7)\n",
      "Collecting docstring-parser>=0.16 (from tyro>=0.5.11->trl==0.8.6)\n",
      "  Using cached docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
      "Requirement already satisfied: rich>=11.1.0 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.8.6) (13.7.1)\n",
      "Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl==0.8.6)\n",
      "  Using cached shtab-1.7.1-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/thsch026/.local/lib/python3.10/site-packages (from pandas->datasets==2.18.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.18.0) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.18.0) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/thsch026/.local/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.18.0) (1.16.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.8.6) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/thsch026/.local/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.8.6) (2.17.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate==0.29.3) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate==0.29.3) (1.3.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl==0.8.6) (0.1.2)\n",
      "Installing collected packages: shtab, docstring-parser, tyro, transformers, datasets, trl, peft, bitsandbytes\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.40.0\n",
      "    Uninstalling transformers-4.40.0:\n",
      "      Successfully uninstalled transformers-4.40.0\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 2.19.0\n",
      "    Uninstalling datasets-2.19.0:\n",
      "      Successfully uninstalled datasets-2.19.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "transformer-smaller-training-vocab 0.4.0 requires torch!=2.0.1,<3.0.0,>=1.8.0, but you have torch 2.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed bitsandbytes-0.43.1 datasets-2.18.0 docstring-parser-0.16 peft-0.10.0 shtab-1.7.1 transformers-4.40.1 trl-0.8.6 tyro-0.8.5\n"
     ]
    }
   ],
   "source": [
    "# Install Pytorch for FSDP and FA/SDPA\n",
    "!pip install torch==2.0.1 tensorboard\n",
    "#pip install tensorboard datasets\n",
    " \n",
    "# Install Hugging Face libraries\n",
    "!pip install pydantic==2.0.0\n",
    "!pip install  --upgrade \"transformers==4.40.1\" \"datasets==2.18.0\" \"accelerate==0.29.3\" \"evaluate==0.4.1\" \"bitsandbytes==0.43.1\" \"huggingface_hub==0.22.2\" \"trl==0.8.6\" \"peft==0.10.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14f6a6c3-a099-4a58-a13d-31cc62eff4e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch-revgrad           0.2.0                    pypi_0    pypi\n",
      "torch                     2.0.1                    pypi_0    pypi\n",
      "torchaudio                2.0.1+cu117              pypi_0    pypi\n",
      "torchvision               0.15.1+cu117             pypi_0    pypi\n",
      "adapter-transformers      3.0.1                    pypi_0    pypi\n",
      "transformers              4.40.1                   pypi_0    pypi\n",
      "pydantic                  2.0                      pypi_0    pypi\n",
      "pydantic-core             2.0.1                    pypi_0    pypi\n"
     ]
    }
   ],
   "source": [
    "# Double check environment\n",
    "# torch must be 2.0.0, transfomers must be 4.40, pydantic must be 2.0.0\n",
    "!conda list | grep torch\n",
    "!conda list | grep transformers\n",
    "!conda list | grep pydantic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b08633-4dc0-42ec-8948-67bae2e38fd9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Wahrscheinlich obsolet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8734781-8c0a-4c90-99e9-e17b94173e09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install transformers==4.40.1 torch==2.0.1 trl peft tensorboard pydantic==2.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b013a1bf-840d-455d-8143-436ca637ff2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install transformers==4.40.1\n",
    "!pip install torch==2.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f3b0bb-aadc-46d0-9082-5cd8a8509502",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install trl\n",
    "!pip install peft"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f07eb34-06ae-4609-807d-907b9ccfc5f0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Login to hugging face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be3c760-fbfb-4db0-b34f-e0dabfd4c5fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!huggingface-cli login --token \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4548a057-94c5-45e6-80ab-134986bcd9b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a5ffaa99-fe34-4eff-a2e2-de298aec8088",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Free GPU Memory\n",
    "- Alternatively > Restart Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726343b5-4fea-490d-a005-b2c5256dfd78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "del model\n",
    "del trainer\n",
    "torch.cuda.empty_cache()\n",
    "print(torch.cuda.memory_reserved(0))\n",
    "print(torch.cuda.memory_allocated(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06adbd9f-cf3b-46fb-912d-a8d4a9602c66",
   "metadata": {
    "tags": []
   },
   "source": [
    "##  Workflow for Lora tuning\n",
    "- Runs also well with \"qlora3\" kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99735949-c7be-437c-8539-216299cf792b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Set environment and Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24f738ba-985c-43e5-8c38-5381956b1597",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:256\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "!echo $CUDA_VISIBLE_DEVICES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07d26a0-efd3-451e-852d-f4bd740a2a99",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.2 Try to use parallel mode if GPUs are available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1dac8097-4db6-419c-a4e3-a71b91a33040",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if torch.cuda.device_count() > 1:\n",
    "    \n",
    "    model = torch.nn.DataParallel(model)\n",
    "\n",
    "print(\"CUDA: \", torch.cuda.is_available())\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a72d2d46-b709-435a-8440-b8707dda3eda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import wandb\n",
    "import transformers\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model, PeftModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ee84d6-63e2-4a9a-bbc9-88848919d926",
   "metadata": {},
   "source": [
    "## 1. Choose Model for workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f9ad3d-d142-4d29-987f-a86642c25b17",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### For Llama 3 8B HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a727218d-aeda-44d8-8e7f-c74d5c71d6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_location = \"meta-llama/Meta-Llama-3-8B\"\n",
    "ouput_dir = \"/home/thsch026/masterarbeit/models/generated/lora/Meta-Llama-3-8B_ms-marco\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d669ba4-157b-42c2-a2f9-b99ad6e64731",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Llama 2 7B chat hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2335b461-6e22-46eb-bd30-4a8a84d67d1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_location = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "output_model = \"/home/thsch026/masterarbeit/models/generated/lora/Llama-2-7b-chat_ms-marco\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567b3c1a-d76f-4c1b-a020-80267f8b9fc1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Mistral 7B Instruct v0.2 - Dataset MS_Marco V1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f832e54-3a27-4fd4-8d21-499a27563038",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_location = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "output_model = \"/home/thsch026/masterarbeit/models/generated/lora/Mistral-7B-Instruct_ms-marco\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb09164-c91b-471d-afe8-c002f4c95ac1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Initalize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94a9ba11-eb80-4d15-a06c-6aa5837f5212",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thsch026/my-envs/qlora3/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "475edce36d0f4da4941e38ca9c0395c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_location)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_location,\n",
    "   # load_in_8bit=True, # was 8bit\n",
    "    device_map=\"cuda\", # was auto\n",
    "    force_download=False,\n",
    "    torch_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0519acb-62e4-4464-8e8b-f9590395d48d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "tokenizer.pad_token = \"!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a3b13c-c64b-40e5-9b7b-9e048e9c5456",
   "metadata": {},
   "source": [
    "## 3. Configure Lora Settings\n",
    "For explanation of the values for LORA configuration see below:\n",
    "- https://medium.com/@drishtisharma96505/comparative-analysis-of-lora-parameters-on-llama-2-with-flash-attention-574b913295d4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b20e5a88-4593-4e04-945a-4bf9ef46df10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "CUTOFF_LEN = 768\n",
    "LORA_R = 8\n",
    "LORA_ALPHA = 2 * LORA_R\n",
    "LORA_DROPOUT = 0.1\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\"\n",
    "                    , \"down_proj\", \"lm_head\"], #these are the  names for the layers\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b877f0-24e5-4a59-bdb8-4ee5ed0f3557",
   "metadata": {},
   "source": [
    "## 4. Prepare dataset for finetuning\n",
    "Here I use a general dataset from Microsoft: MS_marco (v1.1) for the finetuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6520ce8a-ec0b-4d8c-8cdd-99ea9fe46b31",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset DatasetDict({\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'passages', 'query', 'query_id', 'query_type', 'wellFormedAnswers'],\n",
      "        num_rows: 10047\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'passages', 'query', 'query_id', 'query_type', 'wellFormedAnswers'],\n",
      "        num_rows: 82326\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'passages', 'query', 'query_id', 'query_type', 'wellFormedAnswers'],\n",
      "        num_rows: 9650\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('ms_marco','v1.1') # General dataset\n",
    "print(\"dataset\", dataset)\n",
    "train_data = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439d5c91-d3b3-4975-83d5-6e1cc7d25da4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4.1 Select the right prompt structure for the model you want use LORA on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409c9bdf-9169-440e-b2b4-68153c6f62f5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Prompt Structure for Mistral 7 B Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7f036da-bada-4619-b222-f761adb0f384",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_prompt(user_query):  #The prompt format is taken from the official Mistral huggingface page\n",
    "    if user_query[\"answers\"] is not None and user_query[\"query\"] is not None:\n",
    "      p =  \"<s> [INST]\" + str(user_query[\"query\"]) + \"[/INST]\" +  str(user_query[\"answers\"]) + \"</s>\"\n",
    "      return p\n",
    "    else:\n",
    "        p = \"<s> [INST]\" + \"Hello\" + \"[/INST]\" +  \"Hello\" + \"</s>\"\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361d05aa-1e31-4409-84ae-fda25bb8465e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Prompt Structure for Llama3 8B Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825729a4-562a-4806-ac72-df50a9ca2e02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_prompt(user_query):  #The prompt format is taken from the official Meta Llama3 page\n",
    "    if user_query[\"answers\"] is not None and user_query[\"query\"] is not None:\n",
    "        p = \"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\" + str(user_query[\"query\"]) + \"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\n",
    "    else\n",
    "        p = \"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\" + \"Hello!\" + \"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c802472f-ae60-46b4-89e6-5039b618ba3b",
   "metadata": {},
   "source": [
    "#### Prompt Structure for Llama2 7B chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b3f8b10-6830-4cce-a283-02497da508f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_prompt(user_query):  #The prompt format is taken from the official Meta Llama3 page\n",
    "    if user_query[\"answers\"] is not None and user_query[\"query\"] is not None:\n",
    "        p = \"<s>[INST]\" + str(user_query[\"query\"]) + \"[/INST]\"\n",
    "    else:\n",
    "        p = \"<s>[INST]\" + \"Hello!\" + \"[/INST]\"\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7e5921-b8f1-43a6-bd71-41069f8374cf",
   "metadata": {},
   "source": [
    "### 4.2 Tokenize the train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae6919af-5263-4875-88bb-f7e088926d4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize(prompt):\n",
    "    return tokenizer(\n",
    "        prompt + tokenizer.eos_token,\n",
    "        truncation=True,\n",
    "        max_length=CUTOFF_LEN ,\n",
    "        padding=\"max_length\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68acb15a-5ea5-41ad-b9d5-0a7041db776e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bf1b6bd1dcc4c99987f40f1790cd25d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9650 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data = train_data.shuffle().map(lambda x: tokenize(generate_prompt(x)), remove_columns=['answers', 'passages', 'query', 'query_id', 'query_type', 'wellFormedAnswers'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4d58c2-fa78-4e54-b8e0-7d61e5bf7b43",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 5. Prepare trainer object and start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "050cd7a2-3755-4d82-8fa4-e2cd86f5dcdf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model, \n",
    "    train_dataset=train_data,\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=4,\n",
    "        num_train_epochs=3,    # 3 or 6 is good\n",
    "        learning_rate=1e-4,\n",
    "        logging_steps=100,\n",
    "        optim=\"adamw_torch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        output_dir=output_model\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")\n",
    "model.config.use_cache = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "976dda06-388a-4341-afe8-0e5cddc2d7d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mthomas-t-schmitt\u001b[0m (\u001b[33mpumaai\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/thsch026/masterarbeit/Slim/notebooks/wandb/run-20240712_073307-6sqekm8t</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/pumaai/huggingface/runs/6sqekm8t' target=\"_blank\">devout-river-142</a></strong> to <a href='https://wandb.ai/pumaai/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/pumaai/huggingface' target=\"_blank\">https://wandb.ai/pumaai/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/pumaai/huggingface/runs/6sqekm8t' target=\"_blank\">https://wandb.ai/pumaai/huggingface/runs/6sqekm8t</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7236' max='7236' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7236/7236 11:50:13, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.280800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.516500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.524300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.491800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.538300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.479700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.544000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.485200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.517400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.507800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.517700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.508000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.468600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.477800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.464500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.472600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>1.501900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>1.516600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>1.485500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.473000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>1.490100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>1.483100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>1.463100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>1.482100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.289200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>1.289400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>1.282400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>1.257500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>1.274800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.268200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>1.235400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>1.259000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>1.246500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>1.258700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.232400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>1.240900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>1.247700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>1.222200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>1.246600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.195800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>1.203800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>1.228900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>1.209000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>1.190900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>1.209600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>1.204100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>1.193100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>1.190700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>1.064300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.001800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>1.013400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>1.042400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>1.003400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>1.016300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.985600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>1.021500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>1.008400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.991900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>0.999400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>1.018500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>0.993300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>0.985300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>1.004800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>0.990700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.967900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>0.995000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>0.991100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>0.988000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>1.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.998600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7100</td>\n",
       "      <td>0.992100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>0.983000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7236, training_loss=1.254859856836963, metrics={'train_runtime': 42635.2116, 'train_samples_per_second': 0.679, 'train_steps_per_second': 0.17, 'total_flos': 8.839519869893345e+17, 'train_loss': 1.254859856836963, 'epoch': 2.999378238341969})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a914132d-e561-43c3-92a2-d942b7f818be",
   "metadata": {},
   "source": [
    "## 6. Merge Adapter to original Model and save\n",
    "Merges the specified adapter generated by the wokflow above with the original model.\n",
    "\n",
    "- \"adapter_config.json\" mut may be edited depending on peft version\n",
    "- \"model_id\" is the lora adapter that was created in the steps above\n",
    "- you may must copy the files from the latest checkpoint to the directory specified in \"model_id\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ebb7211-c41a-4c4d-830e-91030f8aea67",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thsch026/my-envs/qlora3/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ff21693f4034c0bbab9ad3a9937c024",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'peft.peft_model.PeftModelForCausalLM'>\n",
      "<class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'>\n"
     ]
    }
   ],
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "# Local path of adapter model\n",
    "model_id = \"/home/thsch026/masterarbeit/models/generated/lora/Llama-2-7b-chat_ms-marco/checkpoint-7236\"\n",
    "peft_model = AutoPeftModelForCausalLM.from_pretrained(model_id)\n",
    "print(type(peft_model))\n",
    "\n",
    "merged_model = peft_model.merge_and_unload()\n",
    "# The adapters are merged now and it is transformers class again\n",
    "print(type(merged_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c369baf9-4027-4be5-bd91-521c3dc8d197",
   "metadata": {},
   "source": [
    "## 7. Save the merged model together with the tokenizer of the original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "49da9426-c83c-421e-9ab8-9b150adfc815",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thsch026/my-envs/qlora3/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('/home/thsch026/masterarbeit/models/generated/lora/Llama-2-7b-chat_ms-marco/model/tokenizer_config.json',\n",
       " '/home/thsch026/masterarbeit/models/generated/lora/Llama-2-7b-chat_ms-marco/model/special_tokens_map.json',\n",
       " '/home/thsch026/masterarbeit/models/generated/lora/Llama-2-7b-chat_ms-marco/model/tokenizer.json')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dir = \"/home/thsch026/masterarbeit/models/generated/lora/Llama-2-7b-chat_ms-marco/model\"\n",
    "\n",
    "merged_model.save_pretrained(output_dir)\n",
    "tokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097cfbee-4c0f-479c-9a29-19c8cfff871b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qlora3",
   "language": "python",
   "name": "qlora3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
