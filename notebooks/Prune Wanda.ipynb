{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92873e25-1e6a-4f3f-a82e-89ef7b3aae2b",
   "metadata": {},
   "source": [
    "# Wanda Pruning\n",
    "- Source: https://github.com/locuslab/wanda\n",
    "- Paper: @article{sun2023wanda,\n",
    "  title={A Simple and Effective Pruning Approach for Large Language Models}, \n",
    "  author={Sun, Mingjie and Liu, Zhuang and Bair, Anna and Kolter, J. Zico},\n",
    "  year={2023},\n",
    "  journal={arXiv preprint arXiv:2306.11695}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd559bb-264f-41f4-9979-efc9836f00d6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Activate Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "207fb5e2-c569-47bc-89fb-db623fb042d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(/home/thsch026/my-envs/prune) \n"
     ]
    }
   ],
   "source": [
    "conda activate prune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96f2d0e9-d216-42e6-9844-5c6ec4a79cef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(/home/thsch026/my-envs/prune) \n"
     ]
    }
   ],
   "source": [
    "cd /home/thsch026/masterarbeit/experiment/wanda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba5c4e0-f622-4485-9d40-469797b6c9f2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Prepare Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9561c05d-f649-4e9e-ade7-05386cc7f6b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install transformers==4.28.0 datasets==2.11.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d59657c-22fd-432a-aef0-76298027e4ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install accelerate==0.18.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954d6f4d-4b2f-4650-8611-7c00ea60c852",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install --upgrade datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbd5503-3ad4-4183-9b25-41039b4b68c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install SentencePiece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6973636-04de-4698-95be-21270264d1a2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Workaround for problems with dataset loading\n",
    "Die nachdolgenden Zeilen müssen im Script data.py (liegt im Verzeichnis wanda/lib) ausgetauscht werden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866281e6-8cc2-4f99-af45-535615fac9bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "traindata = load_dataset('allenai/c4', data_files={'train': 'en/c4-train.00000-of-01024.json.gz'}, split='train')\n",
    "valdata = load_dataset('allenai/c4', data_files={'validation': 'en/c4-validation.00000-of-00008.json.gz'}, split='validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f106c6cc-e295-4e75-9f62-85dc842b5502",
   "metadata": {},
   "source": [
    "## Durchführen des Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095beb42-f027-46c6-98e0-ae285988954d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4ee37a5-c315-482c-a80b-0876f77b80f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(/home/thsch026/my-envs/prune) \n",
      "(/home/thsch026/my-envs/prune) \n",
      "(/home/thsch026/my-envs/prune) \n",
      "(/home/thsch026/my-envs/prune) \n",
      "(/home/thsch026/my-envs/prune) \n",
      "(/home/thsch026/my-envs/prune) \n",
      "(/home/thsch026/my-envs/prune) \n",
      "(/home/thsch026/my-envs/prune) \n",
      "(/home/thsch026/my-envs/prune) \n",
      "(/home/thsch026/my-envs/prune) \n",
      "(/home/thsch026/my-envs/prune) \n"
     ]
    }
   ],
   "source": [
    "conda activate prune\n",
    "export SCRIPTDIR=\"/home/thsch026/masterarbeit/experiment/wanda\"\n",
    "export SAVEMODEL=\"/home/thsch026/masterarbeit/models/generated/prune/wanda/llama2\"\n",
    "\n",
    "\n",
    "cd  $SCRIPTDIR\n",
    "# Chhose Model to prune\n",
    "export MODEL=\"/home/thsch026/masterarbeit/models/llama2/llama-2-7b-chat-hf\"\n",
    "\n",
    "\n",
    "#export MODEL=\"mistralai/Mistral-7B-Instruct-v0.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d0b7444-aaee-40d5-aba8-ba5de102a02a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch 2.0.0\n",
      "transformers 4.28.0\n",
      "accelerate 0.18.0\n",
      "# of gpus:  1\n",
      "loading llm model /home/thsch026/masterarbeit/models/llama2/llama-2-7b-chat-hf\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:50<00:00, 16.79s/it]\n",
      "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at /home/thsch026/masterarbeit/models/llama2/llama-2-7b-chat-hf and are newly initialized: ['model.layers.27.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.23.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.25.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.28.self_attn.rotary_emb.inv_freq', 'model.layers.31.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.26.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.24.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.30.self_attn.rotary_emb.inv_freq', 'model.layers.22.self_attn.rotary_emb.inv_freq', 'model.layers.29.self_attn.rotary_emb.inv_freq']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "use device  cuda:0\n",
      "pruning starts\n",
      "loading calibdation data\n",
      "dataset loading complete\n",
      "pruning layer 0 name self_attn.q_proj\n",
      "pruning layer 0 name self_attn.k_proj\n",
      "pruning layer 0 name self_attn.v_proj\n",
      "pruning layer 0 name self_attn.o_proj\n",
      "pruning layer 0 name mlp.gate_proj\n",
      "pruning layer 0 name mlp.down_proj\n",
      "pruning layer 0 name mlp.up_proj\n",
      "pruning layer 1 name self_attn.q_proj\n",
      "pruning layer 1 name self_attn.k_proj\n",
      "pruning layer 1 name self_attn.v_proj\n",
      "pruning layer 1 name self_attn.o_proj\n",
      "pruning layer 1 name mlp.gate_proj\n",
      "pruning layer 1 name mlp.down_proj\n",
      "pruning layer 1 name mlp.up_proj\n",
      "pruning layer 2 name self_attn.q_proj\n",
      "pruning layer 2 name self_attn.k_proj\n",
      "pruning layer 2 name self_attn.v_proj\n",
      "pruning layer 2 name self_attn.o_proj\n",
      "pruning layer 2 name mlp.gate_proj\n",
      "pruning layer 2 name mlp.down_proj\n",
      "pruning layer 2 name mlp.up_proj\n",
      "pruning layer 3 name self_attn.q_proj\n",
      "pruning layer 3 name self_attn.k_proj\n",
      "pruning layer 3 name self_attn.v_proj\n",
      "pruning layer 3 name self_attn.o_proj\n",
      "pruning layer 3 name mlp.gate_proj\n",
      "pruning layer 3 name mlp.down_proj\n",
      "pruning layer 3 name mlp.up_proj\n",
      "pruning layer 4 name self_attn.q_proj\n",
      "pruning layer 4 name self_attn.k_proj\n",
      "pruning layer 4 name self_attn.v_proj\n",
      "pruning layer 4 name self_attn.o_proj\n",
      "pruning layer 4 name mlp.gate_proj\n",
      "pruning layer 4 name mlp.down_proj\n",
      "pruning layer 4 name mlp.up_proj\n",
      "pruning layer 5 name self_attn.q_proj\n",
      "pruning layer 5 name self_attn.k_proj\n",
      "pruning layer 5 name self_attn.v_proj\n",
      "pruning layer 5 name self_attn.o_proj\n",
      "pruning layer 5 name mlp.gate_proj\n",
      "pruning layer 5 name mlp.down_proj\n",
      "pruning layer 5 name mlp.up_proj\n",
      "pruning layer 6 name self_attn.q_proj\n",
      "pruning layer 6 name self_attn.k_proj\n",
      "pruning layer 6 name self_attn.v_proj\n",
      "pruning layer 6 name self_attn.o_proj\n",
      "pruning layer 6 name mlp.gate_proj\n",
      "pruning layer 6 name mlp.down_proj\n",
      "pruning layer 6 name mlp.up_proj\n",
      "pruning layer 7 name self_attn.q_proj\n",
      "pruning layer 7 name self_attn.k_proj\n",
      "pruning layer 7 name self_attn.v_proj\n",
      "pruning layer 7 name self_attn.o_proj\n",
      "pruning layer 7 name mlp.gate_proj\n",
      "pruning layer 7 name mlp.down_proj\n",
      "pruning layer 7 name mlp.up_proj\n",
      "pruning layer 8 name self_attn.q_proj\n",
      "pruning layer 8 name self_attn.k_proj\n",
      "pruning layer 8 name self_attn.v_proj\n",
      "pruning layer 8 name self_attn.o_proj\n",
      "pruning layer 8 name mlp.gate_proj\n",
      "pruning layer 8 name mlp.down_proj\n",
      "pruning layer 8 name mlp.up_proj\n",
      "pruning layer 9 name self_attn.q_proj\n",
      "pruning layer 9 name self_attn.k_proj\n",
      "pruning layer 9 name self_attn.v_proj\n",
      "pruning layer 9 name self_attn.o_proj\n",
      "pruning layer 9 name mlp.gate_proj\n",
      "pruning layer 9 name mlp.down_proj\n",
      "pruning layer 9 name mlp.up_proj\n",
      "pruning layer 10 name self_attn.q_proj\n",
      "pruning layer 10 name self_attn.k_proj\n",
      "pruning layer 10 name self_attn.v_proj\n",
      "pruning layer 10 name self_attn.o_proj\n",
      "pruning layer 10 name mlp.gate_proj\n",
      "pruning layer 10 name mlp.down_proj\n",
      "pruning layer 10 name mlp.up_proj\n",
      "pruning layer 11 name self_attn.q_proj\n",
      "pruning layer 11 name self_attn.k_proj\n",
      "pruning layer 11 name self_attn.v_proj\n",
      "pruning layer 11 name self_attn.o_proj\n",
      "pruning layer 11 name mlp.gate_proj\n",
      "pruning layer 11 name mlp.down_proj\n",
      "pruning layer 11 name mlp.up_proj\n",
      "pruning layer 12 name self_attn.q_proj\n",
      "pruning layer 12 name self_attn.k_proj\n",
      "pruning layer 12 name self_attn.v_proj\n",
      "pruning layer 12 name self_attn.o_proj\n",
      "pruning layer 12 name mlp.gate_proj\n",
      "pruning layer 12 name mlp.down_proj\n",
      "pruning layer 12 name mlp.up_proj\n",
      "pruning layer 13 name self_attn.q_proj\n",
      "pruning layer 13 name self_attn.k_proj\n",
      "pruning layer 13 name self_attn.v_proj\n",
      "pruning layer 13 name self_attn.o_proj\n",
      "pruning layer 13 name mlp.gate_proj\n",
      "pruning layer 13 name mlp.down_proj\n",
      "pruning layer 13 name mlp.up_proj\n",
      "pruning layer 14 name self_attn.q_proj\n",
      "pruning layer 14 name self_attn.k_proj\n",
      "pruning layer 14 name self_attn.v_proj\n",
      "pruning layer 14 name self_attn.o_proj\n",
      "pruning layer 14 name mlp.gate_proj\n",
      "pruning layer 14 name mlp.down_proj\n",
      "pruning layer 14 name mlp.up_proj\n",
      "pruning layer 15 name self_attn.q_proj\n",
      "pruning layer 15 name self_attn.k_proj\n",
      "pruning layer 15 name self_attn.v_proj\n",
      "pruning layer 15 name self_attn.o_proj\n",
      "pruning layer 15 name mlp.gate_proj\n",
      "pruning layer 15 name mlp.down_proj\n",
      "pruning layer 15 name mlp.up_proj\n",
      "pruning layer 16 name self_attn.q_proj\n",
      "pruning layer 16 name self_attn.k_proj\n",
      "pruning layer 16 name self_attn.v_proj\n",
      "pruning layer 16 name self_attn.o_proj\n",
      "pruning layer 16 name mlp.gate_proj\n",
      "pruning layer 16 name mlp.down_proj\n",
      "pruning layer 16 name mlp.up_proj\n",
      "pruning layer 17 name self_attn.q_proj\n",
      "pruning layer 17 name self_attn.k_proj\n",
      "pruning layer 17 name self_attn.v_proj\n",
      "pruning layer 17 name self_attn.o_proj\n",
      "pruning layer 17 name mlp.gate_proj\n",
      "pruning layer 17 name mlp.down_proj\n",
      "pruning layer 17 name mlp.up_proj\n",
      "pruning layer 18 name self_attn.q_proj\n",
      "pruning layer 18 name self_attn.k_proj\n",
      "pruning layer 18 name self_attn.v_proj\n",
      "pruning layer 18 name self_attn.o_proj\n",
      "pruning layer 18 name mlp.gate_proj\n",
      "pruning layer 18 name mlp.down_proj\n",
      "pruning layer 18 name mlp.up_proj\n",
      "pruning layer 19 name self_attn.q_proj\n",
      "pruning layer 19 name self_attn.k_proj\n",
      "pruning layer 19 name self_attn.v_proj\n",
      "pruning layer 19 name self_attn.o_proj\n",
      "pruning layer 19 name mlp.gate_proj\n",
      "pruning layer 19 name mlp.down_proj\n",
      "pruning layer 19 name mlp.up_proj\n",
      "pruning layer 20 name self_attn.q_proj\n",
      "pruning layer 20 name self_attn.k_proj\n",
      "pruning layer 20 name self_attn.v_proj\n",
      "pruning layer 20 name self_attn.o_proj\n",
      "pruning layer 20 name mlp.gate_proj\n",
      "pruning layer 20 name mlp.down_proj\n",
      "pruning layer 20 name mlp.up_proj\n",
      "pruning layer 21 name self_attn.q_proj\n",
      "pruning layer 21 name self_attn.k_proj\n",
      "pruning layer 21 name self_attn.v_proj\n",
      "pruning layer 21 name self_attn.o_proj\n",
      "pruning layer 21 name mlp.gate_proj\n",
      "pruning layer 21 name mlp.down_proj\n",
      "pruning layer 21 name mlp.up_proj\n",
      "pruning layer 22 name self_attn.q_proj\n",
      "pruning layer 22 name self_attn.k_proj\n",
      "pruning layer 22 name self_attn.v_proj\n",
      "pruning layer 22 name self_attn.o_proj\n",
      "pruning layer 22 name mlp.gate_proj\n",
      "pruning layer 22 name mlp.down_proj\n",
      "pruning layer 22 name mlp.up_proj\n",
      "pruning layer 23 name self_attn.q_proj\n",
      "pruning layer 23 name self_attn.k_proj\n",
      "pruning layer 23 name self_attn.v_proj\n",
      "pruning layer 23 name self_attn.o_proj\n",
      "pruning layer 23 name mlp.gate_proj\n",
      "pruning layer 23 name mlp.down_proj\n",
      "pruning layer 23 name mlp.up_proj\n",
      "pruning layer 24 name self_attn.q_proj\n",
      "pruning layer 24 name self_attn.k_proj\n",
      "pruning layer 24 name self_attn.v_proj\n",
      "pruning layer 24 name self_attn.o_proj\n",
      "pruning layer 24 name mlp.gate_proj\n",
      "pruning layer 24 name mlp.down_proj\n",
      "pruning layer 24 name mlp.up_proj\n",
      "pruning layer 25 name self_attn.q_proj\n",
      "pruning layer 25 name self_attn.k_proj\n",
      "pruning layer 25 name self_attn.v_proj\n",
      "pruning layer 25 name self_attn.o_proj\n",
      "pruning layer 25 name mlp.gate_proj\n",
      "pruning layer 25 name mlp.down_proj\n",
      "pruning layer 25 name mlp.up_proj\n",
      "pruning layer 26 name self_attn.q_proj\n",
      "pruning layer 26 name self_attn.k_proj\n",
      "pruning layer 26 name self_attn.v_proj\n",
      "pruning layer 26 name self_attn.o_proj\n",
      "pruning layer 26 name mlp.gate_proj\n",
      "pruning layer 26 name mlp.down_proj\n",
      "pruning layer 26 name mlp.up_proj\n",
      "pruning layer 27 name self_attn.q_proj\n",
      "pruning layer 27 name self_attn.k_proj\n",
      "pruning layer 27 name self_attn.v_proj\n",
      "pruning layer 27 name self_attn.o_proj\n",
      "pruning layer 27 name mlp.gate_proj\n",
      "pruning layer 27 name mlp.down_proj\n",
      "pruning layer 27 name mlp.up_proj\n",
      "pruning layer 28 name self_attn.q_proj\n",
      "pruning layer 28 name self_attn.k_proj\n",
      "pruning layer 28 name self_attn.v_proj\n",
      "pruning layer 28 name self_attn.o_proj\n",
      "pruning layer 28 name mlp.gate_proj\n",
      "pruning layer 28 name mlp.down_proj\n",
      "pruning layer 28 name mlp.up_proj\n",
      "pruning layer 29 name self_attn.q_proj\n",
      "pruning layer 29 name self_attn.k_proj\n",
      "pruning layer 29 name self_attn.v_proj\n",
      "pruning layer 29 name self_attn.o_proj\n",
      "pruning layer 29 name mlp.gate_proj\n",
      "pruning layer 29 name mlp.down_proj\n",
      "pruning layer 29 name mlp.up_proj\n",
      "pruning layer 30 name self_attn.q_proj\n",
      "pruning layer 30 name self_attn.k_proj\n",
      "pruning layer 30 name self_attn.v_proj\n",
      "pruning layer 30 name self_attn.o_proj\n",
      "pruning layer 30 name mlp.gate_proj\n",
      "pruning layer 30 name mlp.down_proj\n",
      "pruning layer 30 name mlp.up_proj\n",
      "pruning layer 31 name self_attn.q_proj\n",
      "pruning layer 31 name self_attn.k_proj\n",
      "pruning layer 31 name self_attn.v_proj\n",
      "pruning layer 31 name self_attn.o_proj\n",
      "pruning layer 31 name mlp.gate_proj\n",
      "pruning layer 31 name mlp.down_proj\n",
      "pruning layer 31 name mlp.up_proj\n",
      "******************************\n",
      "layer 0 sparsity 0.500000\n",
      "layer 1 sparsity 0.500000\n",
      "layer 2 sparsity 0.500000\n",
      "layer 3 sparsity 0.500000\n",
      "layer 4 sparsity 0.500000\n",
      "layer 5 sparsity 0.500000\n",
      "layer 6 sparsity 0.500000\n",
      "layer 7 sparsity 0.500000\n",
      "layer 8 sparsity 0.500000\n",
      "layer 9 sparsity 0.500000\n",
      "layer 10 sparsity 0.500000\n",
      "layer 11 sparsity 0.500000\n",
      "layer 12 sparsity 0.500000\n",
      "layer 13 sparsity 0.500000\n",
      "layer 14 sparsity 0.500000\n",
      "layer 15 sparsity 0.500000\n",
      "layer 16 sparsity 0.500000\n",
      "layer 17 sparsity 0.500000\n",
      "layer 18 sparsity 0.500000\n",
      "layer 19 sparsity 0.500000\n",
      "layer 20 sparsity 0.500000\n",
      "layer 21 sparsity 0.500000\n",
      "layer 22 sparsity 0.500000\n",
      "layer 23 sparsity 0.500000\n",
      "layer 24 sparsity 0.500000\n",
      "layer 25 sparsity 0.500000\n",
      "layer 26 sparsity 0.500000\n",
      "layer 27 sparsity 0.500000\n",
      "layer 28 sparsity 0.500000\n",
      "layer 29 sparsity 0.500000\n",
      "layer 30 sparsity 0.500000\n",
      "layer 31 sparsity 0.500000\n",
      "sparsity sanity check 0.5000\n",
      "******************************\n",
      "evaluating on wikitext2\n",
      "nsamples 166\n",
      "sample 0\n",
      "sample 50\n",
      "sample 100\n",
      "sample 150\n",
      "wikitext perplexity 8.782569885253906\n",
      "(/home/thsch026/my-envs/prune) \n"
     ]
    }
   ],
   "source": [
    "python main.py \\\n",
    "    --model $MODEL \\\n",
    "    --prune_method wanda \\\n",
    "    --sparsity_ratio 0.5 \\\n",
    "    --save_model /home/thsch026/masterarbeit/models/generated/prune/wanda/llama2 \\\n",
    "    --sparsity_type unstructured \\\n",
    "    --save /home/thsch026/masterarbeit/models/generated/prune/wanda/llama2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5916f36-9719-4ee8-9b79-764f73a7efd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
