{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58a16ba8-ce96-479f-8e25-eb48bd7affc4",
   "metadata": {},
   "source": [
    "# LM evaluation harness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9fce36-6947-49a3-b4c8-c943353185d4",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Das Evaluation framework scheint gut geeignet für Tests mit verschiedenen Benchmarks. Das Repository hierzu ist https://github.com/EleutherAI/lm-evaluation-harness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e13934-9c8a-4aac-a3cd-3274b20c17a6",
   "metadata": {},
   "source": [
    "Cite\n",
    "@misc{eval-harness,\n",
    "  author       = {Gao, Leo and Tow, Jonathan and Abbasi, Baber and Biderman, Stella and Black, Sid and DiPofi, Anthony and Foster, Charles and Golding, Laurence and Hsu, Jeffrey and Le Noac'h, Alain and Li, Haonan and McDonell, Kyle and Muennighoff, Niklas and Ociepa, Chris and Phang, Jason and Reynolds, Laria and Schoelkopf, Hailey and Skowron, Aviya and Sutawika, Lintang and Tang, Eric and Thite, Anish and Wang, Ben and Wang, Kevin and Zou, Andy},\n",
    "  title        = {A framework for few-shot language model evaluation},\n",
    "  month        = 12,\n",
    "  year         = 2023,\n",
    "  publisher    = {Zenodo},\n",
    "  version      = {v0.4.0},\n",
    "  doi          = {10.5281/zenodo.10256836},\n",
    "  url          = {https://zenodo.org/records/10256836}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f83c29b-0866-486e-b14f-9b100ac98b5f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setup Environment (Muss nach jedem Neustart des Servers erfolgen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c128c22-ea8c-46f0-8afa-2117627be1ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(/home/thsch026/my-envs/eval) \n",
      "(/home/thsch026/my-envs/eval) \n",
      "(/home/thsch026/my-envs/eval) \n"
     ]
    }
   ],
   "source": [
    "conda activate eval\n",
    "#pip install torch==2.0.1 packaging\n",
    "cd \"/home/thsch026/masterarbeit/work/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8b202c-cffb-4de1-8dac-2b1c3e8ccf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone Git Repo and install dependencies\n",
    "git clone https://github.com/EleutherAI/lm-evaluation-harness\n",
    "cd lm-evaluation-harness\n",
    "pip install -e .\n",
    "pip install torch==2.0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d20d83c-1d06-43ad-a28c-cf0cbd1dfed1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### In einigen Fällen wird 'sentencepiece' verwendet und muss installiert werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1c5296-97b5-4aa2-9dc1-3a75ba877a77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1ad3c4-8524-4d78-b473-f4d74af10b92",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Installliert wandb falls nicht vorhanden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c11bac-3e86-4d06-9ffc-d95ffdafdaf9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install lm_eval[wandb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43095741-716f-48f1-b861-b6be04e8aead",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wandb login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072a96de-dfc2-4a20-bc46-a0beae36e57b",
   "metadata": {},
   "outputs": [],
   "source": [
    " #export MODEL=\"/home/thsch026/masterarbeit/models/llama3/Meta-Llama-3-8B-Instruct-HF\"\n",
    "export MODEL=\"/home/thsch026/masterarbeit/models/generated/llama3-8B-lora-instruct-toxic\"\n",
    "lm_eval \\\n",
    "    --model hf \\\n",
    "    --model_args pretrained=$MODEL \\\n",
    "    --tasks hellaswag \\\n",
    "    --device cuda:0 \\\n",
    "    --batch_size 8 \\\n",
    "    --log_samples \\\n",
    "    --output_path output/llama2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884f0180-2e33-4808-84bf-a33d7a8f5e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = \"EleutherAI/gpt-j-6B\"\n",
    "#!export MODEL=\"/home/thsch026/masterarbeit/models/llama3/Meta-Llama-3-8B-Instruct-HF\"\n",
    "export MODEL=\"Weyaxi/Einstein-v6.1-Llama3-8B\"\n",
    "\n",
    "lm_eval --model hf  --model_args pretrained=$MODEL  --tasks hellaswag --device cuda:0 --batch_size 8 --output_path \"../../out_eval\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eedb6b6-73ec-4c84-af47-ecee705eaf57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#### Beispiel für eine Visualisierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddc4dd9-ae78-4ae3-84be-36eb4f37a671",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualize\n",
    "# Zeno APi Key = zen_JmwQ0TQiAxDxsbV34hDZSBA6XcI4GFR88cNBTLSCruY\n",
    "\n",
    "export ZENO_API_KEY=zen_JmwQ0TQiAxDxsbV34hDZSBA6XcI4GFR88cNBTLSCruY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a9abfd-e1fc-4f5e-8f68-b21219164d65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "conda activate eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ce1e050-5b78-4a88-9e82-1b7761de00d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(/home/thsch026/my-envs/eval) \n",
      "(/home/thsch026/my-envs/eval) \n",
      "2024-05-14:10:18:33,918 INFO     [__main__.py:254] Verbosity set to INFO\n",
      "2024-05-14:10:18:48,105 INFO     [__main__.py:341] Selected Tasks: ['hellaswag']\n",
      "2024-05-14:10:18:48,148 INFO     [evaluator.py:141] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
      "2024-05-14:10:18:48,148 INFO     [evaluator.py:187] Initializing hf model, with arguments: {'pretrained': 'Weyaxi/Einstein-v6.1-Llama3-8B', 'dtype': 'float16'}\n",
      "2024-05-14:10:18:48,363 WARNING  [logging.py:61] Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "2024-05-14:10:18:48,363 INFO     [huggingface.py:165] Using device 'cuda:0'\n",
      "/home/thsch026/my-envs/eval/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [02:44<00:00, 41.09s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/home/thsch026/my-envs/eval/lib/python3.10/site-packages/datasets/load.py:1486: FutureWarning: The repository for hellaswag contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/hellaswag\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "2024-05-14:10:21:48,663 INFO     [task.py:398] Building contexts for hellaswag on rank 0...\n",
      "100%|███████████████████████████████████| 10042/10042 [00:02<00:00, 3447.72it/s]\n",
      "2024-05-14:10:21:53,816 INFO     [evaluator.py:404] Running loglikelihood requests\n",
      "Running loglikelihood requests: 100%|█████| 40168/40168 [09:13<00:00, 72.62it/s]\n",
      "fatal: not a git repository (or any parent up to mount point /home)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "2024-05-14:10:31:40,247 INFO     [evaluation_tracker.py:132] Saving results aggregated\n",
      "2024-05-14:10:31:40,260 INFO     [evaluation_tracker.py:203] Saving samples results\n",
      "hf (pretrained=Weyaxi/Einstein-v6.1-Llama3-8B,dtype=float16), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 4\n",
      "|  Tasks  |Version|Filter|n-shot| Metric |Value |   |Stderr|\n",
      "|---------|------:|------|-----:|--------|-----:|---|-----:|\n",
      "|hellaswag|      1|none  |     0|acc     |0.6179|±  |0.0048|\n",
      "|         |       |none  |     0|acc_norm|0.8055|±  |0.0039|\n",
      "\n",
      "(/home/thsch026/my-envs/eval) \n"
     ]
    }
   ],
   "source": [
    "export MODEL=\"Weyaxi/Einstein-v6.1-Llama3-8B\" # Precached LlamaModel from HF\n",
    "\n",
    "lm_eval --model hf  --model_args pretrained=$MODEL,dtype=float16  --tasks hellaswag --device cuda:0 --batch_size 4 --log_samples --output_path \"/home/thsch026/masterarbeit/experiment/out_eval\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b51f0b-97c6-4ce4-984c-405a36811fa0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cd /home/thsch026/masterarbeit/experiment/lm-evaluation-harness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8674163-1bec-4584-94a9-82c6a43ff2c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "python scripts/zeno_visualize.py --data_path ../out_eval --project_name \"Initial Tests\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94548b4-be57-49c2-aa62-4623309df86a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export MODEL=\"Weyaxi/Einstein-v6.1-Llama3-8B\" # Precached LlamaModel from HF\n",
    "#export MODEL=\"/home/thsch026/masterarbeit/models/generated/llama3-8B-lora-instruct-toxic/checkpoint-3810\"\n",
    "export MODEL=\"/home/thsch026/masterarbeit/models/llama3/Meta-Llama-3-8B-Instruct-HF\"\n",
    "export MODEL=\"/home/thsch026/masterarbeit/models/llama3/Meta-Llama-3-8B-Instruct-HF\"\n",
    "\n",
    "lm_eval --model hf  --model_args pretrained=$MODEL,dtype=float16,tokenizer=\"/home/thsch026/masterarbeit/models/llama3/Meta-Llama-3-8B-Instruct-HF\"  --tasks hellaswag --device cuda:0 --batch_size 4 --wandb_args project=lm-eval-harness-integration --log_samples --output_path \"/home/thsch026/masterarbeit/experiment/out_wan\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b52f75-aa1b-48e9-969f-9fe05b8267c8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Produktive Evaluierungen mit Visualisierung (Mit Weights and Biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7709b69e-edd6-424e-9ac4-554b888f3463",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Evaluation run HINWEISE\n",
    "-> Das JSON config file des Checkpoints muss nach config.json kopiert werden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1db4e05c-5bc5-4bf9-9d3b-4d77dd913e75",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(/home/thsch026/my-envs/eval) \n"
     ]
    }
   ],
   "source": [
    "conda activate eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1578520a-dd7a-477b-b24a-99cf3e53a0d2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Benchmarks für die Referenzmodelle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0f6c62-53e9-4144-b820-f7305f55cd83",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Reference Model Llama-3-8B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5e75a3e-c048-4d1e-b6ac-ee1ba68f2a29",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(/home/thsch026/my-envs/eval) \n",
      "(/home/thsch026/my-envs/eval) \n",
      "(/home/thsch026/my-envs/eval) \n",
      "(/home/thsch026/my-envs/eval) \n"
     ]
    }
   ],
   "source": [
    "export MODEL=\"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "export WANDB_NAME=\"Llama 3 8B instruct Referenz\"\n",
    "export WANDB_NOTES=\"Llama 3 8B instruct Referenz Messung\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5cbf69-9fcf-4395-a2c6-57dca59dfb61",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Reference Model LLama-3-8B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "046b1906-7068-490a-9f80-858303e06664",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(/home/thsch026/my-envs/eval) \n",
      "(/home/thsch026/my-envs/eval) \n",
      "(/home/thsch026/my-envs/eval) \n",
      "(/home/thsch026/my-envs/eval) \n"
     ]
    }
   ],
   "source": [
    "export MODEL=\"meta-llama/Meta-Llama-3-8B\"\n",
    "export WANDB_NAME=\"meta-llama/Meta-Llama-3-8B\"\n",
    "export WANDB_NOTES=\"Meta-Llama-3-8B Referenz Messung\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9178a104-eacb-4216-a2d8-8f9673643a79",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Reference Model Mistral-7B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "47414e4f-d417-43a5-b26c-786586d19fff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(/home/thsch026/my-envs/eval) \n",
      "(/home/thsch026/my-envs/eval) \n",
      "(/home/thsch026/my-envs/eval) \n",
      "(/home/thsch026/my-envs/eval) \n"
     ]
    }
   ],
   "source": [
    "# Reference Model Mistral-7B-Instruct\n",
    "export MODEL=\"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "export WANDB_NAME=\"Mistral-7B-Instruct Referenz \"\n",
    "export WANDB_NOTES=\"Mistral-7B-Instruct Referenz Messung\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d934aab3-0694-4501-b0d2-710d83b085f6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Reference Model Llama-2-7B chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "129cb4a1-16b8-43ac-95fb-af12e689a3c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(/home/thsch026/my-envs/eval) \n",
      "(/home/thsch026/my-envs/eval) \n",
      "(/home/thsch026/my-envs/eval) \n",
      "(/home/thsch026/my-envs/eval) \n"
     ]
    }
   ],
   "source": [
    "# Reference Model Llama-2-7B chat\n",
    "export MODEL=\"meta-llama/Llama-2-7b-chat-hf\"\n",
    "export WANDB_NAME=\"Llama-2-7B-chat-hf\"\n",
    "export WANDB_NOTES=\"Llama2-7B-chat Referenz Messung\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d641358a-0888-4efc-9084-2432b269a511",
   "metadata": {},
   "source": [
    "#### Pruned Model Mistral 7B ohne die Layer 22-29"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b4d10d-4123-4cf3-962e-7e205b394e61",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Benchmarks AutoAWQ Kompression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "447c11fa-612b-49df-ae39-f4ac9374f627",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pip install torch==2.0.1 install autoawq\n",
      "Requirement already satisfied: torch==2.0.1 in /home/thsch026/my-envs/eval/lib/python3.10/site-packages (2.0.1)\n",
      "Requirement already satisfied: install in /home/thsch026/my-envs/eval/lib/python3.10/site-packages (1.3.5)\n",
      "Requirement already satisfied: autoawq in /home/thsch026/my-envs/eval/lib/python3.10/site-packages (0.2.5)\n",
      "Requirement already satisfied: filelock in /home/thsch026/my-envs/eval/lib/python3.10/site-packages (from torch==2.0.1) (3.14.0)\n",
      "Requirement already satisfied: typing-extensions in /home/thsch026/.local/lib/python3.10/site-packages (from torch==2.0.1) (4.11.0)\n",
      "Requirement already satisfied: sympy in /home/thsch026/my-envs/eval/lib/python3.10/site-packages (from torch==2.0.1) (1.12)\n",
      "Requirement already satisfied: networkx in /home/thsch026/my-envs/eval/lib/python3.10/site-packages (from torch==2.0.1) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/thsch026/my-envs/eval/lib/python3.10/site-packages (from torch==2.0.1) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/thsch026/my-envs/eval/lib/python3.10/site-packages (from torch==2.0.1) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/thsch026/my-envs/eval/lib/python3.10/site-packages (from torch==2.0.1) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /home/thsch026/my-envs/eval/lib/python3.10/site-packages (from torch==2.0.1) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/thsch026/my-envs/eval/lib/python3.10/site-packages (from torch==2.0.1) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/thsch026/my-envs/eval/lib/python3.10/site-packages (from torch==2.0.1) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/thsch026/my-envs/eval/lib/python3.10/site-packages (from torch==2.0.1) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /home/thsch026/my-envs/eval/lib/python3.10/site-packages (from torch==2.0.1) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /home/thsch026/my-envs/eval/lib/python3.10/site-packages (from torch==2.0.1) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /home/thsch026/my-envs/eval/lib/python3.10/site-packages (from torch==2.0.1) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /home/thsch026/my-envs/eval/lib/python3.10/site-packages (from torch==2.0.1) (2.14.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /home/thsch026/my-envs/eval/lib/python3.10/site-packages (from torch==2.0.1) (11.7.91)\n",
      "Requirement already satisfied: triton==2.0.0 in /home/thsch026/my-envs/eval/lib/python3.10/site-packages (from torch==2.0.1) (2.0.0)\n",
      "Requirement already satisfied: setuptools in /home/thsch026/my-envs/eval/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (69.5.1)\n",
      "Requirement already satisfied: wheel in /home/thsch026/my-envs/eval/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (0.43.0)\n",
      "Requirement already satisfied: cmake in /home/thsch026/my-envs/eval/lib/python3.10/site-packages (from triton==2.0.0->torch==2.0.1) (3.29.2)\n",
      "Requirement already satisfied: lit in /home/thsch026/my-envs/eval/lib/python3.10/site-packages (from triton==2.0.0->torch==2.0.1) (18.1.4)\n",
      "Requirement already satisfied: transformers>=4.35.0 in /home/thsch026/my-envs/eval/lib/python3.10/site-packages (from autoawq) (4.40.1)\n",
      "Requirement already satisfied: tokenizers>=0.12.1 in /home/thsch026/my-envs/eval/lib/python3.10/site-packages (from autoawq) (0.19.1)\n",
      "Requirement already satisfied: accelerate in /home/thsch026/my-envs/eval/lib/python3.10/site-packages (from autoawq) (0.30.0)\n",
      "Requirement already satisfied: datasets in /home/thsch026/my-envs/eval/lib/python3.10/site-packages (from autoawq) (2.19.1)\n",
      "Requirement already satisfied: zstandard in /home/thsch026/my-envs/eval/lib/python3.10/site-packages (from autoawq) (0.22.0)\n",
      "Requirement already satisfied: autoawq-kernels in /home/thsch026/my-envs/eval/lib/python3.10/site-packages (from autoawq) (0.0.6)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /home/thsch026/my-envs/eval/lib/python3.10/site-packages (from tokenizers>=0.12.1->autoawq) (0.23.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/thsch026/my-envs/eval/lib/python3.10/site-packages (from transformers>=4.35.0->autoawq) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/thsch026/my-envs/eval/lib/python3.10/site-packages (from transformers>=4.35.0->autoawq) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/thsch026/my-envs/eval/lib/python3.10/site-packages (from transformers>=4.35.0->autoawq) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/thsch026/my-envs/eval/lib/python3.10/site-packages (from transformers>=4.35.0->autoawq) (2024.4.28)\n",
      "Requirement already satisfied: requests in /home/thsch026/my-envs/eval/lib/python3.10/site-packages (from transformers>=4.35.0->autoawq) (2.31.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/thsch026/my-envs/eval/lib/python3.10/site-packages (from transformers>=4.35.0->autoawq) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/thsch026/my-envs/eval/lib/python3.10/site-packages (from transformers>=4.35.0->autoawq) (4.66.4)\n",
      "Requirement already satisfied: psutil in /home/thsch026/.local/lib/python3.10/site-packages (from accelerate->autoawq) (5.9.8)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /home/thsch026/my-envs/eval/lib/python3.10/site-packages (from datasets->autoawq) (16.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/thsch026/my-envs/eval/lib/python3.10/site-packages (from datasets->autoawq) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/thsch026/my-envs/eval/lib/python3.10/site-packages (from datasets->autoawq) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/thsch026/my-envs/eval/lib/python3.10/site-packages (from datasets->autoawq) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /home/thsch026/my-envs/eval/lib/python3.10/site-packages (from datasets->autoawq) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /home/thsch026/my-envs/eval/lib/python3.10/site-packages (from datasets->autoawq) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /home/thsch026/my-envs/eval/lib/python3.10/site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets->autoawq) (2024.3.1)\n",
      "Requirement already satisfied: aiohttp in /home/thsch026/my-envs/eval/lib/python3.10/site-packages (from datasets->autoawq) (3.9.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/thsch026/my-envs/eval/lib/python3.10/site-packages (from jinja2->torch==2.0.1) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/thsch026/my-envs/eval/lib/python3.10/site-packages (from sympy->torch==2.0.1) (1.3.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/thsch026/my-envs/eval/lib/python3.10/site-packages (from aiohttp->datasets->autoawq) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/thsch026/my-envs/eval/lib/python3.10/site-packages (from aiohttp->datasets->autoawq) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/thsch026/my-envs/eval/lib/python3.10/site-packages (from aiohttp->datasets->autoawq) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/thsch026/my-envs/eval/lib/python3.10/site-packages (from aiohttp->datasets->autoawq) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/thsch026/my-envs/eval/lib/python3.10/site-packages (from aiohttp->datasets->autoawq) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/thsch026/my-envs/eval/lib/python3.10/site-packages (from aiohttp->datasets->autoawq) (4.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/thsch026/my-envs/eval/lib/python3.10/site-packages (from requests->transformers>=4.35.0->autoawq) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/thsch026/my-envs/eval/lib/python3.10/site-packages (from requests->transformers>=4.35.0->autoawq) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/thsch026/my-envs/eval/lib/python3.10/site-packages (from requests->transformers>=4.35.0->autoawq) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/thsch026/my-envs/eval/lib/python3.10/site-packages (from requests->transformers>=4.35.0->autoawq) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/thsch026/.local/lib/python3.10/site-packages (from pandas->datasets->autoawq) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/thsch026/my-envs/eval/lib/python3.10/site-packages (from pandas->datasets->autoawq) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/thsch026/my-envs/eval/lib/python3.10/site-packages (from pandas->datasets->autoawq) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/thsch026/.local/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->autoawq) (1.16.0)\n",
      "(/home/thsch026/my-envs/eval) \n"
     ]
    }
   ],
   "source": [
    "!pip install autoawq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2154ba4f-fe53-4eaa-b92a-50a9d07e9d5b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Llama-3-8B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdcd88b-c3cd-490f-ab5d-f826b6e7a6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Llama-3-8B AutoAWQ\n",
    "export MODEL=\"/home/thsch026/masterarbeit/models/generated/llm-awq/Llama-3-8B-AWQ-4bit\"\n",
    "\n",
    "# Settings für die WANDB Darstellung\n",
    "export WANDB_NAME=\"Llama-3-8B AutoAWQ \"\n",
    "export WANDB_NOTES=\"Llama3 quantized in 4 Bit with Autoawq\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a155685-7a52-49e8-8e58-cf34c49c2ed9",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### LLama-3-8B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6191bda9-4d41-4dc3-be2c-d9140a842ecf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(/home/thsch026/my-envs/eval) \n",
      "(/home/thsch026/my-envs/eval) \n",
      "(/home/thsch026/my-envs/eval) \n",
      "(/home/thsch026/my-envs/eval) \n",
      "(/home/thsch026/my-envs/eval) \n",
      "(/home/thsch026/my-envs/eval) \n"
     ]
    }
   ],
   "source": [
    "# Llama-3-8B AutoAWQ\n",
    "export MODEL=\"/home/thsch026/masterarbeit/models/generated/llm-awq/Meta-Llama-3-8B-Instruct-AWQ\"\n",
    "\n",
    "# Settings für die WANDB Darstellung\n",
    "export WANDB_NAME=\"Llama 3 8B Instruct AWQ \"\n",
    "export WANDB_NOTES=\"Llama3 8B Instruct quantized in 4 Bit with Autoawq\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69435d96-0ae2-4c55-a337-8f1c757ea2d2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Mistral 7B instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1d544d80-5989-4dab-9d85-0ccf8e12525b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(/home/thsch026/my-envs/eval) \n",
      "(/home/thsch026/my-envs/eval) \n",
      "(/home/thsch026/my-envs/eval) \n",
      "(/home/thsch026/my-envs/eval) \n",
      "(/home/thsch026/my-envs/eval) \n",
      "(/home/thsch026/my-envs/eval) \n"
     ]
    }
   ],
   "source": [
    "\n",
    "export MODEL=\"/home/thsch026/masterarbeit/models/generated/llm-awq/Mistral-7B-Instruct-v0.2-AWQ-4bit\"\n",
    "\n",
    "# Settings für die WANDB Darstellung\n",
    "export WANDB_NAME=\"Mistral 7B Instruct v0.2 AWQ-4bit\"\n",
    "export WANDB_NOTES=\"Mistral 7B quantized with AWQ to 4 Bit\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6b6486-ec10-4ca0-ae1e-9d8cd67e1c41",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Llama2-7B-chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "013b1be7-5f91-4215-aac8-db7528a19346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(/home/thsch026/my-envs/eval) \n",
      "(/home/thsch026/my-envs/eval) \n",
      "(/home/thsch026/my-envs/eval) \n",
      "(/home/thsch026/my-envs/eval) \n",
      "(/home/thsch026/my-envs/eval) \n"
     ]
    }
   ],
   "source": [
    "export MODEL=\"/home/thsch026/masterarbeit/models/generated/llm-awq/Llama-2-7b-chat-HF-AWQ-4bit\"\n",
    "\n",
    "# Settings für die WANDB Darstellung\n",
    "export WANDB_NAME=\"Llama-2-7b-chat-HF-AWQ-4bit\"\n",
    "export WANDB_NOTES=\"LLama-2-7B quantized with AWQ to 4 Bit\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4500d1b1-7799-4258-87ef-68322112c204",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Benchmarks Pruning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7f6eeb-56f6-49b3-a078-cce766fdee0f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Benchmarks Pruning mit Wanda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ec971da-6f54-43d9-889d-cfa80036a919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(/home/thsch026/my-envs/eval) \n",
      "(/home/thsch026/my-envs/eval) \n",
      "(/home/thsch026/my-envs/eval) \n",
      "(/home/thsch026/my-envs/eval) \n",
      "(/home/thsch026/my-envs/eval) \n"
     ]
    }
   ],
   "source": [
    "export MODEL=\"/home/thsch026/masterarbeit/models/generated/prune/wanda/\"\n",
    "\n",
    "# Settings für die WANDB Darstellung\n",
    "export WANDB_NAME=\"Llama-2-7b-chat-HF-Pruned\"\n",
    "export WANDB_NOTES=\"LLama-2-7B pruned with wanda\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d0443d-f03e-491b-817f-58c24d28b385",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Benchmarks Pruning  mit PruneMe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a466fe-a93c-4349-b28b-b3c5bb3e3bfd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Pruned Model Llama3-8B-Instruct-HF 8 Layer reduziert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44c976b2-9dea-4817-9684-f6a7f813da65",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(/home/thsch026/my-envs/eval) \n",
      "(/home/thsch026/my-envs/eval) \n",
      "(/home/thsch026/my-envs/eval) \n"
     ]
    }
   ],
   "source": [
    "export MODEL=\"/home/thsch026/masterarbeit/models/generated/prune/pruneme/Meta-Llama-3-8B-Instruct-8_Ext\"\n",
    "export WANDB_NAME=\"Llama-3-8B-Instruct PruneMe 8_Ext \"\n",
    "export WANDB_NOTES=\"Llama3 8B Instruct Layer 23-29 mit PruneMe extrahiert\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86c016e-42b6-4c86-bb9b-c9f64005a08c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Pruned Model Llama3-8B-Instruct-HF 12 Layer reduziert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37d82056-bc4c-48a7-b44f-ebba1fd282ee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(/home/thsch026/my-envs/eval) \n",
      "(/home/thsch026/my-envs/eval) \n",
      "(/home/thsch026/my-envs/eval) \n"
     ]
    }
   ],
   "source": [
    "export MODEL=\"/home/thsch026/masterarbeit/models/generated/prune/pruneme/Meta-Llama-3-8B-Instruct-12_Ext\"\n",
    "export WANDB_NAME=\"Llama-3-8B-Instruct PruneMe 12_Ext \"\n",
    "export WANDB_NOTES=\"Llama3 8B Instruct 12 Layer mit PruneMe extrahiert\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9279779-6112-48a1-bcc0-6b6f556e66fd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Pruned Model Llama3-8B 8 Layer reduziert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "511b0726-1849-4608-bc50-21806c3d92a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(/home/thsch026/my-envs/eval) \n",
      "(/home/thsch026/my-envs/eval) \n",
      "(/home/thsch026/my-envs/eval) \n"
     ]
    }
   ],
   "source": [
    "export MODEL=\"/home/thsch026/masterarbeit/models/generated/prune/pruneme/Meta-Llama-3-8B-8_Ext\"\n",
    "export WANDB_NAME=\"Llama-3-8B PruneMe 8_Ext \"\n",
    "export WANDB_NOTES=\"Llama3 8B Instruct 8 Layer mit PruneMe extrahiert\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514040fd-07ee-4ed2-8f97-72b6ddd322bc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Pruned Model Llama3-8B 12 Layer reduziert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8361b6dc-22c7-42d7-8b14-b98bc53b587e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(/home/thsch026/my-envs/eval) \n",
      "(/home/thsch026/my-envs/eval) \n",
      "(/home/thsch026/my-envs/eval) \n"
     ]
    }
   ],
   "source": [
    "export MODEL=\"/home/thsch026/masterarbeit/models/generated/prune/pruneme/Meta-Llama-3-8B-12_Ext\"\n",
    "export WANDB_NAME=\"Llama-3-8B PruneMe 12_Ext \"\n",
    "export WANDB_NOTES=\"Llama3 8B Instruct 12 Layer mit PruneMe extrahiert\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93755c80-7e9b-4994-8bfa-ab2933b6c8b8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Pruned Model Mistral 7B 8 Layer reduziert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06f124c3-4e11-4f9d-8302-f183746f9387",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(/home/thsch026/my-envs/eval) \n",
      "(/home/thsch026/my-envs/eval) \n",
      "(/home/thsch026/my-envs/eval) \n"
     ]
    }
   ],
   "source": [
    "export MODEL=\"/home/thsch026/masterarbeit/models/generated/prune/pruneme/Mistral-7B-Instruct-v0.2-8_Ext\"\n",
    "export WANDB_NAME=\"Mistral 7B Instruct PruneMe 8_Ext \"\n",
    "export WANDB_NOTES=\"Mistral 7B Instruct 8 Layer mit PruneMe extrahiert\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9142eed1-bda6-4e48-95e9-b920cbc5a463",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Pruned Model Mistral 7B 12 Layer reduziert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0df0e187-fc1d-490f-8fca-d4fa5763cba6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(/home/thsch026/my-envs/eval) \n",
      "(/home/thsch026/my-envs/eval) \n",
      "(/home/thsch026/my-envs/eval) \n"
     ]
    }
   ],
   "source": [
    "export MODEL=\"/home/thsch026/masterarbeit/models/generated/prune/pruneme/Mistral-7B-Instruct-v0.2-12_Ext\"\n",
    "export WANDB_NAME=\"Mistral 7B Instruct PruneMe 12_Ext \"\n",
    "export WANDB_NOTES=\"Mistral 7B Instruct 12 Layer mit PruneMe extrahiert\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1951c52-fb83-4518-8e88-f90b789864fb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Pruned Model Llama2 7B Chat 8 Layer reduziert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6a596fd-e429-4e38-b326-58f28645476a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(/home/thsch026/my-envs/eval) \n",
      "(/home/thsch026/my-envs/eval) \n",
      "(/home/thsch026/my-envs/eval) \n"
     ]
    }
   ],
   "source": [
    "export MODEL=\"/home/thsch026/masterarbeit/models/generated/prune/pruneme/Llama-2-7b-chat-hf-8_Ext\"\n",
    "export WANDB_NAME=\"Llama2 7B chat PruneMe 8_Ext \"\n",
    "export WANDB_NOTES=\"Llama2 7B chat 8 Layer mit PruneMe extrahiert\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf642c3-0be0-4c1f-af38-939453e868c7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Pruned Model Llama2 7B Chat 12 Layer reduziert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9bfc1f9-a750-40a6-8801-9039b5818077",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(/home/thsch026/my-envs/eval) \n",
      "(/home/thsch026/my-envs/eval) \n",
      "(/home/thsch026/my-envs/eval) \n"
     ]
    }
   ],
   "source": [
    "export MODEL=\"/home/thsch026/masterarbeit/models/generated/prune/pruneme/Llama-2-7b-chat-hf-12_Ext\"\n",
    "export WANDB_NAME=\"Llama2 7B chat PruneMe 12_Ext \"\n",
    "export WANDB_NOTES=\"Llama2 7B chat 12 Layer mit PruneMe extrahiert\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee023a3-4598-41dc-9e62-d399132a6234",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Pruned Model Mistral 7B ohne die Layer 22-29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "66be1d4b-2741-4fbf-9f1c-cf15491dba0b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(/home/thsch026/my-envs/eval) \n",
      "(/home/thsch026/my-envs/eval) \n",
      "(/home/thsch026/my-envs/eval) \n"
     ]
    }
   ],
   "source": [
    "export MODEL=\"/home/thsch026/masterarbeit/models/generated/prune/pruneme/mistral7_pruneme\"\n",
    "export WANDB_NAME=\"Mistral7 B PruneMe \"\n",
    "export WANDB_NOTES=\"Mistral7B Layer 22-29 mit PruneMe extrahiert\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3221c68-2e39-4d4d-9d91-d3f414828334",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Pruned Model Llama2-7B-Chat-HF ohne die Layer 21-29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e97fac01-01ee-4583-9ed2-410fe14c8a6a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(/home/thsch026/my-envs/eval) \n",
      "(/home/thsch026/my-envs/eval) \n",
      "(/home/thsch026/my-envs/eval) \n"
     ]
    }
   ],
   "source": [
    "export MODEL=\"/home/thsch026/masterarbeit/models/generated/prune/pruneme/llama2_pruneme\"\n",
    "export WANDB_NAME=\"Llama2 PruneMe \"\n",
    "export WANDB_NOTES=\"Llama2 Layer 21-29 mit PruneMe extrahiert\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11dfb4c-4e7e-4d80-87e4-625f84f32557",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Pruned Model Llama2-7B-Chat-HF ohne die Layer 18-30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73641b8c-8a2c-47e5-a5ea-9121dfd279ac",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Pruning mit shortened-llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa677da8-fb29-4ab4-bda9-fbbf845a1320",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Meta-LLama-3-8B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e653992-aa8d-44cd-a36e-946c68a3f122",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(/home/thsch026/my-envs/eval) \n",
      "(/home/thsch026/my-envs/eval) \n",
      "(/home/thsch026/my-envs/eval) \n"
     ]
    }
   ],
   "source": [
    "export MODEL=\"/home/thsch026/masterarbeit/models/generated/prune/shortened-llm/Meta-LLama-3-8B\"\n",
    "export WANDB_NAME=\"Meta LLama 3 8B shortened llm\"\n",
    "export WANDB_NOTES=\"Meta LLama 3 8B shortened llm six layers extracted\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6924a6ab-1b12-41b9-a0d6-f2bbcb9937f5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Meta LLama 3 8B instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "698f99fd-e99a-4d57-ad82-262d61be7f59",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(/home/thsch026/my-envs/eval) \n",
      "(/home/thsch026/my-envs/eval) \n",
      "(/home/thsch026/my-envs/eval) \n"
     ]
    }
   ],
   "source": [
    "export MODEL=\"/home/thsch026/masterarbeit/models/generated/prune/shortened-llm/Meta-LLama-3-8B-instruct\"\n",
    "export WANDB_NAME=\"Meta LLama 3 8B instruct shortened llm\"\n",
    "export WANDB_NOTES=\"Meta LLama 3 8B instruct shortened llm six layers extracted\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7265bc70-c699-4f77-8712-c81ad515ba4b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Mistral-7B-Instruct-v02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bedcf0f0-a16b-4dc9-92c5-db53932c147c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(/home/thsch026/my-envs/eval) \n",
      "(/home/thsch026/my-envs/eval) \n",
      "(/home/thsch026/my-envs/eval) \n"
     ]
    }
   ],
   "source": [
    "export MODEL=\"/home/thsch026/masterarbeit/models/generated/prune/shortened-llm/Mistral-7B-Instruct-v02\"\n",
    "export WANDB_NAME=\"Mistral-7B-Instruct-v02 shortened llm\"\n",
    "export WANDB_NOTES=\"Mistral-7B-Instruct-v02 shortened llm six layers extracted\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e60b5c-163c-446e-904d-570f53aa0f09",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Llama-2-7b-hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7178eae-0658-4bfc-b558-c200984b6ac0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(/home/thsch026/my-envs/eval) \n",
      "(/home/thsch026/my-envs/eval) \n",
      "(/home/thsch026/my-envs/eval) \n"
     ]
    }
   ],
   "source": [
    "export MODEL=\"/home/thsch026/masterarbeit/models/generated/prune/shortened-llm/Llama-2-7b-hf\"\n",
    "export WANDB_NAME=\"Llama-2-7b-hf shortened llm\"\n",
    "export WANDB_NOTES=\"Llama-2-7b-hf shortened llm six layers extracted\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec199dc-83cc-4f12-b4c2-bff072ac0694",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Llama 2 7b chat hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5285fc4d-2bb1-40ad-8331-723c8fa5293b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(/home/thsch026/my-envs/eval) \n",
      "(/home/thsch026/my-envs/eval) \n",
      "(/home/thsch026/my-envs/eval) \n"
     ]
    }
   ],
   "source": [
    "export MODEL=\"/home/thsch026/masterarbeit/models/generated/prune/shortened-llm/Llama-2-7b-chat-hf\"\n",
    "export WANDB_NAME=\"Llama 2 7b chat hf shortened llm\"\n",
    "export WANDB_NOTES=\"Llama 2 7b chat hf shortened llm six layers extracted\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fd8740-8f94-46ca-a4ea-620c1c871ea5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Benchmarks LORA Kompression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654828e6-85f7-4fbf-8869-a42a478b97ec",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Llama-3-8B (Daten ms_marco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b02fb41-ec3f-4095-aab3-8b2ad5269a92",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(/home/thsch026/my-envs/eval) \n",
      "(/home/thsch026/my-envs/eval) \n",
      "(/home/thsch026/my-envs/eval) \n"
     ]
    }
   ],
   "source": [
    "export MODEL=\"/home/thsch026/masterarbeit/models/generated/lora/Meta-Llama-3-8B_ms-marco/model\"\n",
    "export WANDB_NAME=\"Llama3 8B Lora\"\n",
    "export WANDB_NOTES=\"Llama3 8B Lora mit Datensatz ms_marco v1.1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c264dea-2d3e-4bba-af29-d60a0363d1a0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Mistral 7B Instruct(Daten ms_marco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afd8b598-a4f7-41b3-99e8-ab55be5f4f0f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(/home/thsch026/my-envs/eval) \n",
      "(/home/thsch026/my-envs/eval) \n",
      "(/home/thsch026/my-envs/eval) \n"
     ]
    }
   ],
   "source": [
    "export MODEL=\"/home/thsch026/masterarbeit/models/generated/lora/mistral_7B-Instruct_ms-marco/model\"\n",
    "export WANDB_NAME=\"Mistral 7B Instruct Lora\"\n",
    "export WANDB_NOTES=\"Mistral 7B Instruct - Lora mit Datensatz ms_marco v1.1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ef0202-a9f8-4dd2-988d-250771f1fe70",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Llama 3 8B instruct (Daten ms_marco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31b5cb76-0883-4553-bfc4-c95754834843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(/home/thsch026/my-envs/eval) \n",
      "(/home/thsch026/my-envs/eval) \n",
      "(/home/thsch026/my-envs/eval) \n"
     ]
    }
   ],
   "source": [
    "export MODEL=\"/home/thsch026/masterarbeit/models/generated/lora/Llama-3-8B-Instruct-HF_ms-marco\"\n",
    "export WANDB_NAME=\"Llama 3 8B instruct Lora ms_marco\"\n",
    "export WANDB_NOTES=\"Llama 3 8B instruct Lora mit Datensatz ms_marco v1.1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfefaf7-d3f4-4594-bb3d-81104ee442b7",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Llama 2 7B chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73ce87e6-de5b-4f8c-9499-cfd2256133ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(/home/thsch026/my-envs/eval) \n",
      "(/home/thsch026/my-envs/eval) \n",
      "(/home/thsch026/my-envs/eval) \n"
     ]
    }
   ],
   "source": [
    "export MODEL=\"/home/thsch026/masterarbeit/models/generated/lora/Llama-2-7b-chat_ms-marco/model\"\n",
    "export WANDB_NAME=\"Llama 2 7B chat Lora ms_marco\"\n",
    "export WANDB_NOTES=\"Llama 2 7B chat - Lora mit Datensatz ms_marco v1.1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1da2250-4694-4435-abb2-942314d59fac",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Benchmarks Qlora Kompression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7faafbe4-5766-4760-adaa-6d20e2dc1513",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Llama2 7B Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "118dafc3-968c-4ad2-96a4-ee000b5568fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(/home/thsch026/my-envs/eval) \n",
      "(/home/thsch026/my-envs/eval) \n",
      "(/home/thsch026/my-envs/eval) \n"
     ]
    }
   ],
   "source": [
    "export MODEL=\"/home/thsch026/masterarbeit/models/generated/qlora/Llama-2-7b-chat-hf_qlora\"\n",
    "export WANDB_NAME=\"Llama2 7B chat QLora\"\n",
    "export WANDB_NOTES=\"Llama2 7B chat QLora mit Datensatz ?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd2496c-1dd8-43ef-8acd-274403dd6d2d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Llama3 8B Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e863eae-3990-4fe4-8846-928f26b4dc81",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(/home/thsch026/my-envs/eval) \n",
      "(/home/thsch026/my-envs/eval) \n",
      "(/home/thsch026/my-envs/eval) \n"
     ]
    }
   ],
   "source": [
    "export MODEL=\"/home/thsch026/masterarbeit/models/generated/qlora/Meta-Llama-3-8B-Instruct_qlora\"\n",
    "export WANDB_NAME=\"Llama 3 8B instruct QLora\"\n",
    "export WANDB_NOTES=\"Llama 3 8B instruct QLora mit Datensatz ?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b50631-521e-403b-bc08-6ab139d32ae6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Mistral 7B instruct v02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c269ed7d-2506-4428-805d-a20cc6bb848b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(/home/thsch026/my-envs/eval) \n",
      "(/home/thsch026/my-envs/eval) \n",
      "(/home/thsch026/my-envs/eval) \n"
     ]
    }
   ],
   "source": [
    "export MODEL=\"/home/thsch026/masterarbeit/models/generated/qlora/Mistral-7B-Instruct-v0.2_qlora\"\n",
    "export WANDB_NAME=\"Mistral 7B instruct v02 QLora\"\n",
    "export WANDB_NOTES=\"Mistral 7B instruct v02 mit Datensatz ?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f2b321-ca8b-4cbe-a861-b2a6daeaee4c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Benchmarks Lora plus Pruning Kompression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5147bb-10f9-42d4-97c5-e4048eaa741d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Meta Llama 3 8B (prune + lora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c99e1e6d-3d17-4fc8-b871-ba95c2b1fb3a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(/home/thsch026/my-envs/eval) \n",
      "(/home/thsch026/my-envs/eval) \n",
      "(/home/thsch026/my-envs/eval) \n"
     ]
    }
   ],
   "source": [
    "export MODEL=\"/home/thsch026/masterarbeit/models/generated/prune_plus_lora/shortened-llm/Meta-Llama-3-8B_prune_lora\"\n",
    "export WANDB_NAME=\"Meta Llama 3 8B prune + lora\"\n",
    "export WANDB_NOTES=\"Meta Llama 3 8B prune + lora\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75df298f-fafa-4a6e-b32b-1e915591a6b4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Meta Llama 3 8B instruct (prune + lora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "808d2f69-defc-445e-a9d0-645a13336b09",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(/home/thsch026/my-envs/eval) \n",
      "(/home/thsch026/my-envs/eval) \n",
      "(/home/thsch026/my-envs/eval) \n"
     ]
    }
   ],
   "source": [
    "export MODEL=\"/home/thsch026/masterarbeit/models/generated/prune_plus_lora/shortened-llm/Meta-Llama-3-8B-instruct_prune_lora\"\n",
    "export WANDB_NAME=\"Meta Llama 3 8B instruct prune + lora\"\n",
    "export WANDB_NOTES=\"Meta Llama 3 8B  instruct prune + lora\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9d1bbc-dc15-43df-b48c-428ba2160fbf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Mistral 7B Instruct v02 (prune + lora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6172be2-8c65-4a95-b4f8-574cacf20586",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(/home/thsch026/my-envs/eval) \n",
      "(/home/thsch026/my-envs/eval) \n",
      "(/home/thsch026/my-envs/eval) \n"
     ]
    }
   ],
   "source": [
    "export MODEL=\"/home/thsch026/masterarbeit/models/generated/prune_plus_lora/shortened-llm/Mistral-7B-Instruct-v02_prune_lora\"\n",
    "export WANDB_NAME=\"Mistral 7B Instruct v02 prune + lora\"\n",
    "export WANDB_NOTES=\"Mistral 7B Instruct v02 prune + lora\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf13f10b-6541-4bb4-8e87-79f32fbaafbb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Llama 2 7b hf (prune + lora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3d612ab1-b9f5-4c90-9ab4-06af73fd289d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(/home/thsch026/my-envs/eval) \n",
      "(/home/thsch026/my-envs/eval) \n",
      "(/home/thsch026/my-envs/eval) \n"
     ]
    }
   ],
   "source": [
    "export MODEL=\"/home/thsch026/masterarbeit/models/generated/prune_plus_lora/shortened-llm/Llama-2-7b-hf_prune_lora\"\n",
    "export WANDB_NAME=\"Llama 2 7b hf prune + lora\"\n",
    "export WANDB_NOTES=\"Llama 2 7b hf prune + lora\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ceedbf3-06e8-4bba-bc24-e4f9f89ec6cc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Llama 2 7b chat hf (prune + lora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2255d9c9-878d-4ae1-bf4a-bfdeed54626d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(/home/thsch026/my-envs/eval) \n",
      "(/home/thsch026/my-envs/eval) \n",
      "(/home/thsch026/my-envs/eval) \n"
     ]
    }
   ],
   "source": [
    "export MODEL=\"/home/thsch026/masterarbeit/models/generated/prune_plus_lora/shortened-llm/Llama-2-7b-chat-hf_prune_lora\"\n",
    "export WANDB_NAME=\"Llama 2 7b chat hf prune + lora\"\n",
    "export WANDB_NOTES=\"Llama 2 7b chat hf prune + lora\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ed3849-d294-4086-904e-5e2c744f3257",
   "metadata": {},
   "source": [
    "# EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36c6b231-89d0-4671-b554-64db09310cf8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(/home/thsch026/my-envs/eval) \n",
      "0,3\n",
      "(/home/thsch026/my-envs/eval) \n"
     ]
    }
   ],
   "source": [
    "export CUDA_VISIBLE_DEVICES=\"0\"\n",
    "echo $CUDA_VISIBLE_DEVICES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008948cd-707c-4a66-bc59-8112140a0622",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Perplexity evaluation  with wikitext\n",
    "- Runs seperate because of high memory consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf346160-5e9f-4f40-a814-2e81bb81534e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lm_eval --tasks list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b17bb1-15fa-466b-89c3-fd6d5f2fe087",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Modify entries for the results\n",
    "export WANDB_NAME=$WANDB_NAME\" WIKI\"\n",
    "export WANDB_NOTES=$WANDB_NOTES\" WIKI\"\n",
    "echo \"Running task for:\"\n",
    "echo $WANDB_NAME\n",
    "echo $WANDB_NOTES\n",
    "# Run Task\n",
    "lm_eval --model hf  --model_args pretrained=$MODEL,tokenizer=$MODEL,dtype=float16,use_safetensors=True,parallelize=True  --tasks wikitext2 \\\n",
    "        --device cuda --batch_size auto --wandb_args project=lm-eval-harness-integration --log_samples \\\n",
    "        --output_path \"/home/thsch026/masterarbeit/Slim/results\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e855ea19-1663-48e9-aaa0-9886a4e3d9f8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Evaluation with Standard Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bc3c0e6-2eb8-462e-b6aa-58b287b17205",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mthomas-t-schmitt\u001b[0m (\u001b[33mpumaai\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.5 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/thsch026/masterarbeit/work/lm-evaluation-harness/lm-evaluation-harness/wandb/run-20240730_154621-iwxdmm37\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mLlama 3 8B Instruct AWQ \u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/pumaai/lm-eval-harness-integration\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/pumaai/lm-eval-harness-integration/runs/iwxdmm37\u001b[0m\n",
      "2024-07-30:15:46:26,066 INFO     [__main__.py:272] Verbosity set to INFO\n",
      "2024-07-30:15:46:26,152 INFO     [__init__.py:491] `group` and `group_alias` keys in tasks' configs will no longer be used in the next release of lm-eval. `tag` will be used to allow to call a collection of tasks just like `group`. `group` will be removed in order to not cause confusion with the new ConfigurableGroup which will be the offical way to create groups with addition of group-wide configuations.\n",
      "2024-07-30:15:46:39,846 INFO     [__init__.py:512] The tag mmlu is already registered as a group, this tag will not be registered. This may affect tasks you want to call.\n",
      "2024-07-30:15:46:39,852 INFO     [__init__.py:512] The tag mmlu is already registered as a group, this tag will not be registered. This may affect tasks you want to call.\n",
      "2024-07-30:15:46:39,858 INFO     [__init__.py:512] The tag mmlu is already registered as a group, this tag will not be registered. This may affect tasks you want to call.\n",
      "2024-07-30:15:46:39,865 INFO     [__init__.py:512] The tag mmlu is already registered as a group, this tag will not be registered. This may affect tasks you want to call.\n",
      "2024-07-30:15:46:39,872 INFO     [__init__.py:512] The tag mmlu is already registered as a group, this tag will not be registered. This may affect tasks you want to call.\n",
      "2024-07-30:15:46:39,879 INFO     [__init__.py:512] The tag mmlu is already registered as a group, this tag will not be registered. This may affect tasks you want to call.\n",
      "2024-07-30:15:46:39,886 INFO     [__init__.py:512] The tag mmlu is already registered as a group, this tag will not be registered. This may affect tasks you want to call.\n",
      "2024-07-30:15:46:39,894 INFO     [__init__.py:512] The tag mmlu is already registered as a group, this tag will not be registered. This may affect tasks you want to call.\n",
      "2024-07-30:15:46:39,901 INFO     [__init__.py:512] The tag mmlu is already registered as a group, this tag will not be registered. This may affect tasks you want to call.\n",
      "2024-07-30:15:46:39,906 INFO     [__init__.py:512] The tag mmlu is already registered as a group, this tag will not be registered. This may affect tasks you want to call.\n",
      "2024-07-30:15:46:39,911 INFO     [__init__.py:512] The tag mmlu is already registered as a group, this tag will not be registered. This may affect tasks you want to call.\n",
      "2024-07-30:15:46:39,919 INFO     [__init__.py:512] The tag mmlu is already registered as a group, this tag will not be registered. This may affect tasks you want to call.\n",
      "2024-07-30:15:46:39,924 INFO     [__init__.py:512] The tag mmlu is already registered as a group, this tag will not be registered. This may affect tasks you want to call.\n",
      "2024-07-30:15:46:39,930 INFO     [__init__.py:512] The tag mmlu is already registered as a group, this tag will not be registered. This may affect tasks you want to call.\n",
      "2024-07-30:15:46:39,935 INFO     [__init__.py:512] The tag mmlu is already registered as a group, this tag will not be registered. This may affect tasks you want to call.\n",
      "2024-07-30:15:46:39,941 INFO     [__init__.py:512] The tag mmlu is already registered as a group, this tag will not be registered. This may affect tasks you want to call.\n",
      "2024-07-30:15:46:39,949 INFO     [__init__.py:512] The tag mmlu is already registered as a group, this tag will not be registered. This may affect tasks you want to call.\n",
      "2024-07-30:15:46:39,956 INFO     [__init__.py:512] The tag mmlu is already registered as a group, this tag will not be registered. This may affect tasks you want to call.\n",
      "2024-07-30:15:46:39,962 INFO     [__init__.py:512] The tag mmlu is already registered as a group, this tag will not be registered. This may affect tasks you want to call.\n",
      "2024-07-30:15:46:39,969 INFO     [__init__.py:512] The tag mmlu is already registered as a group, this tag will not be registered. This may affect tasks you want to call.\n",
      "2024-07-30:15:46:39,977 INFO     [__init__.py:512] The tag mmlu is already registered as a group, this tag will not be registered. This may affect tasks you want to call.\n",
      "2024-07-30:15:46:39,984 INFO     [__init__.py:512] The tag mmlu is already registered as a group, this tag will not be registered. This may affect tasks you want to call.\n",
      "2024-07-30:15:46:39,991 INFO     [__init__.py:512] The tag mmlu is already registered as a group, this tag will not be registered. This may affect tasks you want to call.\n",
      "2024-07-30:15:46:39,997 INFO     [__init__.py:512] The tag mmlu is already registered as a group, this tag will not be registered. This may affect tasks you want to call.\n",
      "2024-07-30:15:46:40,004 INFO     [__init__.py:512] The tag mmlu is already registered as a group, this tag will not be registered. This may affect tasks you want to call.\n",
      "2024-07-30:15:46:40,012 INFO     [__init__.py:512] The tag mmlu is already registered as a group, this tag will not be registered. This may affect tasks you want to call.\n",
      "2024-07-30:15:46:40,017 INFO     [__init__.py:512] The tag mmlu is already registered as a group, this tag will not be registered. This may affect tasks you want to call.\n",
      "2024-07-30:15:46:40,023 INFO     [__init__.py:512] The tag mmlu is already registered as a group, this tag will not be registered. This may affect tasks you want to call.\n",
      "2024-07-30:15:46:40,029 INFO     [__init__.py:512] The tag mmlu is already registered as a group, this tag will not be registered. This may affect tasks you want to call.\n",
      "2024-07-30:15:46:40,034 INFO     [__init__.py:512] The tag mmlu is already registered as a group, this tag will not be registered. This may affect tasks you want to call.\n",
      "2024-07-30:15:46:40,041 INFO     [__init__.py:512] The tag mmlu is already registered as a group, this tag will not be registered. This may affect tasks you want to call.\n",
      "2024-07-30:15:46:40,048 INFO     [__init__.py:512] The tag mmlu is already registered as a group, this tag will not be registered. This may affect tasks you want to call.\n",
      "2024-07-30:15:46:40,058 INFO     [__init__.py:512] The tag mmlu is already registered as a group, this tag will not be registered. This may affect tasks you want to call.\n",
      "2024-07-30:15:46:40,066 INFO     [__init__.py:512] The tag mmlu is already registered as a group, this tag will not be registered. This may affect tasks you want to call.\n",
      "2024-07-30:15:46:40,074 INFO     [__init__.py:512] The tag mmlu is already registered as a group, this tag will not be registered. This may affect tasks you want to call.\n",
      "2024-07-30:15:46:40,082 INFO     [__init__.py:512] The tag mmlu is already registered as a group, this tag will not be registered. This may affect tasks you want to call.\n",
      "2024-07-30:15:46:40,089 INFO     [__main__.py:376] Selected Tasks: ['arc_challenge', 'hellaswag', 'truthfulqa_mc2', 'winogrande']\n",
      "2024-07-30:15:46:40,094 INFO     [evaluator.py:158] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
      "2024-07-30:15:46:40,094 INFO     [evaluator.py:195] Initializing hf model, with arguments: {'pretrained': '/home/thsch026/masterarbeit/models/generated/llm-awq/Meta-Llama-3-8B-Instruct-AWQ', 'dtype': 'float16'}\n",
      "2024-07-30:15:46:40,193 WARNING  [logging.py:61] Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "2024-07-30:15:46:40,193 INFO     [huggingface.py:178] Device not specified\n",
      "2024-07-30:15:46:40,193 INFO     [huggingface.py:179] Cuda Available? True\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [02:01<00:00, 60.66s/it]\n",
      "2024-07-30:15:49:17,124 INFO     [evaluator.py:274] Setting fewshot random generator seed to 1234\n",
      "2024-07-30:15:49:17,125 INFO     [evaluator.py:274] Setting fewshot random generator seed to 1234\n",
      "2024-07-30:15:49:17,125 INFO     [evaluator.py:274] Setting fewshot random generator seed to 1234\n",
      "2024-07-30:15:49:17,125 INFO     [evaluator.py:274] Setting fewshot random generator seed to 1234\n",
      "2024-07-30:15:49:17,136 INFO     [task.py:423] Building contexts for winogrande on rank 0...\n",
      "100%|████████████████████████████████████| 1267/1267 [00:00<00:00, 74382.50it/s]\n",
      "2024-07-30:15:49:17,222 INFO     [task.py:423] Building contexts for truthfulqa_mc2 on rank 0...\n",
      "100%|████████████████████████████████████████| 817/817 [00:00<00:00, 932.27it/s]\n",
      "2024-07-30:15:49:18,150 INFO     [task.py:423] Building contexts for hellaswag on rank 0...\n",
      "100%|███████████████████████████████████| 10042/10042 [00:03<00:00, 3295.60it/s]\n",
      "2024-07-30:15:49:23,855 INFO     [task.py:423] Building contexts for arc_challenge on rank 0...\n",
      "100%|█████████████████████████████████████| 1172/1172 [00:00<00:00, 1535.44it/s]\n",
      "2024-07-30:15:49:24,712 INFO     [evaluator.py:457] Running loglikelihood requests\n",
      "Running loglikelihood requests:   0%|                 | 0/53271 [00:00<?, ?it/s]Passed argument batch_size = auto:1. Detecting largest batch size\n",
      "Determined largest batch size: 64\n",
      "Running loglikelihood requests: 100%|█████| 53271/53271 [32:31<00:00, 27.29it/s]\n",
      "2024-07-30:16:22:23,786 WARNING  [huggingface.py:1314] Failed to get model SHA for /home/thsch026/masterarbeit/models/generated/llm-awq/Meta-Llama-3-8B-Instruct-AWQ at revision main. Error: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/thsch026/masterarbeit/models/generated/llm-awq/Meta-Llama-3-8B-Instruct-AWQ'. Use `repo_type` argument if needed.\n",
      "2024-07-30:16:22:41,237 INFO     [evaluation_tracker.py:206] Saving results aggregated\n",
      "2024-07-30:16:22:41,255 INFO     [evaluation_tracker.py:287] Saving per-sample results for: arc_challenge\n",
      "2024-07-30:16:22:42,413 INFO     [evaluation_tracker.py:287] Saving per-sample results for: hellaswag\n",
      "2024-07-30:16:22:52,677 INFO     [evaluation_tracker.py:287] Saving per-sample results for: truthfulqa_mc2\n",
      "2024-07-30:16:22:53,411 INFO     [evaluation_tracker.py:287] Saving per-sample results for: winogrande\n",
      "hf (pretrained=/home/thsch026/masterarbeit/models/generated/llm-awq/Meta-Llama-3-8B-Instruct-AWQ,dtype=float16), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: auto (64)\n",
      "|    Tasks     |Version|Filter|n-shot| Metric |   |Value |   |Stderr|\n",
      "|--------------|------:|------|-----:|--------|---|-----:|---|-----:|\n",
      "|arc_challenge |      1|none  |     0|acc     |↑  |0.5171|±  |0.0146|\n",
      "|              |       |none  |     0|acc_norm|↑  |0.5580|±  |0.0145|\n",
      "|hellaswag     |      1|none  |     0|acc     |↑  |0.5692|±  |0.0049|\n",
      "|              |       |none  |     0|acc_norm|↑  |0.7529|±  |0.0043|\n",
      "|truthfulqa_mc2|      2|none  |     0|acc     |↑  |0.5100|±  |0.0152|\n",
      "|winogrande    |      1|none  |     0|acc     |↑  |0.7372|±  |0.0124|\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             arc_challenge/acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        arc_challenge/acc_norm ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: arc_challenge/acc_norm_stderr ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      arc_challenge/acc_stderr ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 hellaswag/acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            hellaswag/acc_norm ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     hellaswag/acc_norm_stderr ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          hellaswag/acc_stderr ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            truthfulqa_mc2/acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     truthfulqa_mc2/acc_stderr ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                winogrande/acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         winogrande/acc_stderr ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             arc_challenge/acc 0.51706\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        arc_challenge/acc_norm 0.55802\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: arc_challenge/acc_norm_stderr 0.01451\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      arc_challenge/acc_stderr 0.0146\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           arc_challenge/alias arc_challenge\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 hellaswag/acc 0.56921\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            hellaswag/acc_norm 0.75294\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     hellaswag/acc_norm_stderr 0.0043\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          hellaswag/acc_stderr 0.00494\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               hellaswag/alias hellaswag\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            truthfulqa_mc2/acc 0.50998\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     truthfulqa_mc2/acc_stderr 0.01522\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          truthfulqa_mc2/alias truthfulqa_mc2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                winogrande/acc 0.73717\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         winogrande/acc_stderr 0.01237\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              winogrande/alias winogrande\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mLlama 3 8B Instruct AWQ \u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/pumaai/lm-eval-harness-integration/runs/iwxdmm37\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/pumaai/lm-eval-harness-integration\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 5 media file(s), 6 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240730_154621-iwxdmm37/logs\u001b[0m\n",
      "(/home/thsch026/my-envs/eval) \n"
     ]
    }
   ],
   "source": [
    "lm_eval --model hf  --model_args pretrained=$MODEL,dtype=float16  --tasks arc_challenge,truthfulqa_mc2,winogrande,hellaswag \\\n",
    "        --device cuda:$CUDA_VISIBLE_DEVICES --batch_size auto --wandb_args project=lm-eval-harness-integration --log_samples \\\n",
    "        --output_path \"/home/thsch026/masterarbeit/Slim/results\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62d91dd-f69d-4313-9ee7-d558b722e5b8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## lm_eval HELP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9d98dba-8c95-4ec5-8863-7309a69e12c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: lm_eval [-h] [--model MODEL] [--tasks task1,task2]\n",
      "               [--model_args MODEL_ARGS] [--num_fewshot N]\n",
      "               [--batch_size auto|auto:N|N] [--max_batch_size N]\n",
      "               [--device DEVICE] [--output_path DIR|DIR/file.json]\n",
      "               [--limit N|0<N<1] [--use_cache DIR]\n",
      "               [--cache_requests {true,refresh,delete}] [--check_integrity]\n",
      "               [--write_out] [--log_samples] [--show_config]\n",
      "               [--include_path DIR] [--gen_kwargs GEN_KWARGS]\n",
      "               [--verbosity CRITICAL|ERROR|WARNING|INFO|DEBUG]\n",
      "               [--wandb_args WANDB_ARGS] [--hf_hub_log_args HF_HUB_LOG_ARGS]\n",
      "               [--predict_only] [--seed SEED] [--trust_remote_code]\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "  --model MODEL, -m MODEL\n",
      "                        Name of model e.g. `hf`\n",
      "  --tasks task1,task2, -t task1,task2\n",
      "                        To get full list of tasks, use the command lm-eval --tasks list\n",
      "  --model_args MODEL_ARGS, -a MODEL_ARGS\n",
      "                        Comma separated string arguments for model, e.g. `pretrained=EleutherAI/pythia-160m,dtype=float32`\n",
      "  --num_fewshot N, -f N\n",
      "                        Number of examples in few-shot context\n",
      "  --batch_size auto|auto:N|N, -b auto|auto:N|N\n",
      "                        Acceptable values are 'auto', 'auto:N' or N, where N is an integer. Default 1.\n",
      "  --max_batch_size N    Maximal batch size to try with --batch_size auto.\n",
      "  --device DEVICE       Device to use (e.g. cuda, cuda:0, cpu).\n",
      "  --output_path DIR|DIR/file.json, -o DIR|DIR/file.json\n",
      "                        The path to the output file where the result metrics will be saved. If the path is a directory and log_samples is true, the results will be saved in the directory. Else the parent directory will be used.\n",
      "  --limit N|0<N<1, -L N|0<N<1\n",
      "                        Limit the number of examples per task. If <1, limit is a percentage of the total number of examples.\n",
      "  --use_cache DIR, -c DIR\n",
      "                        A path to a sqlite db file for caching model responses. `None` if not caching.\n",
      "  --cache_requests {true,refresh,delete}\n",
      "                        Speed up evaluation by caching the building of dataset requests. `None` if not caching.\n",
      "  --check_integrity     Whether to run the relevant part of the test suite for the tasks.\n",
      "  --write_out, -w       Prints the prompt for the first few documents.\n",
      "  --log_samples, -s     If True, write out all model outputs and documents for per-sample measurement and post-hoc analysis. Use with --output_path.\n",
      "  --show_config         If True, shows the the full config of all tasks at the end of the evaluation.\n",
      "  --include_path DIR    Additional path to include if there are external tasks to include.\n",
      "  --gen_kwargs GEN_KWARGS\n",
      "                        String arguments for model generation on greedy_until tasks, e.g. `temperature=0,top_k=0,top_p=0`.\n",
      "  --verbosity CRITICAL|ERROR|WARNING|INFO|DEBUG, -v CRITICAL|ERROR|WARNING|INFO|DEBUG\n",
      "                        Controls the reported logging error level. Set to DEBUG when testing + adding new task configurations for comprehensive log output.\n",
      "  --wandb_args WANDB_ARGS\n",
      "                        Comma separated string arguments passed to wandb.init, e.g. `project=lm-eval,job_type=eval\n",
      "  --hf_hub_log_args HF_HUB_LOG_ARGS\n",
      "                        Comma separated string arguments passed to Hugging Face Hub's log function, e.g. `hub_results_org=EleutherAI,hub_repo_name=lm-eval-results`\n",
      "  --predict_only, -x    Use with --log_samples. Only model outputs will be saved and metrics will not be evaluated.\n",
      "  --seed SEED           Set seed for python's random, numpy, torch, and fewshot sampling.\n",
      "                        Accepts a comma-separated list of 4 values for python's random, numpy, torch, and fewshot sampling seeds, respectively, or a single integer to set the same seed for all three.\n",
      "                        The values are either an integer or 'None' to not set the seed. Default is `0,1234,1234,1234` (for backward compatibility).\n",
      "                        E.g. `--seed 0,None,8,52` sets `random.seed(0)`, `torch.manual_seed(8)`, and fewshot sampling seed to 52. Here numpy's seed is not set since the second value is `None`.\n",
      "                        E.g, `--seed 42` sets all four seeds to 42.\n",
      "  --trust_remote_code   Sets trust_remote_code to True to execute code to create HF Datasets from the Hub\n",
      "(/home/thsch026/my-envs/eval) \n"
     ]
    }
   ],
   "source": [
    "lm_eval --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4b74b5-2e8f-4f5a-9ddf-30e35273e619",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
