{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0a4dfaf-34c1-4cb4-9737-3279bbfba9d6",
   "metadata": {},
   "source": [
    "# Notebook für qlora tasks\n",
    "- Quelle: https://github.com/artidoro/qlora\n",
    "- Hier muss man sich noch mal die Beispiele für interference ansehen. Es wird das Originalmodell und der Adapter mittels Peft geladen und dann die Evaluierung durchgeführt\n",
    "- Das Verfahren ist schon realtiv alt, daher funktiniert es ggf. nicht auf neueren Modellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "632a1b9e-2032-413a-b992-bbdf0474774a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(/home/thsch026/my-envs/qlora) \n"
     ]
    }
   ],
   "source": [
    "conda activate qlora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "908f3cf2-04a3-4eba-9551-e6db1e686406",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /home/thsch026/my-envs/qlora/lib/python3.10/site-packages (0.1.99)\n",
      "(/home/thsch026/my-envs/qlora) \n"
     ]
    }
   ],
   "source": [
    "pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6f47883-5c8e-4e43-8b29-bd128333f0b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(/home/thsch026/my-envs/qlora) \n",
      "(/home/thsch026/my-envs/qlora) \n"
     ]
    }
   ],
   "source": [
    "# Pfad zu den sourcen\n",
    "cd /home/thsch026/masterarbeit/experiment/qlora"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb40b01-f053-4830-94aa-655edaed6378",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Huggingface Login\n",
    "- Wird benötigt wenn die Modelle nicht lokal sind "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f04ef455-5b3f-48e2-9d71-e7f1fb634e57",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(/home/thsch026/my-envs/qlora) \n",
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/thsch026/.cache/huggingface/token\n",
      "Login successful\n",
      "(/home/thsch026/my-envs/qlora) \n"
     ]
    }
   ],
   "source": [
    "export HUGGINGFACE_TOKEN=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\n",
    "huggingface-cli login --token $HUGGINGFACE_TOKEN "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4e93cd-bfbe-4968-a766-0afa2092780a",
   "metadata": {},
   "source": [
    "## Modelle für die Experimente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8475e2c5-86a0-439c-bf25-a95873502ed6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Llama-2-7B-chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae9e54ea-ea55-4047-a0c9-9a59e89ac3bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(/home/thsch026/my-envs/qlora) \n",
      "(/home/thsch026/my-envs/qlora) \n",
      "(/home/thsch026/my-envs/qlora) \n"
     ]
    }
   ],
   "source": [
    "export MODEL=\"/home/thsch026/masterarbeit/models/llama2/llama-2-7b-chat-hf\"\n",
    "# Speicherort\n",
    "export SAVEDIR=\"/home/thsch026/masterarbeit/models/generated/qlora/llama-2-7b-chat_qlora\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae9edbf-4005-4cf9-b2c1-0d28c790054f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Llama-3-8B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5132200-3e97-4c5e-8a75-a6e6fbf5fb17",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(/home/thsch026/my-envs/qlora) \n",
      "(/home/thsch026/my-envs/qlora) \n",
      "(/home/thsch026/my-envs/qlora) \n",
      "(/home/thsch026/my-envs/qlora) \n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "export MODEL=\"/home/thsch026/masterarbeit/models/llama3/Meta-Llama-3-8B-Instruct-HF\"\n",
    "# Speicherort\n",
    "export SAVEDIR=\"/home/thsch026/masterarbeit/models/generated/qlora/Meta-Llama-3-8B-Instruct-HF_qlora\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd59be27-4c7c-46f0-8f27-eb2080a0554f",
   "metadata": {},
   "source": [
    "## Task für Llama3 8B 5000 Training Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6df7aac5-11ef-485b-87c6-d81e141fc6ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/thsch026/my-envs/qlora/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so\n",
      "/home/thsch026/my-envs/qlora/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /home/thsch026/my-envs/qlora did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/home/thsch026/my-envs/qlora/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
      "  warn(msg)\n",
      "/home/thsch026/my-envs/qlora/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/home/thsch026/my-envs/qlora/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//0.0.0.0'), PosixPath('8888/jupyterhub/user/thsch026'), PosixPath('http')}\n",
      "  warn(msg)\n",
      "/home/thsch026/my-envs/qlora/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/home/$NB_USER/.local/share/code-server-$BUILD_IMAGE_NAME/extensions')}\n",
      "  warn(msg)\n",
      "/home/thsch026/my-envs/qlora/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/jupyterhub')}\n",
      "  warn(msg)\n",
      "/home/thsch026/my-envs/qlora/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('80'), PosixPath('//10.100.224.200'), PosixPath('tcp')}\n",
      "  warn(msg)\n",
      "/home/thsch026/my-envs/qlora/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('sha-87126fd'), PosixPath('ghcr.io/fhswf/jupyterhub-k8s/deepml')}\n",
      "  warn(msg)\n",
      "/home/thsch026/my-envs/qlora/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//10.100.192.1'), PosixPath('tcp'), PosixPath('443')}\n",
      "  warn(msg)\n",
      "/home/thsch026/my-envs/qlora/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/jupyterhub/user/thsch026')}\n",
      "  warn(msg)\n",
      "/home/thsch026/my-envs/qlora/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/jupyterhub/user/thsch026/oauth_callback')}\n",
      "  warn(msg)\n",
      "/home/thsch026/my-envs/qlora/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('8081'), PosixPath('tcp'), PosixPath('//10.100.211.22')}\n",
      "  warn(msg)\n",
      "/home/thsch026/my-envs/qlora/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('8001'), PosixPath('//10.100.248.225'), PosixPath('tcp')}\n",
      "  warn(msg)\n",
      "/home/thsch026/my-envs/qlora/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('servers!server=thsch026/\", \"access'), PosixPath('[\"access'), PosixPath('servers!user=thsch026\"]')}\n",
      "  warn(msg)\n",
      "/home/thsch026/my-envs/qlora/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//hub'), PosixPath('8081/jupyterhub/hub/api'), PosixPath('http')}\n",
      "  warn(msg)\n",
      "/home/thsch026/my-envs/qlora/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('(/home/thsch026/my-envs/qlora) PROMPT_UPMCESJBVINR\\\\[\\\\]>')}\n",
      "  warn(msg)\n",
      "/home/thsch026/my-envs/qlora/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('tcp'), PosixPath('8888'), PosixPath('//10.100.211.22')}\n",
      "  warn(msg)\n",
      "/home/thsch026/my-envs/qlora/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/home/$NB_USER/.local/share/code-server-$BUILD_IMAGE_NAME')}\n",
      "  warn(msg)\n",
      "/home/thsch026/my-envs/qlora/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//hub'), PosixPath('8081/jupyterhub/hub/api/users/thsch026/activity'), PosixPath('http')}\n",
      "  warn(msg)\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "/home/thsch026/my-envs/qlora/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0'), PosixPath('/usr/local/cuda/lib64/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /home/thsch026/my-envs/qlora/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n",
      "Namespace(model_name_or_path='/home/thsch026/masterarbeit/models/llama2/llama-2-7b-chat-hf', trust_remote_code=False, use_auth_token=False, eval_dataset_size=1024, max_train_samples=None, max_eval_samples=None, source_max_len=1024, target_max_len=256, dataset='alpaca', dataset_format=None, output_dir='/home/thsch026/masterarbeit/models/generated/qlora/llama-2-7b-chat_qlora', overwrite_output_dir=False, do_train=True, do_eval=False, do_predict=False, evaluation_strategy=<IntervalStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=2, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=16, eval_accumulation_steps=None, eval_delay=0, learning_rate=0.0002, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=0.3, num_train_epochs=3.0, max_steps=5000, lr_scheduler_type=<SchedulerType.CONSTANT: 'constant'>, warmup_ratio=0.03, warmup_steps=0, log_level='passive', log_level_replica='warning', log_on_each_node=True, logging_dir='/home/thsch026/masterarbeit/models/generated/qlora/llama-2-7b-chat_qlora/runs/Jul06_13-43-21_jupyter-thsch026', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=10, logging_nan_inf_filter=True, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=250, save_total_limit=40, save_safetensors=False, save_on_each_node=False, no_cuda=False, use_mps_device=False, seed=42, data_seed=None, jit_mode_eval=False, use_ipex=False, bf16=False, fp16=False, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=0, ddp_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=None, dataloader_num_workers=0, past_index=-1, run_name='/home/thsch026/masterarbeit/models/generated/qlora/llama-2-7b-chat_qlora', disable_tqdm=False, remove_unused_columns=False, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], fsdp=[], fsdp_min_num_params=0, fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False}, fsdp_transformer_layer_cls_to_wrap=None, deepspeed=None, label_smoothing_factor=0.0, optim=<OptimizerNames.PAGED_ADAMW: 'paged_adamw_32bit'>, optim_args=None, adafactor=False, group_by_length=True, length_column_name='length', report_to=[], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, ddp_broadcast_buffers=None, dataloader_pin_memory=True, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=None, hub_private_repo=False, gradient_checkpointing=True, include_inputs_for_metrics=False, fp16_backend='auto', push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', auto_find_batch_size=False, full_determinism=False, torchdynamo=None, ray_scope='last', ddp_timeout=1800, torch_compile=False, torch_compile_backend=None, torch_compile_mode=None, xpu_backend=None, sortish_sampler=False, predict_with_generate=False, generation_max_length=None, generation_num_beams=None, generation_config=GenerationConfig {\n",
      "  \"max_new_tokens\": 256,\n",
      "  \"transformers_version\": \"4.31.0\"\n",
      "}\n",
      ", cache_dir=None, train_on_source=False, mmlu_split='eval', mmlu_dataset='mmlu-fs', do_mmlu_eval=False, max_mmlu_samples=None, mmlu_source_max_len=2048, full_finetune=False, adam8bit=False, double_quant=True, quant_type='nf4', bits=4, lora_r=64, lora_alpha=16, lora_dropout=0.0, max_memory_MB=80000, distributed_state=Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      ", _n_gpu=1, __cached__setup_devices=device(type='cuda', index=0), deepspeed_plugin=None)\n",
      "Detected that training was already completed!\n",
      "loading base model /home/thsch026/masterarbeit/models/llama2/llama-2-7b-chat-hf...\n",
      "/home/thsch026/my-envs/qlora/lib/python3.10/site-packages/transformers/modeling_utils.py:2193: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [01:01<00:00, 20.56s/it]\n",
      "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at /home/thsch026/masterarbeit/models/llama2/llama-2-7b-chat-hf and are newly initialized: ['model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.22.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.25.self_attn.rotary_emb.inv_freq', 'model.layers.31.self_attn.rotary_emb.inv_freq', 'model.layers.24.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.27.self_attn.rotary_emb.inv_freq', 'model.layers.23.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.30.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.26.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.28.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.29.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/thsch026/my-envs/qlora/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1714: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n",
      "Adding special tokens.\n",
      "adding LoRA modules...\n",
      "loaded model\n",
      "trainable params: 79953920.0 || all params: 3660328960 || trainable: 2.1843370056007205\n",
      "torch.float32 422326272 0.11537932153507864\n",
      "torch.uint8 3238002688 0.8846206784649213\n",
      "{'loss': 1.3789, 'learning_rate': 0.0002, 'epoch': 0.01}                        \n",
      "{'loss': 1.2108, 'learning_rate': 0.0002, 'epoch': 0.01}                        \n",
      "{'loss': 0.9885, 'learning_rate': 0.0002, 'epoch': 0.02}                        \n",
      "{'loss': 0.8005, 'learning_rate': 0.0002, 'epoch': 0.02}                        \n",
      "{'loss': 0.8468, 'learning_rate': 0.0002, 'epoch': 0.03}                        \n",
      "{'loss': 1.2944, 'learning_rate': 0.0002, 'epoch': 0.04}                        \n",
      "  1%|▌                                     | 69/5000 [18:13<20:47:53, 15.18s/it]Traceback (most recent call last):\n",
      "  File \"/home/thsch026/masterarbeit/experiment/qlora/qlora.py\", line 841, in <module>\n",
      "    train()\n",
      "  File \"/home/thsch026/masterarbeit/experiment/qlora/qlora.py\", line 803, in train\n",
      "    train_result = trainer.train()\n",
      "  File \"/home/thsch026/my-envs/qlora/lib/python3.10/site-packages/transformers/trainer.py\", line 1539, in train\n",
      "    return inner_training_loop(\n",
      "  File \"/home/thsch026/my-envs/qlora/lib/python3.10/site-packages/transformers/trainer.py\", line 1809, in _inner_training_loop\n",
      "    tr_loss_step = self.training_step(model, inputs)\n",
      "  File \"/home/thsch026/my-envs/qlora/lib/python3.10/site-packages/transformers/trainer.py\", line 2665, in training_step\n",
      "    self.accelerator.backward(loss)\n",
      "  File \"/home/thsch026/my-envs/qlora/lib/python3.10/site-packages/accelerate/accelerator.py\", line 1853, in backward\n",
      "    loss.backward(**kwargs)\n",
      "  File \"/home/thsch026/my-envs/qlora/lib/python3.10/site-packages/torch/_tensor.py\", line 487, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/home/thsch026/my-envs/qlora/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 200, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "KeyboardInterrupt\n",
      "  1%|▌                                     | 69/5000 [18:27<21:58:42, 16.05s/it]\n",
      "\n",
      "(/home/thsch026/my-envs/qlora) \n"
     ]
    }
   ],
   "source": [
    "python qlora.py --per_device_train_batch_size 2 --max_steps 5000 --model_name_or_path $MODEL \\\n",
    "            --output_dir $SAVEDIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ec197a-f707-4e8c-a5c5-c440d02a41be",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Run task form checkpoint for 1000 more steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d4cec9-2b0b-492f-84ca-50749f2c3140",
   "metadata": {},
   "outputs": [],
   "source": [
    "python qlora.py --per_device_train_batch_size 4 --max_steps 500 --model_name_or_path meta-llama/Llama-2-7b-hf \\\n",
    "            --resume_from_checkpoint /home/thsch026/masterarbeit/experiment/qlora/output/checkpoint-250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426ab1a6-d3fe-4c4f-9929-7e7b51942514",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
