{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a42c2be8-0908-47d3-91b8-381d8d6e47f0",
   "metadata": {},
   "source": [
    "# Knowledge Distillation - Train a student with a teacher model\n",
    "- Runs in conda environment(kernel): knowdist\n",
    "- There is an issue with the module \"wandb\" which is hopefully solved now. If not reinstall wandb in environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ed9a7aa-977b-4598-8ac2-d875bef93c17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b83600c0-9336-4612-95ce-b1dd39a9e599",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class KnowledgeDistillationTrainingArguments(TrainingArguments):\n",
    "  def __init__(self, *args, alpha=0.5, temperature=2.0, **kwargs):\n",
    "    #*args allows us to pass a variable number of non-keyword arguments to a Python function.\n",
    "    #**kwargs stands for keyword arguments. The only difference from args is that it uses keywords and returns the values in the form of a dictionary.\n",
    "    super().__init__(*args, **kwargs)\n",
    "    #The super() function is often used with the __init__() method to initialize the attributes of the parent class.\n",
    "    self.alpha = alpha\n",
    "    self.temperature = temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "422dfab8-7e1d-4e62-928e-90ad98a30654",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "594ec5ef-7b68-48d4-88f8-2697a5535221",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class KnowledgeDistillationTrainer(Trainer):\n",
    "  def __init__(self, *args, teacher_model=None, **kwargs):\n",
    "    super().__init__(*args, **kwargs)\n",
    "    self.teacher_model = teacher_model\n",
    "\n",
    "  def compute_loss(self, model, inputs, return_outputs=False):\n",
    "    #Extract cross-entropy loss and logits from student\n",
    "    outputs_student = model(**inputs)\n",
    "    loss_ce = outputs_student.loss\n",
    "    logits_student = outputs_student.logits\n",
    "\n",
    "    # Extract logits from teacher\n",
    "    outputs_teacher = self.teacher_model(**inputs)\n",
    "    logits_teacher = outputs_teacher.logits\n",
    "\n",
    "     #Computing distillation loss by Softening probabilities\n",
    "    loss_fct = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "    #The reduction=batchmean argument in nn.KLDivLoss() specifies that we average the losses over the batch dimension.\n",
    "    loss_kd = self.args.temperature ** 2 * loss_fct(\n",
    "                F.log_softmax(logits_student / self.args.temperature, dim=-1),\n",
    "                F.softmax(logits_teacher / self.args.temperature, dim=-1))\n",
    "\n",
    "    # Return weighted student loss\n",
    "    loss = self.args.alpha * loss_ce + (1. - self.args.alpha) * loss_kd\n",
    "    return (loss, outputs_student) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "430c2844-ca50-45f1-937b-2db43dc87846",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fcf1915-d7e2-4424-8d74-6dbe913e0f7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clinc = load_dataset(\"clinc_oos\", \"plus\")\n",
    "#the plus configuration refers to the subset that contains the out-of-scope training examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02952d0c-76d8-4aab-9eb5-9865d7626cc2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'what expression would i use to say i love you if i were an italian', 'intent': 61}\n"
     ]
    }
   ],
   "source": [
    "sample = clinc[\"train\"][0]\n",
    "print(sample)\n",
    "#Each example in the CLINC150 dataset consists of a query in the text column and its corresponding intent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d513d422-3b00-4052-95d0-dfed6587bc03",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "translate\n"
     ]
    }
   ],
   "source": [
    "intents = clinc[\"train\"].features[\"intent\"]\n",
    "intent = intents.int2str(sample[\"intent\"])\n",
    "print(intent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23a909b5-26ef-44f6-b20c-44494b650a37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a12d7b69-cc99-41da-8b7d-950743d4d8a4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thsch026/my-envs/knowdist/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "student_checkpoint = \"distilbert-base-uncased\"\n",
    "student_tokenizer = AutoTokenizer.from_pretrained(student_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b3606f9-09fe-4113-be6c-c87e72367600",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize_text(batch):\n",
    "  return student_tokenizer(batch[\"text\"], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98f284e0-ab45-485f-8713-e8303e9dd91f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "636c35d679184549af68da4b12fe0e88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clinc_tokenized = clinc.map(tokenize_text, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "#We will remove text column as we don't need it\n",
    "#We will also rename the intent column to labels so it can be automatically detected by the trainer.\n",
    "clinc_tokenized = clinc_tokenized.rename_column(\"intent\", \"labels\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d3a1561-056a-41a4-a96a-fd0bbc50cb53",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7210/2285059630.py:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  accuracy_score = load_metric(\"accuracy\")\n",
      "/home/thsch026/my-envs/knowdist/lib/python3.9/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "accuracy_score = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "  predictions, labels = pred\n",
    "  predictions = np.argmax(predictions, axis=1)\n",
    "  return accuracy_score.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85d78274-0b3d-45cd-83eb-c83a38481faa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 48\n",
    "finetuned_student_ckpt = \"distilbert-base-uncased-finetuned-clinc-student\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b9843ae-b67d-46b2-830c-4dbc35822644",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "student_training_args = KnowledgeDistillationTrainingArguments(\n",
    "    output_dir=finetuned_student_ckpt, evaluation_strategy = \"epoch\",\n",
    "    num_train_epochs=1, learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size, alpha=1, weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "17b40a53-b2a6-4be6-a18a-f81a55bbfa0c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thsch026/my-envs/knowdist/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/thsch026/my-envs/knowdist/lib/python3.9/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "bert_ckpt = \"transformersbook/bert-base-uncased-finetuned-clinc\"\n",
    "pipe = pipeline(\"text-classification\", model=bert_ckpt)\n",
    "\n",
    "id2label = pipe.model.config.id2label\n",
    "label2id = pipe.model.config.label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c4d6df1-6d89-47f5-a604-d9ee33d914a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "num_labels = intents.num_classes\n",
    "student_config = (AutoConfig\n",
    "                  .from_pretrained(student_checkpoint, num_labels=num_labels,\n",
    "                                    id2label=id2label, label2id=label2id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bc114b6a-fb42-4e55-9972-2aaa8b9b9057",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(torch.cuda.is_available())\n",
    "def student_init():\n",
    "  return (AutoModelForSequenceClassification.from_pretrained(student_checkpoint, config=student_config).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "385b02c4-be16-4b3b-bf0b-01caa2c92168",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "teacher_checkpoint = \"transformersbook/bert-base-uncased-finetuned-clinc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "051181ed-c7f6-424f-b51a-a66ccf865514",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "teacher_model = (AutoModelForSequenceClassification\n",
    "                     .from_pretrained(teacher_checkpoint, num_labels=num_labels)\n",
    "                     .to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "df625547-fab9-45b2-b665-ca3d74c43f1d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mthomas-t-schmitt\u001b[0m (\u001b[33mpumaai\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/thsch026/masterarbeit/notebooks/wandb/run-20240515_090152-ttmn2hea</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/pumaai/huggingface/runs/ttmn2hea' target=\"_blank\">desert-disco-8</a></strong> to <a href='https://wandb.ai/pumaai/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/pumaai/huggingface' target=\"_blank\">https://wandb.ai/pumaai/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/pumaai/huggingface/runs/ttmn2hea' target=\"_blank\">https://wandb.ai/pumaai/huggingface/runs/ttmn2hea</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='318' max='318' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [318/318 00:24, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.190294</td>\n",
       "      <td>0.541613</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=318, training_loss=4.5764831926837655, metrics={'train_runtime': 43.6649, 'train_samples_per_second': 349.251, 'train_steps_per_second': 7.283, 'total_flos': 83004337293780.0, 'train_loss': 4.5764831926837655, 'epoch': 1.0})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets start the training\n",
    "distilbert_trainer = KnowledgeDistillationTrainer(model_init=student_init,\n",
    "        teacher_model=teacher_model, args=student_training_args,\n",
    "        train_dataset=clinc_tokenized['train'], eval_dataset=clinc_tokenized['validation'],\n",
    "        compute_metrics=compute_metrics, tokenizer=student_tokenizer)\n",
    "distilbert_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c73057c2-015c-4448-82c5-c234734a2b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_teacher_model():\n",
    "  teacher_model.save_pretrained(\"teacher_model\")\n",
    "def save_student_model():\n",
    "  distilbert_trainer.save_model('student_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e97ca6d1-ca6d-43af-944c-4f18a94334ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "save_teacher_model()\n",
    "save_student_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a36e8548-ed37-492d-8190-0e8c13fa893c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoModelForSequenceClassification\n",
    "import os\n",
    "\n",
    "def compute_parameters(model_path):\n",
    "  model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "  parameters = model.num_parameters()\n",
    "  return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "29538bed-17b4-4f86-aa5a-05d4000f3911",
   "metadata": {
    "id": "8-leprTuIE2w"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher Model:  109598359\n"
     ]
    }
   ],
   "source": [
    "teacher_model_parameters = compute_parameters(model_path=\"teacher_model\")\n",
    "print(\"Teacher Model: \", teacher_model_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bc275a5b-9ce3-4924-835c-d61840c9cd73",
   "metadata": {
    "id": "Plfdn76CLKVS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student Model:  67069591\n"
     ]
    }
   ],
   "source": [
    "student_model_parameters = compute_parameters(model_path=\"student_model\")\n",
    "print(\"Student Model: \", student_model_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445e116b-62e2-4e07-b810-c3487119c04c",
   "metadata": {},
   "source": [
    "### Berchechnet die verminderung der Parameteranzahl im Student Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d6301960-5b11-4ecb-bc30-63302d802597",
   "metadata": {
    "id": "EHRy48NBNPV3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-38.804201438818986\n"
     ]
    }
   ],
   "source": [
    "decrease = (student_model_parameters-teacher_model_parameters)/teacher_model_parameters\n",
    "print(decrease*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5c73ead1-0c70-43a0-8495-1786e98ad60d",
   "metadata": {
    "id": "zQsbyCP7Lfgc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 269MB\n",
      "drwxr-xr-x 2 jovyan users   1MB May  2 12:14 .\n",
      "drwxr-xr-x 9 jovyan users   1MB May  2 12:18 ..\n",
      "-rw-r--r-- 1 jovyan users   1MB May  2 12:18 config.json\n",
      "-rw-r--r-- 1 jovyan users 269MB May  2 12:18 model.safetensors\n",
      "-rw-r--r-- 1 jovyan users   1MB May  2 12:18 special_tokens_map.json\n",
      "-rw-r--r-- 1 jovyan users   1MB May  2 12:18 tokenizer_config.json\n",
      "-rw-r--r-- 1 jovyan users   1MB May  2 12:18 tokenizer.json\n",
      "-rw-r--r-- 1 jovyan users   1MB May  2 12:18 training_args.bin\n",
      "-rw-r--r-- 1 jovyan users   1MB May  2 12:18 vocab.txt\n"
     ]
    }
   ],
   "source": [
    "!ls student_model -al --block-size=MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "48dfb044-8062-48db-92fc-69ade128ff6d",
   "metadata": {
    "id": "vPduu_l7MZeg",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 439MB\n",
      "drwxr-xr-x 2 jovyan users   1MB May  2 12:14 .\n",
      "drwxr-xr-x 9 jovyan users   1MB May  2 12:18 ..\n",
      "-rw-r--r-- 1 jovyan users   1MB May  2 12:18 config.json\n",
      "-rw-r--r-- 1 jovyan users 439MB May  2 12:18 model.safetensors\n"
     ]
    }
   ],
   "source": [
    "!ls teacher_model -al --block-size=MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d9a1354d-979f-4f60-b290-cf7a35698319",
   "metadata": {
    "id": "Og3MC2FOMh_S",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete a transaction from savings to checking of $20000\n",
      "133\n"
     ]
    }
   ],
   "source": [
    "print(clinc['train']['text'][101])\n",
    "print(clinc['train']['intent'][101])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476e89c1-3c5e-4249-b335-95988f4cfd55",
   "metadata": {},
   "source": [
    "# Inference Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f2f9484d-c8a9-49f9-8a69-8021d6cdcd4c",
   "metadata": {
    "id": "22AhklIhOEFL",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thsch026/my-envs/knowdist/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time to process 100 requests for Teacher Model:  17.193105459213257\n"
     ]
    }
   ],
   "source": [
    "#Lets warmup first\n",
    "from transformers import pipeline\n",
    "import time\n",
    "\n",
    "pipe = pipeline(\"text-classification\", model=\"teacher_model\", tokenizer='bert-base-uncased')\n",
    "\n",
    "sample_input = clinc['train']['text'][101]\n",
    "\n",
    "#WARMUP\n",
    "for _ in range(10):\n",
    "  _ = pipe(sample_input)\n",
    "\n",
    "start = time.time()\n",
    "for _ in range(100):\n",
    "  _ = pipe(sample_input)\n",
    "total_time_teacher_model = time.time()-start\n",
    "print(\"Total time to process 100 requests for Teacher Model: \",total_time_teacher_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "024fcdb7-2d21-4d0d-8a11-b9b27d83ac8d",
   "metadata": {
    "id": "ND0Rk_c-Od-s"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time to process 100 requests for Student Model:  9.907214403152466\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"text-classification\", model=\"student_model\", tokenizer=\"distilbert-base-uncased\")\n",
    "\n",
    "sample_input = clinc['train']['text'][101]\n",
    "\n",
    "#WARMUP\n",
    "for _ in range(10):\n",
    "  _ = pipe(sample_input)\n",
    "\n",
    "start = time.time()\n",
    "for _ in range(100):\n",
    "  _ = pipe(sample_input)\n",
    "total_time_student_model = time.time()-start\n",
    "\n",
    "print(\"Total time to process 100 requests for Student Model: \",total_time_student_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e991ae25-4425-4b05-9c56-c3242186094c",
   "metadata": {
    "id": "5XkDVWpQnHK-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42.37681827372556\n"
     ]
    }
   ],
   "source": [
    "decrease_in_time = (total_time_teacher_model-total_time_student_model)/total_time_teacher_model\n",
    "print(decrease_in_time*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268152ba-225d-4a7e-9bda-88b9558fa676",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "knowdist",
   "language": "python",
   "name": "knowdist"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
