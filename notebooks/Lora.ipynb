{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fefea5d-dad0-477b-ba20-32c2591b64f2",
   "metadata": {},
   "source": [
    "# Lora finetuning Ansatz fÃ¼r LLMs\n",
    "- Source: https://medium.com/@prakharsaxena11111/a-general-approach-to-fine-tune-any-llm-using-lora-29d24e47a345"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96439398-2466-4b46-8478-03c7b95080d1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Prepare Environment (Should not be neccessary if lora kernel is used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f6a6c3-a099-4a58-a13d-31cc62eff4e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Double check environment\n",
    "# torch must be 2.0.0, transfomers must be 4.40, pydantic must be 2.0.0\n",
    "!conda list | grep torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8734781-8c0a-4c90-99e9-e17b94173e09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install transformers==4.40.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b013a1bf-840d-455d-8143-436ca637ff2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install transformers==4.40.1\n",
    "!pip install torch==2.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f3b0bb-aadc-46d0-9082-5cd8a8509502",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install trl\n",
    "!pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3c52f7-e37b-4c2a-aa33-36b7ed4230fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install Pytorch for FSDP and FA/SDPA\n",
    "!pip install torch==2.0.1 tensorboard\n",
    "#pip install tensorboard datasets\n",
    " \n",
    "# Install Hugging Face libraries\n",
    "!pip install pydantic==2.0.0\n",
    "!pip install  --upgrade \"transformers==4.40.1\" \"datasets==2.18.0\" \"accelerate==0.29.3\" \"evaluate==0.4.1\" \"bitsandbytes==0.43.1\" \"huggingface_hub==0.22.2\" \"trl==0.8.6\" \"peft==0.10.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f07eb34-06ae-4609-807d-907b9ccfc5f0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Login to hugging face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be3c760-fbfb-4db0-b34f-e0dabfd4c5fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!huggingface-cli login --token \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4548a057-94c5-45e6-80ab-134986bcd9b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a5ffaa99-fe34-4eff-a2e2-de298aec8088",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Free GPU Memory\n",
    "- Alternatively > Restart Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726343b5-4fea-490d-a005-b2c5256dfd78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "del model\n",
    "del trainer\n",
    "torch.cuda.empty_cache()\n",
    "print(torch.cuda.memory_reserved(0))\n",
    "print(torch.cuda.memory_allocated(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06adbd9f-cf3b-46fb-912d-a8d4a9602c66",
   "metadata": {
    "tags": []
   },
   "source": [
    "##  Workflow for Lora tuning\n",
    "- Runs also well with \"torchtune\" kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99735949-c7be-437c-8539-216299cf792b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a72d2d46-b709-435a-8440-b8707dda3eda",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-17 12:35:39.192460: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model, PeftModel\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ee84d6-63e2-4a9a-bbc9-88848919d926",
   "metadata": {},
   "source": [
    "### Define Model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "340fd983-2fae-4aa3-a07e-c3f342e90ba4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5e6c98e9af345828eb936d37a7748e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# -> For Mistral 7B\n",
    "model_location = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "# -> For Llama 3 8B HF\n",
    "# model_location = \"/home/thsch026/masterarbeit/models/llama3/Meta-Llama-3-8B-Instruct-HF\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_location)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_location,\n",
    "   # load_in_8bit=True, # was 8bit\n",
    "    device_map=\"cuda\", # was auto\n",
    "    torch_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0519acb-62e4-4464-8e8b-f9590395d48d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "tokenizer.pad_token = \"!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a3b13c-c64b-40e5-9b7b-9e048e9c5456",
   "metadata": {},
   "source": [
    "### For explanation of the values for LORA configuration below see:\n",
    "- https://medium.com/@drishtisharma96505/comparative-analysis-of-lora-parameters-on-llama-2-with-flash-attention-574b913295d4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b20e5a88-4593-4e04-945a-4bf9ef46df10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Lora paramters\n",
    "CUTOFF_LEN = 768\n",
    "LORA_R = 8\n",
    "LORA_ALPHA = 2 * LORA_R\n",
    "LORA_DROPOUT = 0.1\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\"\n",
    "                    , \"down_proj\", \"lm_head\"], #these are the  names for the layers\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6520ce8a-ec0b-4d8c-8cdd-99ea9fe46b31",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['conv_id', 'user_input', 'model_output', 'human_annotation', 'toxicity', 'jailbreaking', 'openai_moderation'],\n",
      "        num_rows: 5082\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['conv_id', 'user_input', 'model_output', 'human_annotation', 'toxicity', 'jailbreaking', 'openai_moderation'],\n",
      "        num_rows: 5083\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('lmsys/toxic-chat', 'toxicchat0124') #Interesting dataset with toxic chat.\n",
    "print(\"dataset\", dataset)\n",
    "train_data = dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439d5c91-d3b3-4975-83d5-6e1cc7d25da4",
   "metadata": {
    "tags": []
   },
   "source": [
    "###  ToDO: anpassen der Prompt Struktur an das genutzte Modell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409c9bdf-9169-440e-b2b4-68153c6f62f5",
   "metadata": {},
   "source": [
    "Prompt Structure for Mistral 7 B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7f036da-bada-4619-b222-f761adb0f384",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_prompt(user_query):  #The prompt format is taken from the official Mistral huggingface page\n",
    "  if user_query[\"model_output\"] is not None and user_query[\"user_input\"] is not None:\n",
    "      p =  \"<s> [INST]\" + user_query[\"user_input\"] + \"[/INST]\" +  user_query[\"model_output\"] + \"</s>\"\n",
    "      return p\n",
    "  else:\n",
    "    p = \"<s> [INST]\" + \"Hello\" + \"[/INST]\" +  \"Hello\" + \"</s>\"\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361d05aa-1e31-4409-84ae-fda25bb8465e",
   "metadata": {},
   "source": [
    "Prompt Structure for llama3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae6919af-5263-4875-88bb-f7e088926d4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize(prompt):\n",
    "    return tokenizer(\n",
    "        prompt + tokenizer.eos_token,\n",
    "        truncation=True,\n",
    "        max_length=CUTOFF_LEN ,\n",
    "        padding=\"max_length\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "68acb15a-5ea5-41ad-b9d5-0a7041db776e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_data = train_data.shuffle().map(lambda x: tokenize(generate_prompt(x)), remove_columns=['conv_id', 'user_input', 'model_output', 'human_annotation', 'toxicity', 'jailbreaking', 'openai_moderation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050cd7a2-3755-4d82-8fa4-e2cd86f5dcdf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model, \n",
    "    train_dataset=train_data,\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=4,\n",
    "        num_train_epochs=3,    # 3 or 6 is good\n",
    "        learning_rate=1e-4,\n",
    "        logging_steps=5,\n",
    "        optim=\"adamw_torch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        output_dir=\"llama3-8B-lora-instruct-toxic\"\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")\n",
    "model.config.use_cache = False\n",
    "\n",
    "\n",
    "#model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976dda06-388a-4341-afe8-0e5cddc2d7d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6538da0f-a3ce-4699-8901-d823135939ec",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Example 2 - Currently not working CUDA iisues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7587dd-58e2-46f1-bbf3-adbcfa420f89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c29b6d5-5d85-4ee4-91a5-d83aa5e520f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "{\"messages\": [{\"role\": \"system\", \"content\": \"You are...\"}, {\"role\": \"user\", \"content\": \"...\"}, {\"role\": \"assistant\", \"content\": \"...\"}]}\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"You are...\"}, {\"role\": \"user\", \"content\": \"...\"}, {\"role\": \"assistant\", \"content\": \"...\"}]}\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"You are...\"}, {\"role\": \"user\", \"content\": \"...\"}, {\"role\": \"assistant\", \"content\": \"...\"}]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec519b3-1292-475a-930a-d19586149bf3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    " \n",
    "# Convert dataset to OAI messages\n",
    "system_message = \"\"\"You are Llama, an AI assistant created by Philipp to be helpful and honest. Your knowledge spans a wide range of topics, allowing you to engage in substantive conversations and provide analysis on complex subjects.\"\"\"\n",
    " \n",
    "def create_conversation(sample):\n",
    "    if sample[\"messages\"][0][\"role\"] == \"system\":\n",
    "        return sample\n",
    "    else:\n",
    "      sample[\"messages\"] = [{\"role\": \"system\", \"content\": system_message}] + sample[\"messages\"]\n",
    "      return sample\n",
    " \n",
    "# Load dataset from the hub\n",
    "dataset = load_dataset(\"HuggingFaceH4/no_robots\")\n",
    " \n",
    "# Add system message to each conversation\n",
    "columns_to_remove = list(dataset[\"train\"].features)\n",
    "columns_to_remove.remove(\"messages\")\n",
    "dataset = dataset.map(create_conversation, remove_columns=columns_to_remove,batched=False)\n",
    " \n",
    "# Filter out conversations which are corrupted with wrong turns, keep which have even number of turns after adding system message\n",
    "dataset[\"train\"] = dataset[\"train\"].filter(lambda x: len(x[\"messages\"][1:]) % 2 == 0)\n",
    "dataset[\"test\"] = dataset[\"test\"].filter(lambda x: len(x[\"messages\"][1:]) % 2 == 0)\n",
    " \n",
    "# save datasets to disk\n",
    "dataset[\"train\"].to_json(\"train_dataset.json\", orient=\"records\", force_ascii=False)\n",
    "dataset[\"test\"].to_json(\"test_dataset.json\", orient=\"records\", force_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feba0e89-5898-4eb6-a834-3866ec751aa2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import trl\n",
    "#import bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61363177-1c5b-4cb6-8e0d-ecc09efed169",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!unset CUDA_VISIBLE_DEVICES\n",
    "CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b4d28e-b28a-4bde-b992-701edcb8a674",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!torchrun --nproc_per_node=4 ./scripts/run_fsdp_qlora.py --config llama_3_70b_fsdp_qlora.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b6c30e-b35a-4ba2-92d9-39a6c4807b40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb47153-1745-48d3-999c-35f97f0be54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "python -m llama_recipes.finetuning --use_peft --peft_method lora --quantization --model_name $MODEL --output_dir ../llama/models_ft/7B-peft --batch_size_training 2 --gradient_accumulation_steps 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016cc912-5372-478c-a4bb-78498e0231e1",
   "metadata": {},
   "source": [
    "### Source https://medium.com/@prakharsaxena11111/a-general-approach-to-fine-tune-any-llm-using-lora-29d24e47a345"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51e3879-d0fd-4df7-a5e0-5ce0612bb284",
   "metadata": {},
   "source": [
    "### GPU Memory issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0135eb51-de6f-4ea3-a8c1-20337a0febfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "i = 1\n",
    "while i == 1:\n",
    "    time.sleep(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562c05bc-eb36-43a3-bf42-a0c1c3461677",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!nvcc -V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd89e8f-6efd-4569-9bfa-d67f79354804",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!conda list |grep transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08642157-1da3-409f-81cd-2480435acc23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchtune",
   "language": "python",
   "name": "torchtune"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
