{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "600c90f8-8c5e-4350-9aed-834ca71845ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(/home/thsch026/my-envs/torchtune) \n"
     ]
    }
   ],
   "source": [
    "conda activate torchtune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b6e9d4f-0bf1-4917-86a7-664fc6c77d03",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(/home/thsch026/my-envs/torchtune) \n",
      "(/home/thsch026/my-envs/torchtune) \n",
      "/home/thsch026/my-envs/torchtune/lib/python3.10/site-packages/torch/distributed/launch.py:181: FutureWarning: The module torch.distributed.launch is deprecated\n",
      "and will be removed in future. Use torchrun.\n",
      "Note that --use-env is set by default in torchrun.\n",
      "If your script expects `--local-rank` argument to be set, please\n",
      "change it to read from `os.environ['LOCAL_RANK']` instead. See \n",
      "https://pytorch.org/docs/stable/distributed.html#launch-utility for \n",
      "further instructions\n",
      "\n",
      "  warnings.warn(\n",
      "Clearing GPU cache for all ranks\n",
      "--> Running with torch dist debug set to detail\n",
      "--> Model EleutherAI/pythia-410m-deduped\n",
      "\n",
      "--> EleutherAI/pythia-410m-deduped has 405.334016 Million params\n",
      "\n",
      "bFloat16 enabled for mixed precision - using bfSixteen policy\n",
      "/home/thsch026/my-envs/torchtune/lib/python3.10/site-packages/torch/distributed/fsdp/_init_utils.py:295: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.FULL_SHARD since the world size is 1.\n",
      "  warnings.warn(\n",
      "--> applying fsdp activation checkpointing...\n",
      "--> Model EleutherAI/pythia-410m-deduped\n",
      "\n",
      "--> EleutherAI/pythia-410m-deduped has 405.334016 Million params\n",
      "\n",
      "--> applying fsdp activation checkpointing...\n",
      "DistillationModel(\n",
      "  (student): FullyShardedDataParallel(\n",
      "    (_fsdp_wrapped_module): GPTNeoXForCausalLM(\n",
      "      (gpt_neox): GPTNeoXModel(\n",
      "        (embed_in): Embedding(50304, 1024)\n",
      "        (emb_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (layers): ModuleList(\n",
      "          (0-23): 24 x FullyShardedDataParallel(\n",
      "            (_fsdp_wrapped_module): CheckpointWrapper(\n",
      "              (_checkpoint_wrapped_module): GPTNeoXLayer(\n",
      "                (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "                (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "                (attention): GPTNeoXAttention(\n",
      "                  (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "                  (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "                  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                  (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (mlp): GPTNeoXMLP(\n",
      "                  (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                  (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                  (act): GELUActivation()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (embed_out): Linear(in_features=1024, out_features=50304, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (teacher): FullyShardedDataParallel(\n",
      "    (_fsdp_wrapped_module): GPTNeoXForCausalLM(\n",
      "      (gpt_neox): GPTNeoXModel(\n",
      "        (embed_in): Embedding(50304, 1024)\n",
      "        (emb_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (layers): ModuleList(\n",
      "          (0-23): 24 x FullyShardedDataParallel(\n",
      "            (_fsdp_wrapped_module): CheckpointWrapper(\n",
      "              (_checkpoint_wrapped_module): GPTNeoXLayer(\n",
      "                (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "                (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "                (attention): GPTNeoXAttention(\n",
      "                  (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "                  (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "                  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                  (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (mlp): GPTNeoXMLP(\n",
      "                  (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                  (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                  (act): GELUActivation()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (embed_out): Linear(in_features=1024, out_features=50304, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "dataset(file='llm-distillation/datasets/loader/fairytaleQA.py', training_size=1, encoder_decoder=False, generated_by='EleutherAI/pythia-410m-deduped') train\n",
      "\"Column train not in the dataset. Current columns in the dataset: ['context', 'question', 'answers', 'answers_generated']\"\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thsch026/llm-recipes/data/data_utils.py\", line 42, in get_dataset\n",
      "    return getattr(module, func_name)(dataset_config, tokenizer, split)\n",
      "  File \"llm-distillation/datasets/loader/fairytaleQA.py\", line 64, in get_split\n",
      "    dataset = dataset[\"train\"]\n",
      "  File \"/home/thsch026/my-envs/torchtune/lib/python3.10/site-packages/datasets/arrow_dataset.py\", line 2803, in __getitem__\n",
      "    return self._getitem(key)\n",
      "  File \"/home/thsch026/my-envs/torchtune/lib/python3.10/site-packages/datasets/arrow_dataset.py\", line 2787, in _getitem\n",
      "    pa_subtable = query_table(self._data, key, indices=self._indices if self._indices is not None else None)\n",
      "  File \"/home/thsch026/my-envs/torchtune/lib/python3.10/site-packages/datasets/formatting/formatting.py\", line 580, in query_table\n",
      "    _check_valid_column_key(key, table.column_names)\n",
      "  File \"/home/thsch026/my-envs/torchtune/lib/python3.10/site-packages/datasets/formatting/formatting.py\", line 520, in _check_valid_column_key\n",
      "    raise KeyError(f\"Column {key} not in the dataset. Current columns in the dataset: {columns}\")\n",
      "KeyError: \"Column train not in the dataset. Current columns in the dataset: ['context', 'question', 'answers', 'answers_generated']\"\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thsch026/llm-recipes/finetuning.py\", line 80, in <module>\n",
      "    fire.Fire(main)\n",
      "  File \"/home/thsch026/my-envs/torchtune/lib/python3.10/site-packages/fire/core.py\", line 143, in Fire\n",
      "    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n",
      "  File \"/home/thsch026/my-envs/torchtune/lib/python3.10/site-packages/fire/core.py\", line 477, in _Fire\n",
      "    component, remaining_args = _CallAndUpdateTrace(\n",
      "  File \"/home/thsch026/my-envs/torchtune/lib/python3.10/site-packages/fire/core.py\", line 693, in _CallAndUpdateTrace\n",
      "    component = fn(*varargs, **kwargs)\n",
      "  File \"/home/thsch026/llm-recipes/finetuning.py\", line 50, in main\n",
      "    train_dataloader, teacher_train_dataloader, eval_dataloader, teacher_eval_dataloader = get_distillation_dataloader(data_config, train_config, distil_config, student_tokenizer, teacher_tokenizer, rank)\n",
      "  File \"/home/thsch026/llm-recipes/data/data_utils.py\", line 111, in get_distillation_dataloader\n",
      "    student_train_dataloader, student_eval_dataloader = get_dataloader(dataset_config, train_config, student_tokenizer, rank, distil_config)\n",
      "  File \"/home/thsch026/llm-recipes/data/data_utils.py\", line 52, in get_dataloader\n",
      "    dataset_train = get_dataset(\n",
      "  File \"/home/thsch026/llm-recipes/data/data_utils.py\", line 45, in get_dataset\n",
      "    raise ValueError(f\"It seems like the given method name ({func_name}) is not present in the load.py file ({module_path.as_posix()}).\")\n",
      "ValueError: It seems like the given method name (get_split) is not present in the load.py file (llm-distillation/datasets/loader/fairytaleQA.py).\n",
      "ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 3751) of binary: /home/thsch026/my-envs/torchtune/bin/python\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thsch026/my-envs/torchtune/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/home/thsch026/my-envs/torchtune/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/thsch026/my-envs/torchtune/lib/python3.10/site-packages/torch/distributed/launch.py\", line 196, in <module>\n",
      "    main()\n",
      "  File \"/home/thsch026/my-envs/torchtune/lib/python3.10/site-packages/torch/distributed/launch.py\", line 192, in main\n",
      "    launch(args)\n",
      "  File \"/home/thsch026/my-envs/torchtune/lib/python3.10/site-packages/torch/distributed/launch.py\", line 177, in launch\n",
      "    run(args)\n",
      "  File \"/home/thsch026/my-envs/torchtune/lib/python3.10/site-packages/torch/distributed/run.py\", line 785, in run\n",
      "    elastic_launch(\n",
      "  File \"/home/thsch026/my-envs/torchtune/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 134, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \"/home/thsch026/my-envs/torchtune/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 250, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "============================================================\n",
      "llm-recipes/finetuning.py FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "  <NO_OTHER_FAILURES>\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2024-07-13_09:33:19\n",
      "  host      : jupyter-thsch026\n",
      "  rank      : 0 (local_rank: 0)\n",
      "  exitcode  : 1 (pid: 3751)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\n",
      "(/home/thsch026/my-envs/torchtune) \n"
     ]
    },
    {
     "ename": "",
     "evalue": "1",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "cd /home/thsch026/\n",
    "\n",
    "python -m torch.distributed.launch llm-recipes/finetuning.py \\\n",
    "--model_name EleutherAI/pythia-410m-deduped \\\n",
    "--enable_fsdp \\\n",
    "--run_validation False \\\n",
    "--dataset.file llm-distillation/datasets/loader/fairytaleQA.py \\\n",
    "--lr 1e-6 \\\n",
    "--num_epochs 5 \\\n",
    "--batch_size_training 4 \\\n",
    "--val_batch_size 4 \\\n",
    "--output_dir train/test \\\n",
    "--save_step 100 \\\n",
    "--distillation \\\n",
    "--distillation_config.model_name EleutherAI/pythia-410m-deduped \\\n",
    "--distillation_config.enable_fsdp \\\n",
    "--distillation_config.pure_bf16 \\\n",
    "--distillation_config.distil_factor 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4172f3fe-ab4c-4638-a020-fe785609cb9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
