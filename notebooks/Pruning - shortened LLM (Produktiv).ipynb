{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa213e74-c2b9-4c46-ade4-d9af870a5e1d",
   "metadata": {},
   "source": [
    "# Pruning with shortened-llm\n",
    "\n",
    "We perform one-shot pruning by removing unimportant Transformer blocks in LLMs. Compared to recent baselines, our depth pruning achieves faster inference while yielding comparable or superior performance.\n",
    "In retraining pruned models for quality recovery, continued pretraining (CPT) on a large corpus markedly outperforms LoRA-based tuning, particularly at severe pruning ratios.\n",
    "\n",
    "Source: https://github.com/Nota-NetsPresso/shortened-llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f97712-49d8-4805-ad2f-20f5e9fdb9e0",
   "metadata": {},
   "source": [
    "## Procedure for model compression\n",
    "The procedure runs better form a terminal window. So please copy the cells below and use it in a terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13c5dee5-a889-42ea-8a27-e22f17728cfa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(/home/thsch026/my-envs/shortened-llm) \n"
     ]
    }
   ],
   "source": [
    "conda activate shortened-llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96c5f6d3-a67b-42ea-8cbc-018de04b20be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(/home/thsch026/my-envs/shortened-llm) \n"
     ]
    }
   ],
   "source": [
    "cd /home/thsch026/masterarbeit/experiment/shortened-llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac2b79e-652f-4d94-b72b-785e369a4c86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install typing_extensions packaging pygments psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9e778db-ad18-4873-ae83-3c58c3c3bf64",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(/home/thsch026/my-envs/shortened-llm) \n",
      "2\n",
      "(/home/thsch026/my-envs/shortened-llm) \n"
     ]
    }
   ],
   "source": [
    "export CUDA_VISIBLE_DEVICES=\"2\"\n",
    "echo $CUDA_VISIBLE_DEVICES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92d61d4-e132-4189-9214-efce3b0981e4",
   "metadata": {},
   "source": [
    "### Choose the model you want to tune\n",
    "- There are a lot of scripts for models in the script directory. The ones that are used here were edited so that it works in this environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acd132d-bbd3-4a12-ab2b-cbcc0a76aba8",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Llama3-8b-instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22e4f20-277f-44af-9c54-355700baee2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "script/prune_llama3-8b-instruct_crit-taylor.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef42753-fe80-4ee4-ab7c-72d49905e0e6",
   "metadata": {},
   "source": [
    "#### Llama 3 8B instruct - Lora Phase with dataset \"fathyshalab/germanquad_qaeval_qaeval_dataset\" \n",
    "- I made a copy of lora_retrain.py and edited the section for the prompt to fit to the new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765e3374-b8ab-4443-acbd-3f8e7a5ae2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "script/prune_llama3-8b-instruct_crit-taylor_tom1.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f3c2b9-08e8-4247-94a7-00405e083f3b",
   "metadata": {},
   "source": [
    "### Mistral-7B-Instruct-v0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe91b721-0599-4747-8afc-89ccf3f6eb35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "script/prune_Mistral_crit-taylor.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad21aea-2f29-45a2-81cc-491dd724f969",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Not complete - Zweiter Lora lauf mit den deutschen Ãœbersetzungen des ersten Datasets saillab/alpaca_german_taco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631388d2-08a2-499c-a4e5-62801e73126a",
   "metadata": {},
   "outputs": [],
   "source": [
    "script/lora_ger_llama3-8b-instruct-prune-lora_crit-taylor_tom.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98ff0d38-5463-414b-8e8f-1cd6658381f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/thsch026/masterarbeit/experiment/shortened-llm\n",
      "(/home/thsch026/my-envs/shortened-llm) \n"
     ]
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67f42066-9404-4c38-afa4-855b0bf17ad5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_block_sensitivity  output_tune     README.md        results  src\n",
      "output_prune              pyproject.toml  requirement.txt  script   wandb\n",
      "(/home/thsch026/my-envs/shortened-llm) \n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e489834-4294-4271-a79e-b332c99ff791",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
