{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa213e74-c2b9-4c46-ade4-d9af870a5e1d",
   "metadata": {},
   "source": [
    "# Another Pruning approach to test\n",
    "\n",
    "We perform one-shot pruning by removing unimportant Transformer blocks in LLMs. Compared to recent baselines, our depth pruning achieves faster inference while yielding comparable or superior performance.\n",
    "In retraining pruned models for quality recovery, continued pretraining (CPT) on a large corpus markedly outperforms LoRA-based tuning, particularly at severe pruning ratios.\n",
    "\n",
    "Source: https://github.com/Nota-NetsPresso/shortened-llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13c5dee5-a889-42ea-8a27-e22f17728cfa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(/home/thsch026/my-envs/shortened-llm) \n"
     ]
    }
   ],
   "source": [
    "conda activate shortened-llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4a54dd4-2a2a-4a56-8f9b-5f29f866d618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch                     2.3.1                    pypi_0    pypi\n",
      "(/home/thsch026/my-envs/shortened-llm) \n"
     ]
    }
   ],
   "source": [
    "conda list | grep torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7460bc-0105-4f69-8b93-95d3574750f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
