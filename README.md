This repo contains code and additional documents for my master thesis in appilied artificial intelligence.
-> Sorry, no english translation currently available

## Titel:
Effiziente Reduktion großer Sprachmodelle: Methodenvergleich und Anwendungsstrategien für Energieoptimierung und Hardware-Kompatibilität

## Zusammenfassung:
Große Sprachmodelle (Englisch: Large Language Models oder kurz LLMs) wie „Chat-GPT“ oder „Gemini“ haben erstaunliche Fähigkeiten entwickelt und Einzug in unseren Alltag gefunden. Zur Entwicklung und dem Betrieb solcher LLMs benötigt man erhebliche Ressourcen, insbesondere leistungsfähige Grafikkarten (Englisch kurz: GPUs) mit großen Speicherkapazitäten. Daher werden diese Modelle z.Zt. fast ausschließlich von kommerziellen Anbietern in großen Rechenzentren
betrieben und setzen somit eine Internet Verbindung zur Nutzung voraus.
In dieser Arbeit wird die Möglichkeit untersucht LLMs zu komprimieren, ohne dabei wesentliche Verluste ihrer Fähigkeiten in Kauf nehmen zu müssen. Existierende Technologien zur Kompression werden untersucht und kombiniert, um möglichst kleine Modelle zu erhalten, die im Idealfall auf normaler Consumer-Hardware kostengünstig betrieben werden können und so neue (Offline) Einsatzszenarien erschließen.

### Notebooks > Enhält die verwendeten Notebooks mit den Implementationen der Verfahren.
### Scripts > Enthält zusätzliche scripte, die bei einigen Verfahren zur Konfiguration benötigt werden.
